{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Neural Network - Application v8",
      "provenance": [],
      "collapsed_sections": [
        "6_WvDrUUVTOG"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skfo763/Google-ML-Bootcamp-phase1/blob/main/week4/Deep_Neural_Network_Application_v8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtKkFNlJUZv4"
      },
      "source": [
        "# 이미지 분류를 위한 심층 신경망 만들어보기 #\n",
        "\n",
        "이 과제를 마무리하면, 우리는 4주차 마지막, 다시 말해 이번 코스의 마지막 과제를 끝내게 될 것입니다.\n",
        "\n",
        "이번 과제에서는 지난 4주차 첫 번째 과제에서 구현했던 함수들을 사용해서 고양이 이미지를 분류하는 심층 신경망을 만들 것입니다. 2주차의 로지스틱 회귀 구현에 비해 정확도가 향상될 것을 기대해보며, 시작해봅시다.\n",
        "\n",
        "**이번 과제가 끝나고 여러분은:**\n",
        "- 지도 학습 방식으로 훈련된 심층 신경망을 개발할 수 있게 됩니다.\n",
        "\n",
        "이제 시작해봅시다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_WvDrUUVTOG"
      },
      "source": [
        "## 1. 패키지 다운로드 ##\n",
        "\n",
        "과제를 수행하는데 필요한 모든 패키지를 다운로드합니다.\n",
        "\n",
        "- [numpy](www.numpy.org)는 파이썬의 가장 기본적인 과학/수학 연산 관련 패키지입니다.\n",
        "- [h5py](http://www.h5py.org)는 .h5에 저장된 데이터셋과 상호작용하기 위한 패키지입니다.\n",
        "- [matplotlib](http://matplotlib.org)은 파이썬 환경에서 그래프를 그릴 수 있도록 해주는 유명한 라이브러리입니다.\n",
        "- [PIL](http://www.pythonware.com/products/pil/) 과 [scipy](https://www.scipy.org/)는 모든 모델의 학습이 끝난 후, 자기가 가지고 있는 샘플 사진으로 모델을 테스트하는데 사용됩니다.\n",
        "- `dnn_app_utils` 는 4주차 첫 번째 과제인 *Building your Deep Neural Network: Step by Step* 에서 구현했던 함수들을 모은 패키지입니다.\n",
        "- `np.random.seed(1)` 은 랜덤 함수가 일관된 결과를 만들도록 합니다. 채점을 위해 시드를 변경하지 마세요.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qCjkt4_VSwJ"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "from dnn_app_utils_v3 import *\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "np.random.seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv4diFFkWCOI"
      },
      "source": [
        "## 2. 데이터셋 ##\n",
        "\n",
        "이번 과제에서는 지난 2주차 과제 *Logistic Regression as a Neural Network* 에서 사용했었던 고양이 데이터를 그대로 사용합니다. 2주차 과제에서 만들었던 고양이 분류 모델의 최종 정확도는 `70%`였습니다. 이번에 새롭게 만들 모델은 그보다 더 높은 정확도를 가지길 기대해 봅니다.\n",
        "\n",
        "**문제 정의**: 주어진 데이터 셋 `data.h5` 에 대하여,\n",
        "- 훈련 세트 `m_train`은 고양이가 맞음(1), 고양이가 아님(1) 두 종류로 라벨링된 이미지 데이터 묶음입니다.\n",
        "- 테스트 세트 `m_test`는 훈련 세트와 마찬가지로 라벨링된 이미지 데이터 묶음입니다.\n",
        "- 각 이미지는 (num_px, num_px, 3) 의 형태를 하고 있습니다. 여기서 3은 색상 채널(RGB)를 의미합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNOGuw4sXC1J"
      },
      "source": [
        "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01fWhK7sXEbv"
      },
      "source": [
        "아래 코드는 데이터셋의 이미지가 어떻게 생겼는지 보여줍니다. 데이터 셋의 인덱스를 자유롭게 바꾸고 재 시도하여 다른 이미지를 확인해보세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92b0wEPtXQRl"
      },
      "source": [
        "# Example of a picture\n",
        "index = 10\n",
        "plt.imshow(train_x_orig[index])\n",
        "print (\"y = \" + str(train_y[0,index]) + \". It's a \" + classes[train_y[0,index]].decode(\"utf-8\") +  \" picture.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiRrRMBOXT9A"
      },
      "source": [
        "# Explore your dataset \n",
        "m_train = train_x_orig.shape[0]\n",
        "num_px = train_x_orig.shape[1]\n",
        "m_test = test_x_orig.shape[0]\n",
        "\n",
        "print (\"Number of training examples: \" + str(m_train))\n",
        "print (\"Number of testing examples: \" + str(m_test))\n",
        "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
        "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
        "print (\"train_y shape: \" + str(train_y.shape))\n",
        "print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
        "print (\"test_y shape: \" + str(test_y.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZam1d9FXYPd"
      },
      "source": [
        "늘 그랬듯이, 이미지를 신경망에 투입하기 적합한 사이즈로 재배치(reshape) 하고 표준화(standardize) 합시다. 바로 아래 코드 블록을 실행시켜 보세요.\n",
        "\n",
        "<img src=\"arts/imvectorkiank.png\" style=\"width:450px;height:300px;\"/>\n",
        "<center><u>Figure 1</u>: Image to vector conversion.</center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiizaeUWXuRI",
        "outputId": "f9c356e6-70ee-41a4-f3c4-c5b71f9f1b82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "# Reshape the training and test examples \n",
        "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
        "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
        "\n",
        "# Standardize data to have feature values between 0 and 1.\n",
        "train_x = train_x_flatten/255.\n",
        "test_x = test_x_flatten/255.\n",
        "\n",
        "print (\"train_x's shape: \" + str(train_x.shape))\n",
        "print (\"test_x's shape: \" + str(test_x.shape))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d3743c457ce4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Reshape the training and test examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_x_flatten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_x_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m   \u001b[0;31m# The \"-1\" makes reshape flatten the remaining dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_x_flatten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_x_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Standardize data to have feature values between 0 and 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_x_orig' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFRxlBFsXyIE"
      },
      "source": [
        "결과값 $12,288$은 $64 \\times 64 \\times S$(= 재배치된 이미지 벡터 하나의 크기) 입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRxZyoIVYQR9"
      },
      "source": [
        "## 3. 모델 구성하기 ##\n",
        "\n",
        "데이터 셋에 좀 익숙해졌나요? 이제는 고양이 이미지를 분류하는 심층 신경망 모델을 만들 차례입니다.\n",
        "\n",
        "이 과제에선 두 개의 다른 모델을 만들 것입니다.\n",
        "- 2-layer 신경망\n",
        "- L-layer 심층 신경망\n",
        "\n",
        "$L$ 값을 달리 하면 모델의 성능에 어떤 변화가 있는지를 비교하면서 이번 과제를 진행합니다.\n",
        "\n",
        "아래 두 모델의 신경망 구조를 확인해보세요\n",
        "\n",
        "### 3-1. 2-layer 신경망 ###\n",
        "\n",
        "<img src=\"arts/2layerNN_kiank.png\" style=\"width:650px;height:400px;\"/>\n",
        "<center><u>Figure 2</u>: 2-layer neural network.<br>The model can be summarized as: <b>INPUT -> LINEAR -> RELU -> LINEAR -> SIGMOID -> OUTPUT.</b></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "<u>그림 2에 대한 구체적인 설명</u>:\n",
        "- (64, 64, 3) 형태의 입력 이미지는 (12288, 1) 형태의 벡터로 변형됩니다. 이 벡터가 신경망의 최종 입력값으로 들어갑니다.\n",
        "- 이 입력값을 전치한 행렬 $[x_0,x_1,...,x_{12287}]^T$ 을 가중치 행렬 $W^{[1]}$과 곱합니다.\n",
        "- 이후 bias 값 $b^{[1]}$를 더하고, 최종 값에 `relu` 함수를 취해서 최종 활성화값을 담은 벡터 $[a_0^{[1]}, a_1^{[1]},..., a_{n^{[1]}-1}^{[1]}]^T$ 를 얻을 수 있습니다.\n",
        "- 이 과정을 1번 더 반복합니다.\n",
        "- 그 결과값에 $W^{[2]}$ 벡터를 곱하고, bias $b^{[2]}$를 더합니다.\n",
        "- 최종 계산 값에 `sigmoid` 함수를 취합니다. 그 결과가 0.5보다 크면 고양이로 분류합니다.\n",
        "\n",
        "<br>\n",
        "\n",
        "### 3-2. L-layer 심층 신경망 ###\n",
        "\n",
        "L개의 층을 가진 심층 신경망을 위 그림처럼 직접 표현하는 것은 어려운 일입니다. 그래서, 아래 그림처럼 일부 간소화된 신경망을 표현하는 그림을 준비했습니다.\n",
        "\n",
        "<img src=\"arts/LlayerNN_kiank.png\" style=\"width:650px;height:400px;\"/>\n",
        "<center><u>Figure 3</u>: L-layer neural network.<br>The model can be summarized as: <b>[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID</b></center>\n",
        "\n",
        "</br>\n",
        "\n",
        "<u>그림 3에 대한 구체적인 설명</u>:\n",
        "- (64, 64, 3) 형태의 입력 이미지는 (12288, 1) 형태의 벡터로 변형됩니다. 이 벡터가 신경망의 최종 입력값으로 들어갑니다.\n",
        "- 이 입력값을 전치한 행렬 $[x_0,x_1,...,x_{12287}]^T$ 을 가중치 행렬 $W^{[1]}$과 곱하고, bias $b^{[l]}$와 더합니다. 이 결과를 linear unit이라 부릅니다.\n",
        "- 다음으로, 이 linear unit에 `relu` 함수를 취합니다. 일련의 과정은 모델의 구조에 따라 매 $l$번 층의 파라미터 $(W^{[l]}, b^{[l]})$에 대해서 반복적으로 이루어집니다.\n",
        "- 최종적인 linear unit에 `sigmoid` 함수를 취해, 그 겨로가값이 0.5보다 크면 고양이로 분류합니다.\n",
        "\n",
        "<br>\n",
        "\n",
        "### 3-3. 보다 일반적인 방법론 ###\n",
        "\n",
        "기본적으로 모델을 구축하기 위해선 아래의 딥 러닝 방법론을 따르는 것이 좋습니다.\n",
        "1. 파라미터 초기화 / 하이퍼 파라미터 정의\n",
        "2. 사전에 정해진 반복 횟수만큼 반복하기\n",
        "  a. forward propagation\n",
        "  b. 비용 함수 계산\n",
        "  c. backward propagation\n",
        "  d. 파라미터 업데이트 (경사 하강법 사용)\n",
        "3. 훈련된 파라미터들을 사용해서 예측하기.\n",
        "\n",
        "이제 이 두 모델을 구현해보겠습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-MNnPT2d1RH"
      },
      "source": [
        "## 4. 2-layer 신경망 구현해보기 ##\n",
        "\n",
        "**연습 문제** 지난 과제에서 만들었던 helper 함수들을 사용해서 2개의 층을 가지는 신경망을 만들어봅시다. 2-layer의 구조는 *LINEAR -> RELU -> LINEAR -> SIGMOID* 과 같습니다. 지난번에 구현한 함수의 대략적인 구조는 아래와 같습니다.\n",
        "  ```python\n",
        "  def initialize_parameters(n_x, n_h, n_y):\n",
        "      ...\n",
        "      return parameters \n",
        "  def linear_activation_forward(A_prev, W, b, activation):\n",
        "      ...\n",
        "      return A, cache\n",
        "  def compute_cost(AL, Y):\n",
        "      ...\n",
        "      return cost\n",
        "  def linear_activation_backward(dA, cache, activation):\n",
        "      ...\n",
        "      return dA_prev, dW, db\n",
        "  def update_parameters(parameters, grads, learning_rate):\n",
        "      ...\n",
        "      return parameters\n",
        "  ```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8poOW-jYszd"
      },
      "source": [
        "### CONSTANTS DEFINING THE MODEL ####\n",
        "n_x = 12288     # num_px * num_px * 3\n",
        "n_h = 7\n",
        "n_y = 1\n",
        "layers_dims = (n_x, n_h, n_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc18dDU4eWxi"
      },
      "source": [
        "# GRADED FUNCTION: two_layer_model\n",
        "\n",
        "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
        "    \"\"\"\n",
        "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data, of shape (n_x, number of examples)\n",
        "    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n",
        "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
        "    \n",
        "    Returns:\n",
        "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    grads = {}\n",
        "    costs = []                              # to keep track of the cost\n",
        "    m = X.shape[1]                           # number of examples\n",
        "    (n_x, n_h, n_y) = layers_dims\n",
        "    \n",
        "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    parameters = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\".\n",
        "        ### START CODE HERE ### (≈ 2 lines of code)\n",
        "        A1, cache1 = None\n",
        "        A2, cache2 = None\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Compute cost\n",
        "        ### START CODE HERE ### (≈ 1 line of code)\n",
        "        cost = None\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Initializing backward propagation\n",
        "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
        "        \n",
        "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
        "        ### START CODE HERE ### (≈ 2 lines of code)\n",
        "        dA1, dW2, db2 = None\n",
        "        dA0, dW1, db1 = None\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
        "        grads['dW1'] = dW1\n",
        "        grads['db1'] = db1\n",
        "        grads['dW2'] = dW2\n",
        "        grads['db2'] = db2\n",
        "        \n",
        "        # Update parameters.\n",
        "        ### START CODE HERE ### (approx. 1 line of code)\n",
        "        parameters = None\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Retrieve W1, b1, W2, b2 from parameters\n",
        "        W1 = parameters[\"W1\"]\n",
        "        b1 = parameters[\"b1\"]\n",
        "        W2 = parameters[\"W2\"]\n",
        "        b2 = parameters[\"b2\"]\n",
        "        \n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "       \n",
        "    # plot the cost\n",
        "\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per hundreds)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyNmqOxTecN_"
      },
      "source": [
        "아래 코드 블록을 실행시켜서 파라미터를 학습해보세요. 모델이 실제로 동작하는지를 확인해봅시다. cost 값이 감소해야 합니다. 보통 2500회 반복하여 학습하는데 최대 5분정도의 시간이 걸립니다. 0회 반복의 비용이 아래의 모범 답안과 같은지 확인해보고, 일치하지 않는다면 이 주피터 노트북 상단의 사각형(⬛)을 클릭하여 실행을 중지하고 오류를 찾아보세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBPPmDjQecgE"
      },
      "source": [
        "parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmmRUZpwec1E"
      },
      "source": [
        "**모범 답안**:\n",
        "<table> \n",
        "    <tr>\n",
        "        <td> <b>Cost after iteration 0</b></td>\n",
        "        <td> 0.6930497356599888 </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td> <b>Cost after iteration 100</b></td>\n",
        "        <td> 0.6464320953428849 </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td> <b>...</b></td>\n",
        "        <td> ... </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td> <b>Cost after iteration 2400</b></td>\n",
        "        <td> 0.048554785628770226 </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKCD4fHhg3Tw"
      },
      "source": [
        "벡터화된 방식으로 구현했기 때문에, 훈련하는데 큰 시간이 걸리지 않았습니다. 만약 벡터화 방식이 아니었다면 10배는 많은 시간이 들었을 것입니다.\n",
        "\n",
        "이제 훈련시킨 파라미터를 통해 이미지를 분류하는데 사용할 수 있습니다. 훈련 세트와 테스트 세트에 대한 예측 정확도를 확인해봅시다. 아래 코드를 실행시켜보세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDn1vUROhMbC"
      },
      "source": [
        "predictions_train = predict(train_x, train_y, parameters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EGXNoo9hOKw"
      },
      "source": [
        "**모범 답안**:\n",
        "<table> \n",
        "    <tr>\n",
        "        <td><b>Accuracy</b></td>\n",
        "        <td> 1.0 </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSBJBMF2hPey"
      },
      "source": [
        "predictions_test = predict(test_x, test_y, parameters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3j8r-b4hPrR"
      },
      "source": [
        "**모범 답안**:\n",
        "<table> \n",
        "    <tr>\n",
        "        <td> <b>Accuracy</b></td>\n",
        "        <td> 0.72 </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv4jJsNwhi_e"
      },
      "source": [
        "**참고** : 1500회 정도 되는 더 적은 반복 횟수로 모델을 실행하면 테스트 세트에서도 더 좋은 정확도를 얻을 수 있습니다. 이를 *early stopping* 이라고 하며, 다음 코스에서 이에 대해 설명합니다. *early stopping* 은 과적합을 방지하는 방법입니다.\n",
        "\n",
        "어쨌거나 축하합니다! 2-layer 신경망이 70%의 정확도를 가졌던 로지스틱 회귀보다 더 나은 72%의 성능을 가지는 것으로 보입니다. $L$-layer 모델을 쓴다면 더 좋은 성능을 가지는지 살펴보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAdhuG6BiDub"
      },
      "source": [
        "## 5. L-layer 신경망 구현하기 ##\n",
        "\n",
        "**연습 문제**: 위와 마찬가지로 지난 과제에 구현한 helper 함수들을 사용해서 L-layer 신경망을 만들어보세요. 구조는 <u>[LINEAR -> RELU] $\\times$(L-1) -> LINEAR -> SIGMOID</u> 와 같습니다. 지난번에 구현한 함수들의 대략적인 구조는 아래와 같습니다.\n",
        "  ```python\n",
        "  def initialize_parameters_deep(layers_dims):\n",
        "    ...\n",
        "    return parameters \n",
        "  def L_model_forward(X, parameters):\n",
        "      ...\n",
        "      return AL, caches\n",
        "  def compute_cost(AL, Y):\n",
        "      ...\n",
        "      return cost\n",
        "  def L_model_backward(AL, Y, caches):\n",
        "      ...\n",
        "      return grads\n",
        "  def update_parameters(parameters, grads, learning_rate):\n",
        "      ...\n",
        "      return parameters\n",
        "  ```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALeIcDlahXD_"
      },
      "source": [
        "### CONSTANTS ###\n",
        "layers_dims = [12288, 20, 7, 5, 1] #  4-layer model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2n4Sf9hisjs"
      },
      "source": [
        "# GRADED FUNCTION: L_layer_model\n",
        "\n",
        "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
        "    \"\"\"\n",
        "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
        "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    print_cost -- if True, it prints the cost every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(1)\n",
        "    costs = []                         # keep track of cost\n",
        "    \n",
        "    # Parameters initialization. (≈ 1 line of code)\n",
        "    ### START CODE HERE ###\n",
        "    parameters = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
        "        ### START CODE HERE ### (≈ 1 line of code)\n",
        "        AL, caches = None\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Compute cost.\n",
        "        ### START CODE HERE ### (≈ 1 line of code)\n",
        "        cost = None\n",
        "        ### END CODE HERE ###\n",
        "    \n",
        "        # Backward propagation.\n",
        "        ### START CODE HERE ### (≈ 1 line of code)\n",
        "        grads = None\n",
        "        ### END CODE HERE ###\n",
        " \n",
        "        # Update parameters.\n",
        "        ### START CODE HERE ### (≈ 1 line of code)\n",
        "        parameters = None\n",
        "        ### END CODE HERE ###\n",
        "                \n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "            \n",
        "    # plot the cost\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per hundreds)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zula5S1wiuSs"
      },
      "source": [
        "이제 4개의 층을 가지는 신경망을 학습시켜봅시다.\n",
        "\n",
        "아래 코드 블록을 실행시켜서 파라미터를 학습해보세요. 모델이 실제로 동작하는지를 확인해봅시다. cost 값이 감소해야 합니다. 보통 2500회 반복하여 학습하는데 최대 5분정도의 시간이 걸립니다. 0회 반복의 비용이 아래의 모범 답안과 같은지 확인해보고, 일치하지 않는다면 이 주피터 노트북 상단의 사각형(⬛)을 클릭하여 실행을 중지하고 오류를 찾아보세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BDJT--Zi0w8"
      },
      "source": [
        "parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2n2LtKji69x"
      },
      "source": [
        "**모범 답안**:\n",
        "<table> \n",
        "    <tr>\n",
        "        <td><b>Cost after iteration 0</b></td>\n",
        "        <td> 0.771749 </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><b>Cost after iteration 100</b></td>\n",
        "        <td> 0.672053 </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><b>...</b></td>\n",
        "        <td> ... </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><b>Cost after iteration 2400</b></td>\n",
        "        <td> 0.092878 </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnxLx53yi2uz"
      },
      "source": [
        "pred_test = predict(test_x, test_y, parameters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhpWKODJi7TH"
      },
      "source": [
        "<table>\n",
        "    <tr>\n",
        "    <td>\n",
        "    <b>Train Accuracy</b>\n",
        "    </td>\n",
        "    <td>\n",
        "    0.985645933014\n",
        "    </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXnMS-pPjLj-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g58BbWs8jLyv"
      },
      "source": [
        "<table>\n",
        "    <tr>\n",
        "    <td>\n",
        "    <b>Test Accuracy</b>\n",
        "    </td>\n",
        "    <td>\n",
        "    0.985645933014\n",
        "    </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaE4HFo_jTv0"
      },
      "source": [
        "동일한 테스트 세트에서 4개의 층을 가지는 신경망이 2-layer 신경망보다 8% 더 나은 정확도를 가집니다. 모델을 개선하는데 성공했습니다. 축하합니다.\n",
        "\n",
        "심층 신경망을 어떻게 개선하는가에 관한 내용은 다음 코스에서 다를 예정입니다. 이 내용은 하이퍼 파라미터 (`learning_rate`, `layers_dims`, `num_iterations` 등등)를 체계적으로 검색하여 더 높은 정확도를 얻는 방법을 말합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR_0FVZCkaOx"
      },
      "source": [
        "## 6. 결과 분석하기 ##\n",
        "\n",
        "먼저 이 모델이 올바르지 않게 예측한 이미지를 확인해봅시다. 아래 코드를 실행하면, 모델이 라벨을 잘못 지정한 몇 가지 이미지들을 보여줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPsqhf9VkWtW"
      },
      "source": [
        "print_mislabeled_images(classes, test_x, test_y, pred_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QybKP5F2k5mt"
      },
      "source": [
        "**모델이 제대로 수행하지 못하는 몇 가지 이미지 유형은 다음과 같습니다.**\n",
        "- 특이한 위치의 고양이 몸\n",
        "- 비슷한 색상의 배경에 고양이가 나타납니다.\n",
        "- 특이한 고양이 색과 종\n",
        "- 카메라 앵글\n",
        "- 그림의 밝기\n",
        "- 규모 변화 (고양이가 이미지에서 매우 크거나 작음)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyy9SEJClFLu"
      },
      "source": [
        "## 7 - 자체 이미지를 통한 테스트 (선택 학습 / 채점 안됨) ##\n",
        "\n",
        "과제를 모두 마치신 것을 축하드립니다! 이제 직접 자신이 가지고 있는 이미지를 가지고 모델의 output을 확인해봅시다. 아래의 방법을 따라해보세요.\n",
        "\n",
        "1. 이 notebook 의 상단 표시줄에서 \"File\"을 클릭 후, \"open\"을 클릭하여 Cousera Hub로 이동합니다.\n",
        "2. \"images\" 폴더에 있는 Jupyter Notebook의 디렉토리에 이미지를 추가합니다.\n",
        "3. 아래 코드에서 이미지 이름을 변경합니다.\n",
        "4. 코드를 실행하고, 알고리즘이 올바른 결과를 내는지 확인하세요! (1 = 고양이, 0 = 고양이 아님)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6-0_3ivlGPG"
      },
      "source": [
        "## START CODE HERE ##\n",
        "my_image = \"my_image.jpg\" # change this to the name of your image file \n",
        "my_label_y = [1] # the true class of your image (1 -> cat, 0 -> non-cat)\n",
        "## END CODE HERE ##\n",
        "\n",
        "fname = \"images/\" + my_image\n",
        "image = np.array(ndimage.imread(fname, flatten=False))\n",
        "my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((num_px*num_px*3,1))\n",
        "my_image = my_image/255.\n",
        "my_predicted_image = predict(my_image, my_label_y, parameters)\n",
        "\n",
        "plt.imshow(image)\n",
        "print (\"y = \" + str(np.squeeze(my_predicted_image)) + \", your L-layer model predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veesOKc2lMHL"
      },
      "source": [
        "**참고 문헌**:\n",
        "- for auto-reloading external module: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython"
      ]
    }
  ]
}