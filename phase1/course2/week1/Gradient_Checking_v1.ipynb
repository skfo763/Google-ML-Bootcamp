{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gradient Checking v1",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skfo763/Google-ML-Bootcamp-phase1/blob/main/course2/week1/Gradient_Checking_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JhXk0zMKpPK"
      },
      "source": [
        "# Gradient Checking (가중치 검사하기) #\n",
        "\n",
        "이번 주차 마지막 과제에 오신것을 환영합니다! 이 과제에서 여러분은 gradient checking을 직접 구현할 것입니다. 주어진 상황을 한번 가정해봅시다.\n",
        "\n",
        "여러분은 글로벌 모바일 결제 시스템을 개발하는 팀에 속해 있습니다. 여러분은 딥 러닝 모델을 이용하여 허위 결제를 감지하는 프로그램을 만들고자 합니다. 만약 사용자 계정이 해커에게 탈취당했다는 등, 특정 결제 정보가 사기로 의심된다면 이를 감지하는 프로그램을 만들고자 하는 것입니다.\n",
        "\n",
        "하지만 이 모델의 역전파를 계산하는 작업은 쉽지 않았고, 때때로 버그도 발견됩니다. 이 시스템은 결제와 관련된 매우 중요한 애플리케이션이기 때문에, 이 회사의 CEO는 여러분의 모델이 올바르게 학습되었는지 정말로 확신하길 원합니다. CEO는 \"당신의 역전파 연산이 실제로 잘 동작한다는 증거를 가져오시오!\" 라며 당신에게 자료를 요구합니다. 그에게 확신을 주기 위해, 우리는 `Gradient Checking` 을 사용합니다.\n",
        "\n",
        "시작해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3xkci2PL2H4"
      },
      "source": [
        "# Packages\n",
        "import numpy as np\n",
        "from testCases import *\n",
        "from gc_utils import sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIFG7dGxL6ZF"
      },
      "source": [
        "## 1. Gradient check의 작동 원리 ##\n",
        "\n",
        "역전파 연산은 기울기인 $\\frac{\\partial J}{\\partial\\theta}$($\\theta$는 모델의 파라미터를 의미합니다)를 계산합니다. $J$는 정방향 연산과 손실 함수를 통해 계산되는 값입니다.\n",
        "\n",
        "정방향 연산은 비교적 쉽게 구현할 수 있기 때문에, 여러분은 그 연산을 통해 계산한 비용 $J$가 정확한 값이라는 것에 대해 거의 100% 확신할 수 있습니다. 그러므로 $\\frac{\\partial J}{\\partial\\theta}$를 계산하는 코드를 검증할 때 $J$를 계산하는 코드를 사용할 수 있습니다.\n",
        "\n",
        "미분(혹은 기울기)의 정의를 다시 한번 살펴보겠습니다 :\n",
        "\n",
        "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$\n",
        "\n",
        "만약 여러분이 $\\displaystyle \\lim_{\\varepsilon \\to 0}$ 과 같은 극한 꼴에 익숙하지 않다면, 이는 $\\varepsilon$이 굉장히 굉장히 작은 값에 근접한 값이라고 이해하면 됩니다.\n",
        "\n",
        "다음을 기억하세요.\n",
        "- 계산이 올바른지 검증해야할 값은 $\\frac{\\partial J}{\\partial \\theta}$입니다.\n",
        "-  $J$의 값이 올바르다고 가정한다면, $J(\\theta + \\varepsilon)$과 $J(\\theta - \\varepsilon)$ ($\\theta$는 실수)를 계산하여 위의 변수가 올바르게 계산되었는지 검증할 수 있습니다.\n",
        "\n",
        "\n",
        "(1)번 공식과, 아주 작은 값인 $\\varepsilon$을 사용하여 계산된 $\\frac{\\partial J}{\\partial \\theta}$ 값이 올바르다는 것을 검증해봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkApvFOcP1mA"
      },
      "source": [
        "## 2. 1차원 Gradient Checking ##\n",
        "\n",
        "$J(\\theta) = \\theta x$의 꼴을 하고 있는 1차원 선형 함수를 생각해봅시다. 모델은 $x$를 입력으로 받고, 단일 실수 매개 변수 $\\theta$만 가지고 있습니다.\n",
        "\n",
        "여러분은 $J(.)$와 그 기울기인 $\\frac{\\partial J}{\\partial \\theta}$ 를 계산하는 코드를 구현할 것입니다. 그리고 gradient checking을 이용해 $J$를 올바르게 구하고 있는지 검증합니다.\n",
        "\n",
        "<img src=\"arts/1Dgrad_kiank.png\" style=\"width:600px;height:250px;\">\n",
        "\n",
        "**그림 1**: **1D linear model**\n",
        "<br>\n",
        "\n",
        "위의 다이어그램은 핵심 단계를 나타냅니다. 첫번째로 $x$에서 시작해서, 함수 $J(x)$를 계산하는 forward propagation 과정을 진행합니다. 이후 기울기 $\\frac{\\partial J}{\\partial \\theta}$를 계산하는 back propagation 작업이 이어집니다.\n",
        "\n",
        "**연습 문제**: 위 함수에 대해 간단히 정방향 연산와 역방향 연산을 구현해보세요. \n",
        "\n",
        "*I.e.*, $J(.)$(\"forward propagation\")과 $\\theta$에 대한 J 함수의 기울기(\"backwrad propagation\")을 각각 별개의 함수를 통해 계산하세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uvlaVkjMHhi"
      },
      "source": [
        "# GRADED FUNCTION: forward_propagation\n",
        "\n",
        "def forward_propagation(x, theta):\n",
        "    \"\"\"\n",
        "    Implement the linear forward propagation (compute J) presented in Figure 1 (J(theta) = theta * x)\n",
        "    \n",
        "    Arguments:\n",
        "    x -- a real-valued input\n",
        "    theta -- our parameter, a real number as well\n",
        "    \n",
        "    Returns:\n",
        "    J -- the value of function J, computed using the formula J(theta) = theta * x\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### (approx. 1 line)\n",
        "    J = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return J"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdaxQ6n8RNej"
      },
      "source": [
        "x, theta = 2, 4\n",
        "J = forward_propagation(x, theta)\n",
        "print (\"J = \" + str(J))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOfe9H1JRNQW"
      },
      "source": [
        "**모범 답안**:\n",
        "<table style=>\n",
        "    <tr>\n",
        "        <td>  <b> J </b>  </td>\n",
        "        <td> 8</td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUP8tRu7RTMr"
      },
      "source": [
        "**연습 문제**: 이제 그림 1번에 나타난 backward propagation(기울기 계산)을 구현할 차례입니다. 함수 $J(\\theta) = {\\theta}x$의 $\\theta$에 대한 미분계수(기울기)를 구해보세요. 미적분에 대해서 잘 알지 못해도 괜찮습니다. `dtheta` 변수는 $dtheta = \\frac { \\partial J }{ \\partial \\theta} = x$ 과 같은 공식을 통해서 얻어낼 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0TTC29QRSJ4"
      },
      "source": [
        "# GRADED FUNCTION: backward_propagation\n",
        "\n",
        "def backward_propagation(x, theta):\n",
        "    \"\"\"\n",
        "    Computes the derivative of J with respect to theta (see Figure 1).\n",
        "    \n",
        "    Arguments:\n",
        "    x -- a real-valued input\n",
        "    theta -- our parameter, a real number as well\n",
        "    \n",
        "    Returns:\n",
        "    dtheta -- the gradient of the cost with respect to theta\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### (approx. 1 line)\n",
        "    dtheta = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return dtheta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMaq2BlvR7EN"
      },
      "source": [
        "x, theta = 2, 4\n",
        "dtheta = backward_propagation(x, theta)\n",
        "print (\"dtheta = \" + str(dtheta))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX0vGlrgR7l5"
      },
      "source": [
        "**모범 답안**:\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>  <b> dtheta </b>  </td>\n",
        "        <td> 2 </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3aFdoc-SAJk"
      },
      "source": [
        "**연습 문제**: `backward_propagation()` 함수가 기울기 $\\frac{\\partial J}{\\partial \\theta}$를 올바르게 계산하고 있는지 체크하기 위해, gradient checking을 직접 구현해 봅시다.\n",
        "\n",
        "**지시 사항**\n",
        "- 첫번째로는, 위의 (1)번 공식과 매우 작은 값인 $\\varepsilon$을 사용해 `gradapprox` 변수를 계산하세요. 아래 스텝을 따라서 계산하면 됩니다.\n",
        "  1. $\\theta^{+} = \\theta + \\varepsilon$\n",
        "  2. $\\theta^{-} = \\theta - \\varepsilon$\n",
        "  3. $J^{+} = J(\\theta^{+})$\n",
        "  4. $J^{-} = J(\\theta^{-})$\n",
        "  5. $gradapprox = \\frac{J^{+} - J^{-}}{2  \\varepsilon}$\n",
        "- 그리고 나서 backward propagation을 통해 기울기를 구해보세요. 그 값을 `grad` 변수에 저장합니다.\n",
        "- 마지막으로, 아래 공식을 써서 `gradapprox`와 `grad` 사이의 차이를 비교해보세요.\n",
        "$$ difference = \\frac {\\mid\\mid grad - gradapprox \\mid\\mid_2}{\\mid\\mid grad \\mid\\mid_2 + \\mid\\mid gradapprox \\mid\\mid_2} \\tag{2}$$\n",
        "  1. `np.linalg.norm(...)` 함수를 사용하여 분자를 계산하세요.\n",
        "  2. 같은 방법으로 분모를 계산하세요. `np,linalg.norm` 함수를 두 번 호출해야 합니다.\n",
        "  3. 두 값을 나누세요.\n",
        "- 만약 두 값 사이의 차이가 충분히 작다($10^{-7}$보다 작다)면, 여러분은 `backwrad_propagation`이 꽤 잘 작동하고 있다고 자신있게 말할 수 있습니다. 그렇지 않다면, 기울기 계산에 뭔가 실수가 있었을 가능성이 높습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6OiYWzBR_zR"
      },
      "source": [
        "# GRADED FUNCTION: gradient_check\n",
        "\n",
        "def gradient_check(x, theta, epsilon = 1e-7):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation presented in Figure 1.\n",
        "    \n",
        "    Arguments:\n",
        "    x -- a real-valued input\n",
        "    theta -- our parameter, a real number as well\n",
        "    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)\n",
        "    \n",
        "    Returns:\n",
        "    difference -- difference (2) between the approximated gradient and the backward propagation gradient\n",
        "    \"\"\"\n",
        "    \n",
        "    # Compute gradapprox using left side of formula (1). epsilon is small enough, you don't need to worry about the limit.\n",
        "    ### START CODE HERE ### (approx. 5 lines)\n",
        "    thetaplus = None                               # Step 1\n",
        "    thetaminus = None                              # Step 2\n",
        "    J_plus = None                                  # Step 3\n",
        "    J_minus = None                                 # Step 4\n",
        "    gradapprox = None                              # Step 5\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Check if gradapprox is close enough to the output of backward_propagation()\n",
        "    ### START CODE HERE ### (approx. 1 line)\n",
        "    grad = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    ### START CODE HERE ### (approx. 1 line)\n",
        "    numerator = None                               # Step 1'\n",
        "    denominator = None                             # Step 2'\n",
        "    difference = None                              # Step 3'\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    if difference < 1e-7:\n",
        "        print (\"The gradient is correct!\")\n",
        "    else:\n",
        "        print (\"The gradient is wrong!\")\n",
        "    \n",
        "    return difference"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bz24k0VoT7wd"
      },
      "source": [
        "x, theta = 2, 4\n",
        "difference = gradient_check(x, theta)\n",
        "print(\"difference = \" + str(difference))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKVmb7PRT92t"
      },
      "source": [
        "**모범 답안**:\n",
        "<br>\n",
        "`The gradient is correct!`\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>  <b> difference </b>  </td>\n",
        "        <td> 2.9193358103083e-10 </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNxqPhCQUJ_t"
      },
      "source": [
        "두 값 사이의 차이가 임계치인 $10^{-7}$보다 작습니다. 따라서 `backward_propagation()`함수가 gradient를 올바르게 계산했다고 확신할 수 있습니다.\n",
        "\n",
        "이제 보다 일반적인 상황에서, 즉 비용 함수 J가 1차원의 단일 입력값보다 많은 값을 받을 때의 gradient check에 대해서 다뤄보겠습니다. 인공 신경망을 훈련할 때, $\\theta$는 실제로는 행렬 $W^{[l]}$과 $b^{[l]}$ 여러개로 이루어져 있습니다. 그러므로 다차원 행렬이 입력으로 들어올 때의 gradient check을 어떻게 하는지 이해하는 것은 매우 중요하다고 할 수 있습니다. 시작해봅시다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtqITJ6JVB62"
      },
      "source": [
        "## 3. N차원 Gradient checking ##\n",
        "\n",
        "아래는 과제 도입부에서 설명했던 허위 결제 감지 모델의 forward, backward propagation을 나타내는 그림입니다.\n",
        "\n",
        "<img src=\"arts/NDgrad_kiank.png\" style=\"width:600px;height:400px;\">\n",
        "\n",
        "**그림 2**: **deep neural network**<br>*LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID*\n",
        "\n",
        "이제 미리 구현된 forward propagation과 backward propagation을 훑어봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS1ojHQmUB7e"
      },
      "source": [
        "def forward_propagation_n(X, Y, parameters):\n",
        "    \"\"\"\n",
        "    Implements the forward propagation (and computes the cost) presented in Figure 3.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- training set for m examples\n",
        "    Y -- labels for m examples \n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
        "                    W1 -- weight matrix of shape (5, 4)\n",
        "                    b1 -- bias vector of shape (5, 1)\n",
        "                    W2 -- weight matrix of shape (3, 5)\n",
        "                    b2 -- bias vector of shape (3, 1)\n",
        "                    W3 -- weight matrix of shape (1, 3)\n",
        "                    b3 -- bias vector of shape (1, 1)\n",
        "    \n",
        "    Returns:\n",
        "    cost -- the cost function (logistic cost for one example)\n",
        "    \"\"\"\n",
        "    \n",
        "    # retrieve parameters\n",
        "    m = X.shape[1]\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "    W3 = parameters[\"W3\"]\n",
        "    b3 = parameters[\"b3\"]\n",
        "\n",
        "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = relu(Z1)\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = relu(Z2)\n",
        "    Z3 = np.dot(W3, A2) + b3\n",
        "    A3 = sigmoid(Z3)\n",
        "\n",
        "    # Cost\n",
        "    logprobs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(1 - A3), 1 - Y)\n",
        "    cost = 1./m * np.sum(logprobs)\n",
        "    \n",
        "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
        "    \n",
        "    return cost, cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiwnvFG9aU-n"
      },
      "source": [
        "def backward_propagation_n(X, Y, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation presented in figure 2.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input datapoint, of shape (input size, 1)\n",
        "    Y -- true \"label\"\n",
        "    cache -- cache output from forward_propagation_n()\n",
        "    \n",
        "    Returns:\n",
        "    gradients -- A dictionary with the gradients of the cost with respect to each parameter, activation and pre-activation variables.\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
        "    \n",
        "    dZ3 = A3 - Y\n",
        "    dW3 = 1./m * np.dot(dZ3, A2.T)\n",
        "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
        "    \n",
        "    dA2 = np.dot(W3.T, dZ3)\n",
        "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
        "    dW2 = 1./m * np.dot(dZ2, A1.T) * 2\n",
        "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
        "    \n",
        "    dA1 = np.dot(W2.T, dZ2)\n",
        "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
        "    dW1 = 1./m * np.dot(dZ1, X.T)\n",
        "    db1 = 4./m * np.sum(dZ1, axis=1, keepdims = True)\n",
        "    \n",
        "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
        "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
        "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
        "    \n",
        "    return gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46UfUrFxaUzL"
      },
      "source": [
        "허위 결제 감지 테스트 세트를 통해 결과를 얻었지만, 이 모델에 대해서 100% 확신할 수 없습니다. gradient가 올바른지 확인하기 위해, N차원 모델에서의 gradient checking을 구현해 보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70nIiYjvavCO"
      },
      "source": [
        "**Gradient checking의 작동 원리는?**\n",
        "\n",
        "이 과제의 1번 챕터와 2번 챕터에서와 마찬가지로, 여러분은 `gradapprox` 변수와 backpropgation 작업에서 계산된 `gradient` 값을 비교할 것입니다. 공식은 이전과 마찬가지로,\n",
        "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$\n",
        "과 같습니다.\n",
        "\n",
        "하지만, $\\theta$는 더 이상 스칼라 값이 아닙니다. 이 값은 `parameters` 라고 명명된 딕셔너리입니다. 이 과제에선 `dictionary_to_vector()`라고 하는 빌트인 함수를 제공하고 있습니다. 이 함수는 `parameters` 디렉토리 내부의 데이터인 (W1, b1, W2, b2, W3, b3)을 벡터로 재배치하여 이어붙인 `values` 변수를 리턴합니다.\n",
        "\n",
        "위 함수와 정 반대의 역할을 하는 함수는 `vector_to_dictionary` 함수입니다. 이 함수는 gradient checking의 결과값인 벡터를 딕셔너리로 다시 변환시켜주는 역할을 합니다.\n",
        "\n",
        "<img src=\"arts/dictionary_to_vector.png\" style=\"width:600px;height:400px;\">\n",
        "\n",
        "**그림 3**: **dictionary_to_vector() 와 vector_to_dictionary()**<br>`gradient_check_n()` 함수를 구현할 때 이 두 함수가 필요합니다.\n",
        "\n",
        "추가로, `gradient` 딕셔너리를 `grad`라는 게터로 바꾸어주는 `gradients_to_vector()` 함수도 제공하니, 이 부분에 대해서도 걱정할 필요 없습니다.\n",
        "\n",
        "**연습 문제**: `gradient_check_n()` 함수를 구현하세요.\n",
        "\n",
        "**지시 사항**: 아래에 나와 있는 gradient check를 위한 간단한 의사 코드를 참조하세요.\n",
        "\n",
        "- `J_plus[i]`를 구하기 위해서는,\n",
        "  1. $\\theta^{+}$ 를 `np.copy(parameters_values)`로 설정합니다.\n",
        "  2. $\\theta^{+}_i$를 $\\theta^{+}_i + \\varepsilon$로 설정합니다.\n",
        "  3. `forward_propagation_n(x, y, vector_to_dictionary(`$\\theta^{+}$ `))`. 함수를 이용하여 $J^{+}_i$ 를 계산합니다.\n",
        "- `J_minus[i]`를 계산하기 위해서는, $\\theta^{-}$ 변수를 가지고 위의 과정을 똑같이 반복합니다.\n",
        "- $gradapprox[i] = \\frac{J^{+}_i - J^{-}_i}{2 \\varepsilon}$ 공식을 이용하여 `gradapprox`를 계산합니다.\n",
        "\n",
        "이 과정을 통해 여러분은 벡터 변수 `gradapprox`를 얻을 수 있습니다. 이 때 `gradapprox[i]`는 `parameter_values[i]`에 대한 기울기의 근사치로 볼 수 있습니다. 이 `gradapprox` 벡터를 역전파에서 실제로 계산한 `gradient` 벡터와 비교해보세요. 이전 1차원 케이스와 동일하게, 아래 공식을 사용하면 됩니다.\n",
        "\n",
        "$$ difference = \\frac {\\| grad - gradapprox \\|_2}{\\| grad \\|_2 + \\| gradapprox \\|_2 } \\tag{3}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmVZ5jkkdjs4"
      },
      "source": [
        "# GRADED FUNCTION: gradient_check_n\n",
        "\n",
        "def gradient_check_n(parameters, gradients, X, Y, epsilon = 1e-7):\n",
        "    \"\"\"\n",
        "    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
        "    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters. \n",
        "    x -- input datapoint, of shape (input size, 1)\n",
        "    y -- true \"label\"\n",
        "    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)\n",
        "    \n",
        "    Returns:\n",
        "    difference -- difference (2) between the approximated gradient and the backward propagation gradient\n",
        "    \"\"\"\n",
        "    \n",
        "    # Set-up variables\n",
        "    parameters_values, _ = dictionary_to_vector(parameters)\n",
        "    grad = gradients_to_vector(gradients)\n",
        "    num_parameters = parameters_values.shape[0]\n",
        "    J_plus = np.zeros((num_parameters, 1))\n",
        "    J_minus = np.zeros((num_parameters, 1))\n",
        "    gradapprox = np.zeros((num_parameters, 1))\n",
        "    \n",
        "    # Compute gradapprox\n",
        "    for i in range(num_parameters):\n",
        "        \n",
        "        # Compute J_plus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_plus[i]\".\n",
        "        # \"_\" is used because the function you have to outputs two parameters but we only care about the first one\n",
        "        ### START CODE HERE ### (approx. 3 lines)\n",
        "        thetaplus = None                                      # Step 1\n",
        "        thetaplus[i][0] = None                                # Step 2\n",
        "        J_plus[i], _ = None                                   # Step 3\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Compute J_minus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_minus[i]\".\n",
        "        ### START CODE HERE ### (approx. 3 lines)\n",
        "        thetaminus = None                                     # Step 1\n",
        "        thetaminus[i][0] = None                               # Step 2        \n",
        "        J_minus[i], _ = None                                  # Step 3\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Compute gradapprox[i]\n",
        "        ### START CODE HERE ### (approx. 1 line)\n",
        "        gradapprox[i] = None\n",
        "        ### END CODE HERE ###\n",
        "    \n",
        "    # Compare gradapprox to backward propagation gradients by computing difference.\n",
        "    ### START CODE HERE ### (approx. 1 line)\n",
        "    numerator = None                                           # Step 1'\n",
        "    denominator = None                                         # Step 2'\n",
        "    difference = None                                          # Step 3'\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    if difference > 2e-7:\n",
        "        print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
        "    else:\n",
        "        print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
        "    \n",
        "    return difference"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc-Me2qCdn5m"
      },
      "source": [
        "X, Y, parameters = gradient_check_n_test_case()\n",
        "\n",
        "cost, cache = forward_propagation_n(X, Y, parameters)\n",
        "gradients = backward_propagation_n(X, Y, cache)\n",
        "difference = gradient_check_n(parameters, gradients, X, Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNxI-I4KdpCw"
      },
      "source": [
        "**모범 답안**:\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>  <b> There is a mistake in the backward propagation!</b>  </td>\n",
        "        <td> difference = 0.285093156781 </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5wreptId016"
      },
      "source": [
        "`backward_propagation_n`함수 코드에 오류가 있는 것 같습니다. 여러분이 구현한 gradient check 코드를 통해 오류를 검출했습니다. 이제 오류가 있다는 것을 알아냈으니, backward_propagation 과정으로 다시 돌아가 오류가 어디에서 발생했는지 찾아서 수정합니다(힌트 : *dW2*와 *db1*을 확인해보세요). 에러를 다 고쳤다고 판단될 경우, gradient check 로직을 한번 더 실행해보세요. 역전파 로직을 수정하는 경우 이를 체크하는 `backward_propagation_n()` 함수가 정의된 코드 블록을 다시 실행해야합니다.\n",
        "\n",
        "기울기 계산을 담당하는 코드가 올바르게 작성되었는지 검증하는 gradient check에 대해서 재대로 이해하셨나요? 실제 검증에 성공했는지에 대한 부분은 채점되지 않지만, 이제 역전파가가 올바르게 구현되었음을 확신 할 때까지 버그를 찾고 그래디언트 검사를 다시 실행하는 것을 추천합니다.\n",
        "\n",
        "**기억해둘 것**\n",
        "- Gradient Checking은 느린 작업입니다. gradient에 대한 근사치를 구하는 $\\frac{\\partial J}{\\partial \\theta} \\approx  \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon}$ 공식의 계산과정이 복잡하기 때문입니다. 이 이유로, 훈련 중 매 반복마다 기울기 검사를 해선 안됩니다.\n",
        "- Gradient Checking은 dropout 정규화 기법에서는 재대로 동작하지 않습니다. 따라서 우선 dropout 없이 기울기 검사 알고리즘을 작동시키고, 그 이후 dropout을 적용하는 것이 좋습니다.\n",
        "\n",
        "축하합니다, 이제 여러분은 이 허위 결제 감지 모델이 잘 작동하고 있다는데 큰 자신감을 가지게 되었습니다. 이제 이 데이터를 가지고 CEO를 설득하는데에도 사용할 수 있겠군요.\n",
        "\n",
        "<br>\n",
        "\n",
        "**이 과제를 통해 기억할 것**:\n",
        "- Gradient Checking(기울기 검사)은 backpropagation을 통해 계산된 기울기와, 실제 함수의 수치적인 근사치가 얼마나 근접한지를 통해서 역방향 계산이 잘 동작하는지를 검증합니다.\n",
        "- 기울기 검사는 태생적으로 느린 작업입니다. 따라서 매 반복마다 기울기 검사 알고리즘을 돌리면 안됩니다. 일반적으로 코드가 올바른지 확인하기 위해서만 실행 한 다음 끄고 실제 훈련 과정에 역 전파를 사용합니다."
      ]
    }
  ]
}