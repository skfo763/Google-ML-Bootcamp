{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic_Regression_with_a_Neural_Network_mindset_Kor",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skfo763/Google-ML-Bootcamp-phase1/blob/main/week2/Logistic_Regression_with_a_Neural_Network_mindset_Kor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSEC4TWeyUwB"
      },
      "source": [
        "# 인공신경망을 활용한 로지스틱 회귀\n",
        "첫 번째 프로그래밍 과제(이자 필수 과제) 에 오신 여러분을 환영합니다! 여러분은 지금부터 고양이를 인식하는 로지스틱 회귀 분류 모델을 만들겁니다. 이 과제는, 인공신경망 방식으로 모델을 학습하는 일련의 과정을 안내하고, 이를 통해 딥러닝에 대한 직관을 연마하도록 해 줄 것입니다.\n",
        "\n",
        "**안내사항:**\n",
        "- 명시적으로 사용해도 된다는 등의 다른 지시사항이 없는 경우, 코드에서 반복문 (for / while)을 사용하지 마세요!\n",
        "\n",
        "**과제를 통해 배울 수 있는 것들:**\n",
        "- 다음을 포함한 기계학습 알고리즘의 일반적인 구조를 구축합니다:\n",
        "    - 파라미터 초기화\n",
        "    - cost function과 gradient의 계산\n",
        "    - 최적화 알고리즘 사용 (경사 하강법)\n",
        "- 위 세 가지 기능을 올바른 순서대로 구성하여 메인 모델을 만듭시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjfpOXDf8evd"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eDNz0iXzzH-"
      },
      "source": [
        "## 1 - 파이썬 패키지들 ##\n",
        "\n",
        "처음으로, 아래의 코드 블록을 실행시켜 이 과제를 하는동안 필요한 패키지들을 import 해봅시다. 필요한 패키지들의 개략적인 소개는 다음과 같습니다.\n",
        "- [numpy](www.numpy.org)는 파이썬의 가장 기본적인 과학/수학 연산 관련 패키지입니다.\n",
        "- [h5py](http://www.h5py.org)는 .h5에 저장된 데이터셋과 상호작용하기 위한 패키지입니다.\n",
        "- [matplotlib](http://matplotlib.org)은 파이썬 환경에서 그래프를 그릴 수 있도록 해주는 유명한 라이브러리입니다.\n",
        "- [PIL](http://www.pythonware.com/products/pil/) 과 [scipy](https://www.scipy.org/)는 모든 모델의 학습이 끝난 후, 자기가 가지고 있는 샘플 사진으로 모델을 테스트하는데 사용됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjY8VwVE0soD"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "import scipy\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "from lr_utils import load_dataset\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7AOZDMZ8dC2"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo0KYGTG09km"
      },
      "source": [
        "## 2 - 문제(데이터) 셋 개요 ##\n",
        "\n",
        "**문제 정의** : 주어진 데이터 셋(\"data.h5\")는 다음의 데이터를 포함하고 있습니다.\n",
        "  - 고양이(y=1) / 고양이가 아님(y=0) 으로 라벨링된 m_train의 이미지 훈련 세트\n",
        "  - 위와 마찬가지로 라벨링된 m_test의 테스트 세트\n",
        "  - 각각의 이미지는 (num_px, num_px, 3)의 shape를 가지고, 마지막 숫자 3이 의미하는 바는 색상 채널이 총 3개 (RGB)입니다. 다시말해 각 이미지는 height과 width이 num_px로 모두 같은 정사각형 이미지입니다.\n",
        "\n",
        "이제 당신은 특정 이미지가 고양이인지 아닌지 여부를 올바르게 분류하는 간단한 이미지 인식 알고리즘을 만들 것입니다.\n",
        "\n",
        "데이터 셋에 좀 더 익숙해지기 위해, 아래의 코드를 실행시켜 데이터 셋을 로드해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MH4k71d2LOU"
      },
      "source": [
        "# Loading the data (cat/non-cat)\n",
        "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt0Adx7m2OgH"
      },
      "source": [
        "전처리를 위해, 각각의 이미지 데이터셋 (train: 훈련용, test: 테스트용) 이름 뒤에 \"_orig\"라는 표기를 붙였습니다. 전처리 이후 실제 훈련과 테스트에 사용되는 train_set_x 와 test_set_x를 얻을 수 있습니다. (라벨링된 데이터인 train_set_y와 test_set_y는 전처리가 필요없습니다)\n",
        "\n",
        "train_set_x_orig 및 test_set_x_orig의 각 row은 이미지를 나타내는 배열입니다. 아래의 코드를 실행하여 샘플 이미지를 볼 수 있습니다. 다른 샘플 이미지를 보려면 index 값을 변경하고 코드 블록을 다시 실행하세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRBvrG7D3M-a"
      },
      "source": [
        "# Example of a picture\n",
        "index = 25\n",
        "plt.imshow(train_set_x_orig[index])\n",
        "print (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CEBO48O3RNw"
      },
      "source": [
        "딥 러닝 학습의 많은 버그들이 행렬/벡터 dimensions을 맞춰주지 않았을 때 발생합니다. 만약 행렬/벡터의 dimension을 올바르게 유지할 수 있다면 버그를 제거하는데 큰 도움이 될 것입니다.\n",
        "\n",
        "**연습 문제**: 다음 변수의 값을 찾아보세요:\n",
        "  - m_train (훈련 데이터의 개수)\n",
        "  - m_test (테스트 데이터의 개수)\n",
        "  - num_px (훈련 이미지의 높이 = 너비)\n",
        "\n",
        "train_set_x_orig의 값이 (m_train, num_px, num_px, 3) 의 shape를 하고 있는 numpy-array라는 것을 기억하세요! 다시 말해, 여러분은 train_set_x_orig.shape[0] 이라는 코드를 통해서 m_train의 값을 얻어올 수 있습니다!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0anATjw4c3H"
      },
      "source": [
        "### START CODE HERE ### (≈ 3 lines of code)\n",
        "m_train = \n",
        "m_test = \n",
        "num_px = \n",
        "### END CODE HERE ###\n",
        "\n",
        "print (\"Number of training examples: m_train = \" + str(m_train))\n",
        "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
        "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
        "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
        "print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
        "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
        "print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
        "print (\"test_set_y shape: \" + str(test_set_y.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VSCTtWr4kh8"
      },
      "source": [
        "**모범 답안(m_train, m_test and num_px)**: \n",
        "<table style=\"width:15%\">\n",
        "  <tr>\n",
        "    <td>m_train</td>\n",
        "    <td> 209 </td> \n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td>m_test</td>\n",
        "    <td> 50 </td> \n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td>num_px</td>\n",
        "    <td> 64 </td> \n",
        "  </tr>\n",
        "  \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJi1--UG45CA"
      },
      "source": [
        "편의를 위해 (num_px, num_px, 3)의 shape를 가진 이미지들을 (num_px * num_px * 3, 1) 의 형태로 재배치(reshape) 해야 합니다. 이 과정을 수행하면 우리의 훈련 및 테스트 데이터셋은 각각의 컬럼이 평평해진(flatten) 형태의 이미지 데이터를 의미하는 numpy-array가 됩니다. 이 배열의 컬럼의 개수는 m_train개가 될 것입니다 (테스트 데이터셋의 경우, m_test개가 되겠죠).\n",
        "\n",
        "\n",
        "**연습 문제**: 훈련 데이터셋과 테스트 데이터셋을 (num_px, num_px, 3) shape의 행렬에서 (num_px * num_px * 3, 1)의 단순 벡터로 재배치해보세요. \n",
        "\n",
        "(a, b, c, d)의 shape를 가진 행렬 X를 (b * c * d, a)로 재배치하기 위해서는 아래의 방법을 사용합니다 :\n",
        "  `X_flatten = X.reshape(X.shape[0], -1).T     # X.T는 X의 전치 행렬`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlP-R6VU6b8O"
      },
      "source": [
        "# Reshape the training and test examples\n",
        "\n",
        "### START CODE HERE ### (≈ 2 lines of code)\n",
        "train_set_x_flatten = \n",
        "test_set_x_flatten =\n",
        "\n",
        "### END CODE HERE ###\n",
        "\n",
        "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
        "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
        "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
        "print (\"test_set_y shape: \" + str(test_set_y.shape))\n",
        "print (\"sanity check after reshaping: \" + str(train_set_x_flatten[0:5,0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVPGiTui6o9G"
      },
      "source": [
        "**모범 답안**: \n",
        "\n",
        "<table style=\"width:35%\">\n",
        "  <tr>\n",
        "    <td>train_set_x_flatten shape</td>\n",
        "    <td> (12288, 209)</td> \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>train_set_y shape</td>\n",
        "    <td>(1, 209)</td> \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>test_set_x_flatten shape</td>\n",
        "    <td>(12288, 50)</td> \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>test_set_y shape</td>\n",
        "    <td>(1, 50)</td> \n",
        "  </tr>\n",
        "  <tr>\n",
        "  <td>재배치가 잘 되었는지 검사</td>\n",
        "  <td>[17 31 56 22 33]</td> \n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca-oAk6U7IaB"
      },
      "source": [
        "이미지의 색상을 표현하려면 각 픽셀에 대해 RGB (빨강, 녹색, 파랑 채널)를 지정해야하므로 픽셀 값은 실제로 0에서 255 사이의 세 숫자로 구성된 벡터입니다.\n",
        "\n",
        "머신 러닝의 일반적인 전처리 단계 중 하나는 데이터 세트를 중앙에 배치하고 표준화하는 것입니다. 즉, 각 샘플에서 전체 numpy 배열의 평균을 뺀 다음 이를 전체 numpy 배열의 표준 편차로 나눕니다. \n",
        "\n",
        "그러나 이미지 데이터 세트의 경우 더 간단하고 편리합니다. 데이터 세트의 모든 행을 255 (픽셀 채널의 최대 값)로 나누는 것으로 표준화를 할 수 있습니다.\n",
        "\n",
        "데이터 세트를 표준화합시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWya3bwE7cfS"
      },
      "source": [
        "train_set_x = train_set_x_flatten/255.\n",
        "test_set_x = test_set_x_flatten/255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rgot9ejG7gtW"
      },
      "source": [
        "**기억해야 할 것:**\n",
        "\n",
        "새로운 데이터셋을 전처리하기 위한 일반적인 방법:\n",
        "\n",
        "1. 주어진 문제(데이터셋)의 dimensions과 shape를 알아낸다 (m_train, m_test, num_px, ...)\n",
        "\n",
        "2. 각 샘플 데이터가 벡터가 되도록 데이터의 세트를 재배치(reshape) 한다. (num_px \\* num_px \\* 3, 1)\n",
        "\n",
        "3. 데이터를 \"표준화\" 한다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X5JlCDK8ZHe"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15mhtXuw8o8K"
      },
      "source": [
        "## 3 - 학습 알고리즘의 전반적인 구조 ##\n",
        "\n",
        "고양이 이미지와 고양이가 아닌 이미지를 구별하는 간단한 알고리즘을 설계해봅시다.\n",
        "\n",
        "신경망 학습 방식을 사용하여 로지스틱 회귀를 구축합니다. 다음 그림은 **로지스틱 회귀가 왜 매우 단순한 신경망인지를 보여줍니다!**\n",
        "\n",
        "<img src='./arts/LogReg_kiank.png' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8KFkzed-PRl"
      },
      "source": [
        "**알고리즘의 수식**:\n",
        "\n",
        "샘플 데이터 $x^{(i)}$ 에 대하여:\n",
        "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
        "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
        "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
        "\n",
        "비용함수는 모든 데이터에 대한 L값을 더한 값입니다.:\n",
        "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n",
        "\n",
        "**핵심**:\n",
        "이 연습 단계에서는 아래와 같은 과정을 수행합니다 : \n",
        "  - 모델 파라미터(가중치, 절편) 초기화\n",
        "  - 모델의 비용 함수를 최소화하도록 하는 파라미터를 학습 \n",
        "  - 학습된 파라미터를 바탕으로 테스트 데이터 셋에 대하여 예측\n",
        "  - 예측 결과를 분석하고, 결론을 도출"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-5g0UQl_4R9"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MLkGsuP_MdG"
      },
      "source": [
        "## 4 - 알고리즘의 각 부분 만들기 ##\n",
        "\n",
        "인공 신경망의 주된 세 가지 스텝은 다음과 같습니다 : \n",
        "  1. 모델의 구조를 정의한다 (input 특성의 개수 등)\n",
        "  2. 모델의 파라미터 (가중치 등)를 초기화한다\n",
        "  3. 반복한다 :\n",
        "    - 현재의 loss 값을 계산 (forward propagation)\n",
        "    - 현재의 gradient 값 계산 (backward propagation)\n",
        "    - 파라미터 업데이트 (gradient descent)\n",
        "\n",
        " 이 과제에서는 1-3의 과정을 별개로 구현하고, 이후 `model()` 함수 내부에 일련의 과정들을 통합할 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04W27KB2_MUK"
      },
      "source": [
        "### 4-1. Helper functions ###\n",
        "\n",
        "**연습 문제** : \"Python Basics\" 과제에서 사용했던 코드를 활용하여, `sigmoid()` 함수를 구현해 봅시다. $sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$ 수식을 구현해야 합니다. `np.exp()` 함수를 사용하면 쉽게 구현할 수 있습니다..\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGSwPP3KAAfw"
      },
      "source": [
        "# GRADED FUNCTION: sigmoid\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Arguments:\n",
        "    z -- A scalar or numpy array of any size.\n",
        "\n",
        "    Return:\n",
        "    s -- sigmoid(z)\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    s = \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fMGqhfuAfow"
      },
      "source": [
        "print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdP1HQpHAe__"
      },
      "source": [
        "**모범 답안**: \n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <td>sigmoid([0, 2])</td>\n",
        "    <td> [ 0.5         0.88079708]</td> \n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8KpCdyjApXO"
      },
      "source": [
        "### 4-2. 파라미터 초기화 ###\n",
        "\n",
        "**연습 문제** : 파라미터를 초기화하는 아래 코드 블록을 구현해봅시다. 여러분은 맨 처음 가중치 값을 의미하는 0의 값만을 가지는 벡터 w를 초기화해야 합니다. 만약 이를 위한 적절한 numpy 함수를 찾지 못하겠다면, Numpy 라이브러리의 `np.zeros()` 함수의 문서를 참조하세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtboErg1BF5k"
      },
      "source": [
        "# GRADED FUNCTION: initialize_with_zeros\n",
        "\n",
        "def initialize_with_zeros(dim):\n",
        "    \"\"\"\n",
        "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
        "    \n",
        "    Argument:\n",
        "    dim -- size of the w vector we want (or number of parameters in this case)\n",
        "    \n",
        "    Returns:\n",
        "    w -- initialized vector of shape (dim, 1)\n",
        "    b -- initialized scalar (corresponds to the bias)\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    w =\n",
        "    b =\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    assert(w.shape == (dim, 1))\n",
        "    assert(isinstance(b, float) or isinstance(b, int))\n",
        "    \n",
        "    return w, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iye5yBkdBHix"
      },
      "source": [
        "dim = 2\n",
        "w, b = initialize_with_zeros(dim)\n",
        "print (\"w = \" + str(w))\n",
        "print (\"b = \" + str(b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta3KP8b7BMdl"
      },
      "source": [
        "**모범 답안**: \n",
        "\n",
        "\n",
        "<table style=\"width:15%\">\n",
        "    <tr>\n",
        "        <td>  w(가중치)  </td>\n",
        "        <td> [[ 0.]\n",
        " [ 0.]] </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>  b(절편)  </td>\n",
        "        <td> 0 </td>\n",
        "    </tr>\n",
        "</table>\n",
        "\n",
        "이미지 input에 대해서, 가중치 w는 (num_px $\\times$ num_px $\\times$ 3, 1) 의 shape를 가지는 벡터가 될 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o14RTsYQBlka"
      },
      "source": [
        "### 4-3. Forward / Backward propagation ###\n",
        "\n",
        "파라미터들이 초기화되었다면, 이제 파라미터들을 학습하기 위해 \"forward\", \"backward\" propagation을 수행합니다.\n",
        "\n",
        "- Forward propagation : 정방향 순전파\n",
        "- Backward propagation : 오차 역전파\n",
        "\n",
        "**연습 문제** : cost function(비용 함수)과 gradient를 계산하는 propagate() 함수를 완성하세요\n",
        "\n",
        "**힌트** : \n",
        "\n",
        "Forward Propagation:\n",
        "- X 행렬에 대하여\n",
        "- $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$ 를 계산합니다.\n",
        "- 다음의 cost function을 계산합니다 : $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
        "\n",
        "Backward Propagation: \n",
        "\n",
        "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
        "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds-Twq_BBRgv"
      },
      "source": [
        "# GRADED FUNCTION: propagate\n",
        "\n",
        "def propagate(w, b, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function and its gradient for the propagation explained above\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of size (num_px * num_px * 3, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
        "\n",
        "    Return:\n",
        "    cost -- negative log-likelihood cost for logistic regression\n",
        "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
        "    db -- gradient of the loss with respect to b, thus same shape as b\n",
        "    \n",
        "    Tips:\n",
        "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    \n",
        "    # FORWARD PROPAGATION (FROM X TO COST)\n",
        "    ### START CODE HERE ### (≈ 2 lines of code)\n",
        "    A =                 # compute activation\n",
        "    cost =              # compute cost\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
        "    ### START CODE HERE ### (≈ 2 lines of code)\n",
        "    dw = \n",
        "    db = \n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    assert(dw.shape == w.shape)\n",
        "    assert(db.dtype == float)\n",
        "    cost = np.squeeze(cost)\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return grads, cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxlN-x3kCyjJ"
      },
      "source": [
        "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
        "grads, cost = propagate(w, b, X, Y)\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))\n",
        "print (\"cost = \" + str(cost))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNR8Q9-zCx-f"
      },
      "source": [
        "**모범 답안**:\n",
        "\n",
        "<table style=\"width:50%\">\n",
        "    <tr>\n",
        "        <td>  dw  </td>\n",
        "      <td> [[ 0.99845601]\n",
        "     [ 2.39507239]]</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>  db  </td>\n",
        "        <td> 0.00145557813678 </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>  cost  </td>\n",
        "        <td> 5.801545319394553 </td>\n",
        "    </tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHt-7CJNC_O6"
      },
      "source": [
        "### 4-4. 최적화 ###\n",
        "\n",
        "- 지금까지 초기화된 파라미터를 가지고\n",
        "- gradient과 cost function을 계산했습니다.\n",
        "- 이제, 계산된 파라미터들을 업데이트합니다.\n",
        "\n",
        "**연습 문제** : 최적화를 위해 가중치 등의 파라미터를 업데이트하는 함수를 작성합시다. 목표는 cost function J를 최소화하는 가중치 w와 절편 b를 찾는 것입니다. 파라미터 $\\theta$ 에 대해서, $\\theta$를 업데이트하는 방법은  $ \\theta = \\theta - \\alpha \\text{ } d\\theta$ 입니다 $\\alpah\\는 학습률(learning rate) 입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdNxZEivDAi2"
      },
      "source": [
        "# GRADED FUNCTION: optimize\n",
        "\n",
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "    \"\"\"\n",
        "    This function optimizes w and b by running a gradient descent algorithm\n",
        "    \n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    print_cost -- True to print the loss every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    params -- dictionary containing the weights w and bias b\n",
        "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
        "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
        "    \n",
        "    Tips:\n",
        "    You basically need to write down two steps and iterate through them:\n",
        "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
        "        2) Update the parameters using gradient descent rule for w and b.\n",
        "    \"\"\"\n",
        "    \n",
        "    costs = []\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "        \n",
        "        \n",
        "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
        "        ### START CODE HERE ### \n",
        "        grads, cost = \n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Retrieve derivatives from grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "        \n",
        "        # update rule (≈ 2 lines of code)\n",
        "        ### START CODE HERE ###\n",
        "        w = \n",
        "        b = \n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Record the costs\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "        \n",
        "        # Print the cost every 100 training iterations\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "    \n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return params, grads, costs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC_opxFhD87o"
      },
      "source": [
        "params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
        "\n",
        "print (\"w = \" + str(params[\"w\"]))\n",
        "print (\"b = \" + str(params[\"b\"]))\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyZ5zzpxD70D"
      },
      "source": [
        "**Expected Output**: \n",
        "\n",
        "<table style=\"width:40%\">\n",
        "    <tr>\n",
        "       <td> w </td>\n",
        "       <td>[[ 0.19033591]\n",
        " [ 0.12259159]] </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "       <td> b </td>\n",
        "       <td> 1.92535983008 </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "       <td> db </td>\n",
        "       <td> 0.219194504541 </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqbaZZAMEyF3"
      },
      "source": [
        "**연습 문제** 위의 함수는 학습된 가중치 w와 절편 b를 나타냅니다. 이제 우리는 이 w와 b 값을 활용해 주어진 데이터셋 X에서 라벨 값(고양이인지 아닌지)을 예측할 수 있게 되었습니다.\n",
        "\n",
        "예측을 담당하는 predict() 함수를 구현해봅시다. 두 가지 스텝이 있습니다.\n",
        "  1. $\\hat{Y} = A = \\sigma(w^T X + b)$ 를 계산합니다.\n",
        "\n",
        "  2. activation 값이 0.5 이하라면 0 - false를, activation 값이 0.5 초과라면 1 - true 값을 리턴합니다. 이 때는 for 루프에서 if/else 문을 사용할 수 있습니다. (물론 이를 따로 벡터화 하는 방법도 있습니다만..) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVzRtxwWGOkP"
      },
      "source": [
        "# GRADED FUNCTION: predict\n",
        "\n",
        "def predict(w, b, X):\n",
        "    '''\n",
        "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
        "    \n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of size (num_px * num_px * 3, number of examples)\n",
        "    \n",
        "    Returns:\n",
        "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
        "    '''\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    Y_prediction = np.zeros((1,m))\n",
        "    w = w.reshape(X.shape[0], 1)\n",
        "    \n",
        "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    A = \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    for i in range(A.shape[1]):\n",
        "        \n",
        "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
        "        ### START CODE HERE ### (≈ 4 lines of code)\n",
        "        \n",
        "\n",
        "\n",
        "        \n",
        "        ### END CODE HERE ###\n",
        "    \n",
        "    assert(Y_prediction.shape == (1, m))\n",
        "    \n",
        "    return Y_prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04j734CdGSEB"
      },
      "source": [
        "w = np.array([[0.1124579],[0.23106775]])\n",
        "b = -0.3\n",
        "X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\n",
        "print (\"predictions = \" + str(predict(w, b, X)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK-vlzfrGSpo"
      },
      "source": [
        "**모범 답안**: \n",
        "\n",
        "<table style=\"width:30%\">\n",
        "    <tr>\n",
        "         <td>\n",
        "             predictions\n",
        "         </td>\n",
        "          <td>\n",
        "            [[ 1.  1.  0.]]\n",
        "         </td>  \n",
        "   </tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNQgFXeMHH3J"
      },
      "source": [
        "**기억해야 할 것:**\n",
        "\n",
        "\n",
        "지금까지 구현했던 몇 가지의 함수들은 :\n",
        "\n",
        "1. 모델의 파라미터인 w와 b를 초기화\n",
        "\n",
        "2. 반복적으로 loss를 최적화하여 파라미터 w, b를 학습\n",
        "  1. cost와 gradient를 계산\n",
        "  2. 경사 하강법을 사용해 파라미터 업데이트\n",
        "\n",
        "3. 학습된 파라미터 w, b를 사용해 주어진 테스트 데이터에 대한 결과값 예측"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-jVQym-H4aI"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLkHfGb7H8HD"
      },
      "source": [
        "## 5 - 하나의 모델로 통합 ##\n",
        "\n",
        "이제 지금까지 구현되었던 모든 코드 들을 올바른 순서로 모아 전체 모델이 어떻게 구성되는지 확인 해봅시다.\n",
        "\n",
        "**연습 문제** : `model()` 함수를 구현해봅시다. 아래의 표기법을 참조하세요.\n",
        "\n",
        "- Y_prediction_test는 테스트 데이터 셋에 대한 예측 결과입니다.\n",
        "- Y_prediction_train는 훈련 데이터 셋에 대한 예측 결과입니다.\n",
        "- w, costs, grads 는 optimize() 함수의 결과값입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObagrlYrH5Xi"
      },
      "source": [
        "# GRADED FUNCTION: model\n",
        "\n",
        "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
        "    \"\"\"\n",
        "    Builds the logistic regression model by calling the function you've implemented previously\n",
        "    \n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
        "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
        "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
        "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
        "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_cost -- Set to true to print the cost every 100 iterations\n",
        "    \n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    \n",
        "    # initialize parameters with zeros (≈ 1 line of code)\n",
        "    w, b = \n",
        "\n",
        "    # Gradient descent (≈ 1 line of code)\n",
        "    parameters, grads, costs =\n",
        "    \n",
        "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "    \n",
        "    # Predict test/train set examples (≈ 2 lines of code)\n",
        "    Y_prediction_test =\n",
        "    Y_prediction_train =\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Print train/test Errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "\n",
        "    \n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test, \n",
        "         \"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"w\" : w, \n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "    \n",
        "    return d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7eCNZqMI1FH"
      },
      "source": [
        "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q00abYesIy9r"
      },
      "source": [
        "**모범 답안**: \n",
        "\n",
        "<table style=\"width:40%\"> \n",
        "  <tr>\n",
        "    <td> Cost after iteration 0 </td> \n",
        "    <td> 0.693147 </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> <center> $\\vdots$ </center> </td> \n",
        "    <td> <center> $\\vdots$ </center> </td> \n",
        "  </tr>  \n",
        "  <tr>\n",
        "    <td> Train Accuracy </td> \n",
        "    <td> 99.04306220095694 % </td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> Test Accuracy </td> \n",
        "    <td> 70.0 % </td>\n",
        "  </tr>\n",
        "</table> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8scbwT--H5ta"
      },
      "source": [
        "**Comment** : 훈련 정확도는 100 %에 가깝습니다. 모델이 작동하고 있으며 훈련 데이터에 맞을만큼 충분한 용량이 있습니다. 테스트 정확도는 약 70% 입니다. 우리가 사용한 데이터 세트가 작다는 점과, 로지스틱 회귀가 선형 분류라는 점을 감안하면 충분히 만족스러운 결과입니다. 하지만 걱정하지 마세요. 다음 주에 더 나은 분류 모델을 만들 수 있습니다!\n",
        "\n",
        "또한 모델이 훈련 데이터를 분명히 과대 적합 하고 있음을 알 수 있습니다. 이 과제 후반부에서 정규화를 사용하여 과대 적합을 줄이는 방법을 배웁니다. \n",
        "\n",
        "어쨌거나, 아래 코드를 사용하고 인덱스 변수를 변경하면 테스트 세트의 이미지 대한 예측을 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwMQPzpfKJ2W"
      },
      "source": [
        "# Example of a picture that was wrongly classified.\n",
        "index = 1\n",
        "plt.imshow(test_set_x[:,index].reshape((num_px, num_px, 3)))\n",
        "print (\"y = \" + str(test_set_y[0,index]) + \", you predicted that it is a \\\"\" + classes[d[\"Y_prediction_test\"][0,index]].decode(\"utf-8\") +  \"\\\" picture.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JYZYueZKQL6"
      },
      "source": [
        "# Plot learning curve (with costs)\n",
        "costs = np.squeeze(d['costs'])\n",
        "plt.plot(costs)\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations (per hundreds)')\n",
        "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjkbH6rXKOhS"
      },
      "source": [
        "**Interpretation** : cost가 지속적으로 감소하는 것을 볼 수 있습니다. 이는 파라미터가 학습되고 있음을 보여줍니다. 그러나 여러분은 훈련 데이터 셋에서 지금보다 더 많이 학습을 수행할 수 있다는걸 볼 수 있습니다. 위의 셀에서 반복 횟수를 늘리고 셀을 다시 실행해보세요. 훈련 세트 정확도는 올라가지만 테스트 세트 정확도는 내려가는 것을 확인할 수 있을 것입니다. 이를 과대 적합이라고합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgR3RswTLKYq"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWkgvwh6LM02"
      },
      "source": [
        "## 6 - 추가적인 분석 (선택 학습 / 채점 안됨)##\n",
        "\n",
        "첫 번째 이미지 분류 모델을 완성하신걸 축하드립니다! 이제 모델을 좀 더 분석해봅시다. 추가로 살펴볼 사항은 학습률 $\\alpha$ 입니다\n",
        "\n",
        "\n",
        "#### 학습률 선택하기 ####\n",
        "\n",
        "**참고** :\n",
        "경사 하강법이 원활하게 작동하도록 하려면 학습률을 잘 선택해야합니다. 학습률 $\\alpha$는 파라미터가 업데이트되는 속도를 결정합니다. 학습률이 너무 크면 최적 값을 초과할 수 있고, 너무 작으면 최적값으로 수렴하기 위해 너무 많은 반복이 필요합니다. 그렇기 때문에 잘 조정된 학습률을 사용하는 것이 중요합니다.\n",
        "\n",
        "앞서 개발한 모델의 학습 곡선을 학습률만 달리 하여 비교해봅시다. 아래 셀을 실행하면 약 1분정도 소요됩니다. 이후 `learning_rates` 변수를 다른 값으로 변경해서 재 실행했을때, 어떤 점이 변화하는지 확인해보세요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQSC7ujKLK1C"
      },
      "source": [
        "learning_rates = [0.01, 0.001, 0.0001]\n",
        "models = {}\n",
        "for i in learning_rates:\n",
        "    print (\"learning rate is: \" + str(i))\n",
        "    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False)\n",
        "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
        "\n",
        "for i in learning_rates:\n",
        "    plt.plot(np.squeeze(models[str(i)][\"costs\"]), label= str(models[str(i)][\"learning_rate\"]))\n",
        "\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations (hundreds)')\n",
        "\n",
        "legend = plt.legend(loc='upper center', shadow=True)\n",
        "frame = legend.get_frame()\n",
        "frame.set_facecolor('0.90')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpDuzTG_Moi-"
      },
      "source": [
        "**Interpretation** :\n",
        "- 학습률이 다르면 cost가 달라지므로, 예측 결과도 달라집니다.\n",
        "- 학습률이 너무 크면 cost가 반복 시마다 오르내릴 수 있습니다. (어쨌거나, 이번 case에선 0.01을 사용하더라도 결국 cost 대비 좋은 값이 됩니다)\n",
        "- 더 낮은 cost가 더 나은 모델을 의미하지는 않습니다. 과대 적합이 있는지 확인해야 합니다. 훈련 정확도가 테스트 정확도보다 훨씬 높을때 발생합니다.\n",
        "- 딥 러닝에서는 일반적으로 다음을 권장합니다.\n",
        "  - cost function을 최소화시키는 학습률을 선택하세요\n",
        "  - 모델이 과대 적합되면 다른 기술을 사용하여 과대 적합을 줄여야 합니다 (이에 대해서는 이후 비디오에서 설명하겠습니다.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9voqQBRNq9b"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k_WlNp2Nr4s"
      },
      "source": [
        "## 7 - 자체 이미지를 통한 테스트 (선택 학습 / 채점 안됨) ##\n",
        "\n",
        "과제를 모두 마치신 것을 축하드립니다! 이제 직접 자신이 가지고 있는 이미지를 가지고 모델의 output을 확인해봅시다. 아래의 방법을 따라해보세요.\n",
        "\n",
        "1. 이 notebook 의 상단 표시줄에서 \"File\"을 클릭 후, \"open\"을 클릭하여 Cousera Hub로 이동합니다.\n",
        "2. \"images\" 폴더에 있는 Jupyter Notebook의 디렉토리에 이미지를 추가합니다.\n",
        "3. 아래 코드에서 이미지 이름을 변경합니다.\n",
        "4. 코드를 실행하고, 알고리즘이 올바른 결과를 내는지 확인하세요! (1 = 고양이, 0 = 고양이 아님)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYvHqcJpNnxb"
      },
      "source": [
        "## START CODE HERE ## (PUT YOUR IMAGE NAME) \n",
        "my_image = \"my_image.jpg\"   # change this to the name of your image file \n",
        "## END CODE HERE ##\n",
        "\n",
        "# We preprocess the image to fit your algorithm.\n",
        "fname = \"images/\" + my_image\n",
        "image = np.array(ndimage.imread(fname, flatten=False))\n",
        "image = image/255.\n",
        "my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((1, num_px*num_px*3)).T\n",
        "my_predicted_image = predict(d[\"w\"], d[\"b\"], my_image)\n",
        "\n",
        "plt.imshow(image)\n",
        "print(\"y = \" + str(np.squeeze(my_predicted_image)) + \", your algorithm predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrzTE-5AOR5K"
      },
      "source": [
        "**과제를 통해 기억할 것** :\n",
        "\n",
        "1. 데이터 전처리는 매우 중요합니다.\n",
        "2. `initialize()`, `propagate()`, `optimize()` 함수를 각각 구현한 이후, `model()` 함수를 통해서 이를 통합했습니다.\n",
        "3. 학습률 (이는 \"하이퍼 파라미터\" 의 한 예입니다)을 튜닝하는 것 은 알고리즘에 큰 차이를 불러올 수 있습니다. 이 부분에 대해서는 추후 더 다룰 예정입니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ-tJaZVPU1w"
      },
      "source": [
        "마지막으로, 원하신다면 이 노트북에서 다양한 작업을 시도해 보세요. 다만 뭔가 시도하기 전에 반드시 사전에 제출하십시오. 제출한 이후, 다음과 같은 항목을 시도해볼 수 있습니다.\n",
        "\n",
        "- 다른 학습률과 반복 횟수로 플레이\n",
        "- 다른 초기화 방법을 시도하고 결과를 비교하십시오.\n",
        "- 다른 전처리로 테스트합니다 (데이터를 중앙에 배치하거나 각 행을 표준 편차로 나눕니다)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRIvhMALPq19"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52FuCacpPmEc"
      },
      "source": [
        "참고 문헌 :\n",
        "\n",
        "- http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\n",
        "- https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c"
      ]
    }
  ]
}