{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Planar_data_classification_with_onehidden_layer_v6c",
      "provenance": [],
      "collapsed_sections": [
        "Qm1JocK-1btb"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skfo763/Google-ML-Bootcamp-phase1/blob/main/week3/Planar_data_classification_with_onehidden_layer_v6c.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSIuImGUsD3f"
      },
      "source": [
        "# 하나의 은닉층을 활용한 평면 데이터 분류\n",
        "\n",
        "3주차 프로그래밍 과제에 오신 것을 환영합니다. 이번 과제에서는 여러분들은 처음으로, 1개의 은닉층을 가진 인공 신경망을 만들게 됩니다. 또 이 모델이 지난 2주차 과제에서 개발했던 로지스틱 회귀 모델과 비교했을 때 큰 차이점이 있다는 것도 알게 될 것입니다.\n",
        "\n",
        "**아래의 것들을 배웁니다** :\n",
        "- 한 개의 은닉층으로 주어진 데이터셋을 2 개의 서로 다른 클래스로 분류하는 인공 신경망을 만듭니다.\n",
        "- tanh와 같은 비선형 활성화 함수를 사용합니다.\n",
        "- 크로스 엔트로피 손실 함수를 계산합니다.\n",
        "- 순전파, 역전파를 구현합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6eZYHsMtHkR"
      },
      "source": [
        "## 1. 패키지 추가##\n",
        "본 과제를 수행하는 동안 필요한 파이썬 패키지를 다운로드 받아봅시다.\n",
        "\n",
        "- [numpy](www.numpy.org)는 파이썬의 가장 기본적인 과학/수학 연산 관련 패키지입니다.\n",
        "- [sklearn](http://scikit-learn.org/stable/)은 데이터 마이닝, 데이터 분석을 위한 간단하고 효율적인 도구를 제공해줍니다.\n",
        "- [matplotlib](http://matplotlib.org)은 파이썬 환경에서 그래프를 그릴 수 있도록 해주는 유명한 라이브러리입니다.\n",
        "- `testCases` 는 과제에서 구현한 함수가 잘 작동하는지 테스트할 수 있는 예시들을 제공합니다.\n",
        "- `planar_utils` 는 이 과제를 수행하는동안 필요한 다양한 함수들을 제공합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkmhulCNtEOi"
      },
      "source": [
        "# Package imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from testCases_v2 import *\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "import sklearn.linear_model\n",
        "from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(1) # set a seed so that the results are consistent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1WUr8HXt3jb"
      },
      "source": [
        "## 2. 데이터 셋 ##\n",
        "우선, 과제에 필요한 데이터셋을 다운로드해봅시다. 아래 코드를 실행하면 \"꽃\" 모양의 2 클래스 데이터셋 X, Y를 얻을 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMzB0hbouNUA"
      },
      "source": [
        "X, Y = load_planar_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPve1oURulW3"
      },
      "source": [
        "`matplotlib` 을 사용해 데이터셋을 시각화해봅시다. 데이터는 빨간 점(라벨 y가 0)과, 파란 점(라벨 y가 1)으로 이루어진 꽃 형상을 하고 있습니다. 이 과제의 목표는 이 데이터에 적합한 모델을 만드는 것입니다. 다시 말해, 우리는 이 데이터 중 빨간 점의 영역과 파란 점의 영역을 구분하는 모델을 만들고자 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrHyP6oZvDmx"
      },
      "source": [
        "# Visualize the data:\n",
        "plt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn-zzcrnvGKB"
      },
      "source": [
        "이 데이터 셋은\n",
        "- 각 데이터의 특성 (x1, x2) 으로 구성된 numpy-array (matrix) X\n",
        "- 라벨링된 결과값 Y (red: 0, blue: 1)로 구성된 numpy array (vector) Y\n",
        "\n",
        "로 이루어져 있습니다.\n",
        "\n",
        "이 데이터가 어떻게 생겼는지 좀 더 자세히 알아봅시다.\n",
        "\n",
        "**연습 문제**: 훈련 세트의 개수는 얼마나 될까요? 추가로, X, Y 변수의 shape는 무엇일까요?\n",
        "**힌트**: numpy array의 shape를 얻는 방법은 무엇일까요?([help](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.shape.html))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyHtABvrv697"
      },
      "source": [
        "### START CODE HERE ### (≈ 3 lines of code)\n",
        "shape_X = None\n",
        "shape_Y = None\n",
        "m = None  # training set size\n",
        "### END CODE HERE ###\n",
        "\n",
        "print ('The shape of X is: ' + str(shape_X))\n",
        "print ('The shape of Y is: ' + str(shape_Y))\n",
        "print ('I have m = %d training examples!' % (m))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCwBVrY5wdJu"
      },
      "source": [
        "**모범 답안**:\n",
        "       \n",
        "<table style=\"width:20%\">\n",
        "  <tr>\n",
        "    <td>shape of X</td>\n",
        "    <td> (2, 400) </td> \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>shape of Y</td>\n",
        "    <td>(1, 400) </td> \n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>m</td>\n",
        "    <td> 400 </td> \n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0Xnug_9wlem"
      },
      "source": [
        "## 3. 단일 로지스틱 회귀 ##\n",
        "\n",
        "전체 인공 신경망을 구현하기 이전에, 이 문제에 적합한 로지스틱 회귀 모델이 어떻게 동작하는지부터 확인해 보겠습니다. 저번 과제처럼 직접 구현하기보단, `sklearn` 라이브러리에서 미리 구현된 빌트인 함수를 사용하겠습니다. 아래 코드 블록을 실행시켜 주어진 데이터셋에 대한 로지스틱 회귀 분류기를 훈련시켜 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS4qRn97wfh_"
      },
      "source": [
        "# Train the logistic regression classifier\n",
        "clf = sklearn.linear_model.LogisticRegressionCV();\n",
        "clf.fit(X.T, Y.T);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4WOEx3RxKFL"
      },
      "source": [
        "이제 여러분은 이 모델에 대한 decision boundary를 그릴 수 있습니다. 아래 코드 블록을 실행시켜 보세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG5o1HvfxiwR"
      },
      "source": [
        "# Plot the decision boundary for logistic regression\n",
        "plot_decision_boundary(lambda x: clf.predict(x), X, Y)\n",
        "plt.title(\"Logistic Regression\")\n",
        "\n",
        "# Print accuracy\n",
        "LR_predictions = clf.predict(X.T)\n",
        "print ('Accuracy of logistic regression: %d ' % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))/float(Y.size)*100) +\n",
        "       '% ' + \"(percentage of correctly labelled datapoints)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XJIa9UQxnVx"
      },
      "source": [
        "**모범 답안**:\n",
        "\n",
        "<table style=\"width:20%\">\n",
        "  <tr>\n",
        "    <td>Accuracy</td>\n",
        "    <td> 47% </td> \n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "**해석**: 주어진 데이터셋 을 선형 함수로 분리할 수 없으므로, 로지스틱 회귀가 재대로 수행되지 않습니다. 인공신경망이 더 잘 작동할 수 있도록 이번 강의에서 배웠던 내용을 적용해봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5Qr6P_txvyp"
      },
      "source": [
        "## 4. 인공 신경망 모델 ##\n",
        "\n",
        "로지스틱 회귀는 주어진 꽃 형상의 데이터셋에는 잘 동작하지 않는것을 확인할 수 있었습니다. 이제 우리는 하나의 은닉층을 가지는 인공 신경망을 훈련시켜 볼 것입니다.\n",
        "\n",
        "**훈련시킬 모델 구조도** :\n",
        "<img src=\"arts/classification_kiank.png\" style=\"width:600px;height:300px;\">\n",
        "\n",
        "**모델의 수학적 구성** :\n",
        "\n",
        "한개의 데이터 $x^{(i)}$에 대하여,\n",
        "\n",
        "$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}\\tag{1}$$ \n",
        "$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$\n",
        "$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\\tag{3}$$\n",
        "$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$\n",
        "$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} > 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}$$\n",
        "\n",
        "모든 데이터 셋에 대하여 예측값을 계산한 이후, 아래의 정의대로 비용 함수 J를 계산할 수 있습니다:\n",
        "\n",
        "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$\n",
        "\n",
        "**리마인더** :\n",
        "1. 인공 신경망의 구조를 정의합니다 (입력 층의 개수, 은닉층의 개수, 등등..)\n",
        "2. 모델의 파라미터들을 초기화합니다.\n",
        "3. 아래의 동작을 반복합니다.\n",
        "  - 정방향 순전파\n",
        "  - 손실함수 계산\n",
        "  - 오차 역전파\n",
        "  - 파라미터 업데이트 (경사 하강법)\n",
        "\n",
        "1-3 단계를 계산하는 도우미 함수를 구현한 다음 `nn_model()` 이라고하는 하나의 함수로 통합하는 경우가 많습니다. `nn_model()` 을 구현하고 올바른 매개 변수를 학습 한 후에는 새 데이터에 대해 예측할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6LFpHN_0TzJ"
      },
      "source": [
        "### 4-1. 인공 신경망 구조 정의하기 ###\n",
        "\n",
        "**연습 문제**: 아래 세 가지 변수를 정의하시오:\n",
        "- `n_x` : 입력 층의 개수\n",
        "- `n_h` : 은닉 층의 개수\n",
        "- `n_y` : 출력 층의 개수\n",
        "\n",
        "**힌트**: 앞서 계산했던 데이터셋 X와 Y의 shape를 이용해서 n_x와 n_y를 찾아보세요, 물론 은닉 층의 개수 n_h는 4로 고정되어야 할 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrBrGUZCz_ym"
      },
      "source": [
        "# GRADED FUNCTION: layer_sizes\n",
        "\n",
        "def layer_sizes(X, Y):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- input dataset of shape (input size, number of examples)\n",
        "    Y -- labels of shape (output size, number of examples)\n",
        "    \n",
        "    Returns:\n",
        "    n_x -- the size of the input layer\n",
        "    n_h -- the size of the hidden layer\n",
        "    n_y -- the size of the output layer\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ### (≈ 3 lines of code)\n",
        "    n_x = None # size of input layer\n",
        "    n_h = None\n",
        "    n_y = None # size of output layer\n",
        "    ### END CODE HERE ###\n",
        "    return (n_x, n_h, n_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4Rch0ps0zND"
      },
      "source": [
        "X_assess, Y_assess = layer_sizes_test_case()\n",
        "(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess)\n",
        "print(\"The size of the input layer is: n_x = \" + str(n_x))\n",
        "print(\"The size of the hidden layer is: n_h = \" + str(n_h))\n",
        "print(\"The size of the output layer is: n_y = \" + str(n_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNV5RW8J0ytb"
      },
      "source": [
        "**모범 답안** (이 데이터는 실제로 인공 신경망에 사용될 데이터는 아닙니다).\n",
        "\n",
        "<table style=\"width:20%\">\n",
        "  <tr>\n",
        "    <td>n_x</td>\n",
        "    <td> 5 </td> \n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>n_h</td>\n",
        "    <td> 4 </td> \n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>n_y</td>\n",
        "    <td> 2 </td> \n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm1JocK-1btb"
      },
      "source": [
        "### 4-2. 모델의 파라미터 초기화하기 ###\n",
        "\n",
        "**연습 문제**: `intialize_parameters()` 를 구현해봅시다.\n",
        "\n",
        "**지시 사항** :\n",
        "- 파라미터의 사이즈가 올바르게 적용되었는지 확인하세요. 필요하다면 위의 신경망 구조도를 참조해도 좋습니다.\n",
        "- 각 layer의 가중치를 랜덤 값으로 초기화해야 합니다.\n",
        "  - `np.random.randn(a, b) * 0.01` 코드를 사용해 (a, b)의 형태를 가지고 있는 행렬을 랜덤으로 초기화할 수 있습니다.\n",
        "- bias 값을 0으로 초기화합니다.\n",
        "  - `np.zeros(a, b)` 코드를 사용해 (a, b) 형태의 행렬을 0으로 초기화할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izjQol-x1F_x"
      },
      "source": [
        "# GRADED FUNCTION: initialize_parameters\n",
        "\n",
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    n_x -- size of the input layer\n",
        "    n_h -- size of the hidden layer\n",
        "    n_y -- size of the output layer\n",
        "    \n",
        "    Returns:\n",
        "    params -- python dictionary containing your parameters:\n",
        "                    W1 -- weight matrix of shape (n_h, n_x)\n",
        "                    b1 -- bias vector of shape (n_h, 1)\n",
        "                    W2 -- weight matrix of shape (n_y, n_h)\n",
        "                    b2 -- bias vector of shape (n_y, 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random.\n",
        "    \n",
        "    ### START CODE HERE ### (≈ 4 lines of code)\n",
        "    W1 = None\n",
        "    b1 = None\n",
        "    W2 = None\n",
        "    b2 = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    assert (W1.shape == (n_h, n_x))\n",
        "    assert (b1.shape == (n_h, 1))\n",
        "    assert (W2.shape == (n_y, n_h))\n",
        "    assert (b2.shape == (n_y, 1))\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kci3pMHB4DPL"
      },
      "source": [
        "n_x, n_h, n_y = initialize_parameters_test_case()\n",
        "\n",
        "parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdwsK5a74Dsr"
      },
      "source": [
        "**모범 답안**:\n",
        "\n",
        "<table style=\"width:90%\">\n",
        "  <tr>\n",
        "    <td>W1</td>\n",
        "    <td> [[-0.00416758 -0.00056267]\n",
        " [-0.02136196  0.01640271]\n",
        " [-0.01793436 -0.00841747]\n",
        " [ 0.00502881 -0.01245288]] </td> \n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td>b1</td>\n",
        "    <td> [[ 0.]\n",
        " [ 0.]\n",
        " [ 0.]\n",
        " [ 0.]] </td> \n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td>W2</td>\n",
        "    <td> [[-0.01057952 -0.00909008  0.00551454  0.02292208]]</td> \n",
        "  </tr>\n",
        "  \n",
        "\n",
        "  <tr>\n",
        "    <td>b2</td>\n",
        "    <td> [[ 0.]] </td> \n",
        "  </tr>\n",
        "  \n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZuGcIIM4O9m"
      },
      "source": [
        "### 4-3. 반복하기 ###\n",
        "\n",
        "**연습 문제**: `forward_propagation()` 함수를 구현하세요.\n",
        "\n",
        "**지시 사항**:\n",
        "- 위에 나와 있는 수식을 참고하세요.\n",
        "- 이 주피터 노트북에 빌트인된 `sigmoid()` 함수를 사용할 수 있습니다.\n",
        "- numpy 라이브러리에 있는 `np.tanh()` 함수를 사용할 수 있습니다.\n",
        "- 구현 해야할 일련의 순서는\n",
        "  - 앞서 구현한 `initalize_parameters()` 함수의 리턴값인 `parameter` 인자로부터 모델의 파라미터를 얻으세요.\n",
        "  - Forward Propagation을 구현하세요. $Z^{[1]}, A^{[1]}, Z^{[2]}$와 $A^{[2]}$(모든 훈련 세트에 대한 예측 결과값을 담은 벡터) 를 계산하세요\n",
        "- Back Propagation에 사용될 값들은 \"cache\"에 저장됩니다. cache는 이후 구현할 back propagation 함수의 인자로 들어갈 예정입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZMTByT15PBG"
      },
      "source": [
        "# GRADED FUNCTION: forward_propagation\n",
        "\n",
        "def forward_propagation(X, parameters):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    X -- input data of size (n_x, m)\n",
        "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
        "    \n",
        "    Returns:\n",
        "    A2 -- The sigmoid output of the second activation\n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
        "    \"\"\"\n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "    ### START CODE HERE ### (≈ 4 lines of code)\n",
        "    W1 = None\n",
        "    b1 = None\n",
        "    W2 = None\n",
        "    b2 = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
        "    ### START CODE HERE ### (≈ 4 lines of code)\n",
        "    Z1 = None\n",
        "    A1 = None\n",
        "    Z2 = None\n",
        "    A2 = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    assert(A2.shape == (1, X.shape[1]))\n",
        "    \n",
        "    cache = {\"Z1\": Z1,\n",
        "             \"A1\": A1,\n",
        "             \"Z2\": Z2,\n",
        "             \"A2\": A2}\n",
        "    \n",
        "    return A2, cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RC6Yi0905iqV"
      },
      "source": [
        "X_assess, parameters = forward_propagation_test_case()\n",
        "A2, cache = forward_propagation(X_assess, parameters)\n",
        "\n",
        "# Note: we use the mean here just to make sure that your output matches ours. \n",
        "print(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h27-nsn45jJk"
      },
      "source": [
        "**모범 답안**:\n",
        "<table style=\"width:50%\">\n",
        "  <tr>\n",
        "    <td> 0.262818640198 0.091999045227 -1.30766601287 0.212877681719 </td> \n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-fVuC03-kNs"
      },
      "source": [
        "이제 각 샘플 데이터의 결과값인 $ a ^ {[2] (i)} $들을 모두 포함하는 $ A ^ {[2]} $ (Python 변수 \"ʻA2`\"에서)를 계산 했으므로, 우리는 다음과 같이 비용 함수를 계산할 수 있습니다. :\n",
        "\n",
        "$$J = - \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{13}$$\n",
        "\n",
        "**연습 문제**: cost $J$ 를 계산하는 `compute_cost()` 함수를 구현하세요\n",
        "\n",
        "**지시 사항**: \n",
        "- 크로스 엔트로피 손실함수를 구현하는데 다양한 방법을 사용할 수 있습니다. 약간의 도움을 주기 위해, 우리가 $- \\sum\\limits_{i=0}^{m}  y^{(i)}\\log(a^{[2](i)})$를 어떻게 구현했는지 알려드리겠습니다:\n",
        "```python\n",
        "logprobs = np.multiply(np.log(A2),Y)\n",
        "cost = - np.sum(logprobs)                # no need to use a for loop!\n",
        "```\n",
        "\n",
        "여러분은 `np.multiply()` 함수를 적용한 다음 `np.sum()`을 사용하거나, 혹은 다이렉트로 `np.dot()` 함수를 사용할 수 있습니다. \n",
        "\n",
        "`np.multiply()` 다음에 `np.sum()` 을 사용하면 최종 결과는 `float` 유형이 되고, `np.dot()` 을 사용하면 결과는 2차원 numpy array가 됩니다. \n",
        "\n",
        "`np.squeeze()`를 사용하여 필요없는 중복된 dimensions를 제거할 수 있습니다. 단일 `float` 값의 경우라면, 0차원 배열로 축소됩니다. `float()` 함수를 이용하면 배열을 `float` 유형으로 캐스팅 할 수도 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWJV-OEP5mEV"
      },
      "source": [
        "# GRADED FUNCTION: compute_cost\n",
        "\n",
        "def compute_cost(A2, Y, parameters):\n",
        "    \"\"\"\n",
        "    Computes the cross-entropy cost given in equation (13)\n",
        "    \n",
        "    Arguments:\n",
        "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
        "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
        "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
        "    [Note that the parameters argument is not used in this function, \n",
        "    but the auto-grader currently expects this parameter.\n",
        "    Future version of this notebook will fix both the notebook \n",
        "    and the auto-grader so that `parameters` is not needed.\n",
        "    For now, please include `parameters` in the function signature,\n",
        "    and also when invoking this function.]\n",
        "    \n",
        "    Returns:\n",
        "    cost -- cross-entropy cost given equation (13)\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    m = Y.shape[1] # number of example\n",
        "\n",
        "    # Compute the cross-entropy cost\n",
        "    ### START CODE HERE ### (≈ 2 lines of code)\n",
        "    logprobs = None\n",
        "    cost = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. \n",
        "                                    # E.g., turns [[17]] into 17 \n",
        "    assert(isinstance(cost, float))\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKA29gp8fb7T"
      },
      "source": [
        "A2, Y_assess, parameters = compute_cost_test_case()\n",
        "\n",
        "print(\"cost = \" + str(compute_cost(A2, Y_assess, parameters)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M9b1vk9fcig"
      },
      "source": [
        "**모범 답안**:\n",
        "<table style=\"width:20%\">\n",
        "  <tr>\n",
        "    <td>cost</td>\n",
        "    <td> 0.693058761... </td> \n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohjks-C2g-hN"
      },
      "source": [
        "앞서 forward propagation을 구현하는 과정에서 값을 저장했던 `cache` 변수를 활용해서 backward propagation을 구현해봅시다.\n",
        "\n",
        "**연습 문제**: `backward_propagation()` 함수를 구현하세요\n",
        "\n",
        "**지시 사항**: Backpropagation은 일반적으로 딥 러닝 분야에서 수학적으로 가장 어려운 부분입니다. 당신을 돕기 위해서, 이번 강의에서 사용했던 슬라이드를 첨부하겠습니다. 이번 과제에선 벡터화된 방식으로 모델을 학습하고 있으므로 슬라이드 오른쪽에 있는 6개의 공식을 참조하는 것이 좋을 것입니다.\n",
        "\n",
        "<img src=\"arts/grad_summary.png\" style=\"width:600px;height:300px;\">\n",
        "\n",
        "<!--\n",
        "$\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } = \\frac{1}{m} (a^{[2](i)} - y^{(i)})$\n",
        "\n",
        "$\\frac{\\partial \\mathcal{J} }{ \\partial W_2 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } a^{[1] (i) T} $\n",
        "\n",
        "$\\frac{\\partial \\mathcal{J} }{ \\partial b_2 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)}}}$\n",
        "\n",
        "$\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} } =  W_2^T \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } * ( 1 - a^{[1] (i) 2}) $\n",
        "\n",
        "$\\frac{\\partial \\mathcal{J} }{ \\partial W_1 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} }  X^T $\n",
        "\n",
        "$\\frac{\\partial \\mathcal{J} _i }{ \\partial b_1 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)}}}$\n",
        "\n",
        "- Note that $*$ denotes elementwise multiplication.\n",
        "- The notation you will use is common in deep learning coding:\n",
        "    - dW1 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_1 }$\n",
        "    - db1 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_1 }$\n",
        "    - dW2 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_2 }$\n",
        "    - db2 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_2 }$\n",
        "!-->\n",
        "\n",
        "- 팁:\n",
        "  - dZ1을 계산하기 위해서, $g^{[1]'}(Z^{[1]})$를 계산해야 합니다. $g^{[1]}(.)$는 할성화 함수이므로, $ a = g ^ {[1]} (z) $이면 $ g ^ {[1] '} (z) = 1- a ^ 2 $입니다. 따라서 $g^{[1]'}(Z^{[1]})$값을 `(1 - np.power(A1, 2))` 코드를 사용해 계산할 수 있습니다\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZI0HHYoixXi"
      },
      "source": [
        "# GRADED FUNCTION: backward_propagation\n",
        "\n",
        "def backward_propagation(parameters, cache, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation using the instructions above.\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing our parameters \n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
        "    X -- input data of shape (2, number of examples)\n",
        "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
        "    \n",
        "    Returns:\n",
        "    grads -- python dictionary containing your gradients with respect to different parameters\n",
        "    \"\"\"\n",
        "    m = X.shape[1]\n",
        "    \n",
        "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
        "    ### START CODE HERE ### (≈ 2 lines of code)\n",
        "    W1 = None\n",
        "    W2 = None\n",
        "    ### END CODE HERE ###\n",
        "        \n",
        "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
        "    ### START CODE HERE ### (≈ 2 lines of code)\n",
        "    A1 = None\n",
        "    A2 = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
        "    ### START CODE HERE ### (≈ 6 lines of code, corresponding to 6 equations on slide above)\n",
        "    dZ2 = None\n",
        "    dW2 = None\n",
        "    db2 = None\n",
        "    dZ1 = None\n",
        "    dW1 = None\n",
        "    db1 = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    grads = {\"dW1\": dW1,\n",
        "             \"db1\": db1,\n",
        "             \"dW2\": dW2,\n",
        "             \"db2\": db2}\n",
        "    \n",
        "    return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PX8v6lSMixsn"
      },
      "source": [
        "parameters, cache, X_assess, Y_assess = backward_propagation_test_case()\n",
        "\n",
        "grads = backward_propagation(parameters, cache, X_assess, Y_assess)\n",
        "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
        "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
        "print (\"dW2 = \"+ str(grads[\"dW2\"]))\n",
        "print (\"db2 = \"+ str(grads[\"db2\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Phh81ezSiyBs"
      },
      "source": [
        "**모범 답안**:\n",
        "\n",
        "<table style=\"width:80%\">\n",
        "  <tr>\n",
        "    <td>dW1</td>\n",
        "    <td> [[ 0.00301023 -0.00747267]\n",
        " [ 0.00257968 -0.00641288]\n",
        " [-0.00156892  0.003893  ]\n",
        " [-0.00652037  0.01618243]] </td> \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>db1</td>\n",
        "    <td>  [[ 0.00176201]\n",
        " [ 0.00150995]\n",
        " [-0.00091736]\n",
        " [-0.00381422]] </td> \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>dW2</td>\n",
        "    <td> [[ 0.00078841  0.01765429 -0.00084166 -0.01022527]] </td> \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>db2</td>\n",
        "    <td> [[-0.16655712]] </td> \n",
        "  </tr>\n",
        "  \n",
        "</table>  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwOP7VnfqcM7"
      },
      "source": [
        "**연습 문제**: 파라미터를 업데이트하는 코드를 구현하세요. 경사 하강법을 사용합니다. 다시 말해 (W1, b1, W2, b2)를 업데이트하려면 (dW1, db1, dW2, db2)를 사용해야 합니다.\n",
        "\n",
        "**경사 하강법 공식**: $ \\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$ ($\\alpha$ = 학습률, $\\theta$ = 파라미터)\n",
        "\n",
        "**Illustration**: 아래 gif 파일은 좋은 학습률 (수렴하는) 을 가진 경사 하강법 알고리즘과 나쁜 학습률 (발산하는)을 가진 경사 하강법 알고리즘을 나타냅니다.\n",
        "\n",
        "이미지 제공 : Adam Harley\n",
        "\n",
        "<img src=\"arts/sgd.gif\" style=\"width:400;height:400;\">\n",
        "\n",
        "<img src=\"arts/sgd_bad.gif\" style=\"width:400;height:400;\">\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66Qu_6AZrDew"
      },
      "source": [
        "# GRADED FUNCTION: update_parameters\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
        "    \"\"\"\n",
        "    Updates parameters using the gradient descent update rule given above\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients \n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "    \"\"\"\n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "    ### START CODE HERE ### (≈ 4 lines of code)\n",
        "    W1 = None\n",
        "    b1 = None\n",
        "    W2 = None\n",
        "    b2 = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Retrieve each gradient from the dictionary \"grads\"\n",
        "    ### START CODE HERE ### (≈ 4 lines of code)\n",
        "    dW1 = None\n",
        "    db1 = None\n",
        "    dW2 = None\n",
        "    db2 = None\n",
        "    ## END CODE HERE ###\n",
        "    \n",
        "    # Update rule for each parameter\n",
        "    ### START CODE HERE ### (≈ 4 lines of code)\n",
        "    W1 = None\n",
        "    b1 = None\n",
        "    W2 = None\n",
        "    b2 = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqjjJSlSzfsV"
      },
      "source": [
        "parameters, grads = update_parameters_test_case()\n",
        "parameters = update_parameters(parameters, grads)\n",
        "\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTsGaQmDzi5f"
      },
      "source": [
        "**모범 답안**:\n",
        "\n",
        "<table style=\"width:80%\">\n",
        "  <tr>\n",
        "    <td>W1</td>\n",
        "    <td> [[-0.00643025  0.01936718]\n",
        " [-0.02410458  0.03978052]\n",
        " [-0.01653973 -0.02096177]\n",
        " [ 0.01046864 -0.05990141]]</td> \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>b1</td>\n",
        "    <td> [[ -1.02420756e-06]\n",
        " [  1.27373948e-05]\n",
        " [  8.32996807e-07]\n",
        " [ -3.20136836e-06]]</td> \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>W2</td>\n",
        "    <td> [[-0.01041081 -0.04463285  0.01758031  0.04747113]] </td> \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>b2</td>\n",
        "    <td> [[ 0.00010457]] </td> \n",
        "  </tr>\n",
        "</table>  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAbYkN2I0sbw"
      },
      "source": [
        "### 4-4. nn_model() 함수로 일련의 과정을 통합하기 ###\n",
        "\n",
        "**연습 문제**: 인공신경망 `nn_model()` 함수를 구현하세요\n",
        "\n",
        "**지시 사항**: 앞서 구현한 함수들을 올바른 순서대로 배치하세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQHqqW1szr1-"
      },
      "source": [
        "# GRADED FUNCTION: nn_model\n",
        "\n",
        "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- dataset of shape (2, number of examples)\n",
        "    Y -- labels of shape (1, number of examples)\n",
        "    n_h -- size of the hidden layer\n",
        "    num_iterations -- Number of iterations in gradient descent loop\n",
        "    print_cost -- if True, print the cost every 1000 iterations\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(3)\n",
        "    n_x = layer_sizes(X, Y)[0]\n",
        "    n_y = layer_sizes(X, Y)[2]\n",
        "    \n",
        "    # Initialize parameters\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    parameters = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "         \n",
        "        ### START CODE HERE ### (≈ 4 lines of code)\n",
        "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
        "        A2, cache = None\n",
        "        \n",
        "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
        "        cost = None\n",
        " \n",
        "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
        "        grads = None\n",
        " \n",
        "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
        "        parameters = None\n",
        "        \n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Print the cost every 1000 iterations\n",
        "        if print_cost and i % 1000 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxjY7VWS1JcO"
      },
      "source": [
        "X_assess, Y_assess = nn_model_test_case()\n",
        "parameters = nn_model(X_assess, Y_assess, 4, num_iterations=10000, print_cost=True)\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoZ-Ezo71KXq"
      },
      "source": [
        "**Expected Output**:\n",
        "\n",
        "<table style=\"width:90%\">\n",
        "<tr> \n",
        "    <td> \n",
        "        cost after iteration 0\n",
        "    </td>\n",
        "    <td> \n",
        "        0.692739\n",
        "    </td>\n",
        "</tr>\n",
        "<tr> \n",
        "    <td> \n",
        "        <center> $\\vdots$ </center>\n",
        "    </td>\n",
        "    <td> \n",
        "        <center> $\\vdots$ </center>\n",
        "    </td>\n",
        "</tr>\n",
        "  <tr>\n",
        "    <td>W1</td>\n",
        "    <td> [[-0.65848169  1.21866811]\n",
        " [-0.76204273  1.39377573]\n",
        " [ 0.5792005  -1.10397703]\n",
        " [ 0.76773391 -1.41477129]]</td> \n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td>b1</td>\n",
        "    <td> [[ 0.287592  ]\n",
        " [ 0.3511264 ]\n",
        " [-0.2431246 ]\n",
        " [-0.35772805]] </td> \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>W2</td>\n",
        "    <td> [[-2.45566237 -3.27042274  2.00784958  3.36773273]] </td> \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>b2</td>\n",
        "    <td> [[ 0.20459656]] </td> \n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7AwEwRh2EmQ"
      },
      "source": [
        "### 4-5. 예측하기 ###\n",
        "\n",
        "**연습 문제** : 지금까지 개발한 모델을 바탕으로, 주어진 문제를 예측하는 `predict()` 함수를 만들어봅시다. 예측을 위해서는 forward propagation을 사용하면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANmXqC_R28Vw"
      },
      "source": [
        "**기억할 것**\n",
        "\n",
        "$y_{prediction} = \\mathbb 1 \\text{{activation > 0.5}} = \\begin{cases}\n",
        "      1 & \\text{if}\\ activation > 0.5 \\\\\n",
        "      0 & \\text{otherwise}\n",
        "    \\end{cases}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMLALmlv3Toj"
      },
      "source": [
        "예를 들어, 임계치에 따라 행렬 X의 각 항목을 0과 1로 설정하려면 아래를 수행합니다. ```X_new = (X > threshold)```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iiRkVha2fal"
      },
      "source": [
        "# GRADED FUNCTION: predict\n",
        "\n",
        "def predict(parameters, X):\n",
        "    \"\"\"\n",
        "    Using the learned parameters, predicts a class for each example in X\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    X -- input data of size (n_x, m)\n",
        "    \n",
        "    Returns\n",
        "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
        "    ### START CODE HERE ### (≈ 2 lines of code)\n",
        "    A2, cache = None\n",
        "    predictions = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQu2ZC6L37Y6"
      },
      "source": [
        "parameters, X_assess = predict_test_case()\n",
        "\n",
        "predictions = predict(parameters, X_assess)\n",
        "print(\"predictions mean = \" + str(np.mean(predictions)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PTy4zJd39Sp"
      },
      "source": [
        "**모범 답안**: \n",
        "<table style=\"width:40%\">\n",
        "  <tr>\n",
        "    <td>predictions mean</td>\n",
        "    <td> 0.666666666667 </td> \n",
        "  </tr>\n",
        "  \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM_TuJcH4d9b"
      },
      "source": [
        "모델을 실행하고 평면형 데이터 세트에서 어떻게 수행되는지 살펴볼 시간입니다. 다음 코드를 실행하여 $n_h$ 숨겨진 유닛의 단일 숨겨진 레이어로 모델을 테스트하십시오."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc3ImW5Q4Beq"
      },
      "source": [
        "# Build a model with a n_h-dimensional hidden layer\n",
        "parameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)\n",
        "\n",
        "# Plot the decision boundary\n",
        "plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n",
        "plt.title(\"Decision Boundary for hidden layer size \" + str(4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpqQl93g4k-W"
      },
      "source": [
        "**모범 답안**:\n",
        "<table style=\"width:40%\">\n",
        "  <tr>\n",
        "    <td>Cost after iteration 9000</td>\n",
        "    <td> 0.218607 </td> \n",
        "  </tr>\n",
        "  \n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV0SDbmC4ogV"
      },
      "source": [
        "# Print accuracy\n",
        "predictions = predict(parameters, X)\n",
        "print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFyNa4b94sa7"
      },
      "source": [
        "**모범 답안**: \n",
        "\n",
        "<table style=\"width:15%\">\n",
        "  <tr>\n",
        "    <td>Accuracy</td>\n",
        "    <td> 90% </td> \n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yvDRiNW4yeY"
      },
      "source": [
        "로지스틱 회귀에 비해 정확도가 정말 높습니다. 모델은 꽃잎 패턴의 데이터 분포를 학습했습니다. 신경망은 로지스틱 회귀와 달리 매우 비선형적인 decision boundaries 까지 학습 할 수 있습니다.\n",
        "\n",
        "이제 다양한 은닉 층의 사이즈를 사용해 보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thXBinEg5ASN"
      },
      "source": [
        "### 4-6. 은닉 층의 크기 조정하기(선택 사항/평가 X) ###\n",
        "\n",
        "다음 코드를 실행해보세요. 1-2분이 소요될 수 있습니다. 다양한 은닉층 크기에 대해 모델의 다양한 동작을 살펴볼 수 있습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSgX6zqS4vCr"
      },
      "source": [
        "# This may take about 2 minutes to run\n",
        "\n",
        "plt.figure(figsize=(16, 32))\n",
        "hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]\n",
        "for i, n_h in enumerate(hidden_layer_sizes):\n",
        "    plt.subplot(5, 2, i+1)\n",
        "    plt.title('Hidden Layer of size %d' % n_h)\n",
        "    parameters = nn_model(X, Y, n_h, num_iterations = 5000)\n",
        "    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n",
        "    predictions = predict(parameters, X)\n",
        "    accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)\n",
        "    print (\"Accuracy for {} hidden units: {} %\".format(n_h, accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrnVJYAl5YNJ"
      },
      "source": [
        "**해석**:\n",
        "- 더 큰 모델 (은닉층이 더 많음)은 결국 가장 큰 모델이 데이터를 과적합 할 때까지 훈련 세트에 더 잘 맞을 수 있습니다.\n",
        "- 가장 좋은 은닉층 크기는 n_h = 5 정도 인 것 같습니다. 실제로 여기 주변의 값은 눈에 띄는 과적 합없이 데이터에 잘 맞는 것처럼 보입니다.\n",
        "- 나중에 과적 합없이 매우 큰 모델 (예 : n_h = 50)을 사용할 수있는 정규화에 대해서도 배우게됩니다.\n",
        "\n",
        "**추가 질문**:\n",
        "\n",
        "참고 : 오른쪽 상단의 파란색 \"과제 제출\" 버튼을 클릭하여 과제를 제출해야합니다.\n",
        "\n",
        "원하는 경우 아래의 몇 가지 선택적인 질문들의 답을 찾아보세요\n",
        "\n",
        "- tanh 활성화를 sigmoid, 혹은 ReLU 활성화 함수로 변경하면 어떻게 될까요?\n",
        "- learning_rate를 조정해보세요. 무슨 일이 일어날까요?\n",
        "- 데이터 세트를 변경하면 어떻게 되나요? (아래 5 부 참조!)\n",
        "\n",
        "**이번 과제에선 아래 내용을 배웠습니다.**\n",
        "\n",
        "- 은닉층을 사용한 완전한 신경망 구축\n",
        "- 비선형 unit을 잘 활용할 수 있게 되었습니다.\n",
        "- 순방향 전파 및 역 전파를 구현\n",
        "- 은닉 레이어 크기를 다양화할 때의 영향을 알 수 있게 되었습니다.\n",
        "\n",
        "수고하셨습니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1L1v7SSD58RW"
      },
      "source": [
        "## 5. 다른 데이터셋을 활용해보기 ##\n",
        "\n",
        "원하는 경우 다음 데이터 세트 각각에 대해 전체 학습 과정을 다시 수행할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_kVeKiz54i5"
      },
      "source": [
        "# Datasets\n",
        "noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()\n",
        "\n",
        "datasets = {\"noisy_circles\": noisy_circles,\n",
        "            \"noisy_moons\": noisy_moons,\n",
        "            \"blobs\": blobs,\n",
        "            \"gaussian_quantiles\": gaussian_quantiles}\n",
        "\n",
        "### START CODE HERE ### (choose your dataset)\n",
        "dataset = \"noisy_moons\"\n",
        "### END CODE HERE ###\n",
        "\n",
        "X, Y = datasets[dataset]\n",
        "X, Y = X.T, Y.reshape(1, Y.shape[0])\n",
        "\n",
        "# make blobs binary\n",
        "if dataset == \"blobs\":\n",
        "    Y = Y%2\n",
        "\n",
        "# Visualize the data\n",
        "plt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek53ugxi6h3v"
      },
      "source": [
        "본 프로그래밍 과제를 완료하신 것을 축하드립니다!\n",
        "\n",
        "참고 문헌:\n",
        "- http://scs.ryerson.ca/~aharley/neural-networks/\n",
        "- http://cs231n.github.io/neural-networks-case-study/"
      ]
    }
  ]
}