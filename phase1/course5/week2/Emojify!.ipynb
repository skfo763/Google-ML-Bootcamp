{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emojify! ",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skfo763/Google-ML-Bootcamp-phase1/blob/main/course5/week2/Emojify!.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-Jrj5I5NOsL"
      },
      "source": [
        "# Emojify! \n",
        "\n",
        "2ì£¼ì°¨ ë‘ ë²ˆì¨° ê³¼ì œì— ì˜¤ì‹  ì—¬ëŸ¬ë¶„ì„ í™˜ì˜í•©ë‹ˆë‹¤! ì´ ê³¼ì œì—ì„œ ì—¬ëŸ¬ë¶„ì€ ë‹¨ì–´ ë²¡í„° í‘œí˜„ì„ ì‚¬ìš©í•´ì„œ Emojifier(ì´ëª¨ì§€ ë³€í˜• ì•Œê³ ë¦¬ì¦˜)ì„ ë§Œë“¤ ê²ƒì…ë‹ˆë‹¤.\n",
        "\n",
        "ì¹´ì¹´ì˜¤í†¡ê³¼ ê°™ì€ í…ìŠ¤íŠ¸ ë©”ì‹œì§€ë¥¼ ë³´ë‚¼ë•Œ, ë” ì˜ˆì˜ê²Œ ë³´ë‚´ê³  ì‹¶ì§€ ì•Šë‚˜ìš”? ì§€ê¸ˆë¶€í„° ë§Œë“¤ emojifier ì•±ì€ ì´ ì‘ì—…ì„ ë„ì™€ì¤ë‹ˆë‹¤. ë”°ë¼ì„œ ì•„ë˜ì™€ ê°™ì€ í…ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•˜ê¸°ë³´ë‹¨,\n",
        "> \"Congratulations on the promotion! Let's get coffee and talk. Love you!\"\n",
        "\n",
        "ë‹¤ìŒê³¼ ê°™ì´ ìë™ìœ¼ë¡œ ì´ëª¨ì§€ë¡œ ë³€ê²½ëœ í…ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "> \"Congratulations on the promotion! ğŸ‘ Let's get coffee and talk. â˜•ï¸ Love you! â¤ï¸\"\n",
        "\n",
        "- íŠ¹ì • ë¬¸ì¥(ì˜ˆë¥¼ ë“¤ì–´, \"Let's go see the baseball game tonight!\")ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ì„œ, ê°€ì¥ ì ì ˆí•œ ì´ëª¨ì§€(ìœ„ì˜ ì˜ˆì‹œì—ì„œ, âš¾ï¸)ë¥¼ ìë™ìœ¼ë¡œ ì°¾ì•„ì£¼ëŠ” ëª¨ë¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkrZyoprODyj"
      },
      "source": [
        "#### Using word vectors to improve emoji lookups\n",
        "\n",
        "- ëŒ€ë¶€ë¶„ì˜ ì´ëª¨ì§€ ì¸í„°í˜ì´ìŠ¤ì—ì„œ, â¤ï¸ì´ëª¨ì§€ëŠ” \"love\"ë¼ëŠ” ì‹¬ë³¼ë³´ë‹¤ëŠ” \"heart\"ë¼ëŠ” ì‹¬ë³¼ë¡œ í‘œí˜„ëœë‹¤ëŠ” ê²ƒì„ ê¸°ì–µí•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "  - ë‹¤ì‹œ ë§í•´, ìœ„ì˜ ì´ëª¨ì§€ë¥¼ ì–»ê¸° ìœ„í•´ì„œëŠ” \"heart\"ë¼ëŠ” í…ìŠ¤íŠ¸ë¥¼ íƒ€ì´í•‘í•´ì•¼ í•˜ë©°, \"love\"ë¥¼ íƒ€ì´í•‘í•˜ë©´ í•´ë‹¹ ì´ëª¨ì§€ê°€ ë‚˜ì˜¤ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì„ ê¸°ì–µí•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "- word vectorsë¥¼ ì‚¬ìš©í•˜ë©´ ë³´ë‹¤ ìœ ì—°í•œ ì´ëª¨ì§€ ì¸í„°í˜ì´ìŠ¤ë¥¼ ë§Œë“¤ ìˆ˜ ì´ìŠµë‹ˆë‹¤.\n",
        "- word vectorë¥¼ ì‚¬ìš©í•  ë•Œ, ì—¬ëŸ¬ë¶„ì˜ í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ê°€ íŠ¹ì • ì´ëª¨ì§€ì˜ ì¼ë¶€ì™€ë§Œ ì¼ì¹˜í•œë‹¤ê³  í•˜ë”ë¼ë„, ì—¬ëŸ¬ë¶„ì´ êµ¬í˜„í•œ ì•Œê³ ë¦¬ì¦˜ì€ ì¶”ê°€ì ì¸ ì—¬ëŸ¬ ë‹¨ì–´ë“¤ì˜ ì—°ê´€ì„±ì„ ì°¾ì•„ ì¼ë°˜í™”í•˜ì—¬, ê°™ì€ ì´ëª¨ì§€ë¥¼ ì–»ì„ ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\n",
        "  - ì´ ì‘ì—…ì€ í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ì— ì—†ëŠ” ë‹¨ì–´ì— ëŒ€í•´ì„œë„ ì‘ë™í•©ë‹ˆë‹¤.\n",
        "  - ì´ëŠ” ì‘ì€ í›ˆë ¨ ë°ì´í„° ì…‹ë§Œì„ ê°€ì§€ê³ ë„ ë¬¸ì¥ì„ ì´ëª¨ì§€ë¡œ ë³€í™˜ì‹œì¼œì£¼ëŠ” ì •í™•í•œ ì•Œê³ ë¦¬ì¦˜ì„ ë§Œë“¤ ìˆ˜ ìˆë„ë¡ í•´ì¤ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooMPf4KGO-eo"
      },
      "source": [
        "#### What you'll build\n",
        "\n",
        "1. ì´ë²ˆ ê³¼ì œì—ì„œ, ì—¬ëŸ¬ë¶„ì€ word embeddingì„ ì‚¬ìš©í•œ ê¸°ë³¸ì ì¸ ëª¨ë¸(Emojifier-V1)ì„ ë§Œë“­ë‹ˆë‹¤.\n",
        "2. ì´í›„ LSTM ì•Œê³ ë¦¬ì¦˜ê³¼ í†µí•©í•˜ì—¬ (1)ë²ˆ ëª¨ë¸ë³´ë‹¤ ë³´ë‹¤ ì •êµí•œ ëª¨ë¸(Emojifier-V2)ë¥¼ ë§Œë“­ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n8zijwaPMWT"
      },
      "source": [
        "ì´ì œ ì‹œì‘í•´ë´…ì‹œë‹¤! ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰ì‹œì¼œ, í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AktBIQegPNef"
      },
      "source": [
        "import numpy as np\n",
        "from emo_utils import *\n",
        "import emoji\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-p6ahywPMNn"
      },
      "source": [
        "## 1 - Baseline model: Emojifier-V1\n",
        "\n",
        "### 1.1 - Dataset EMOJISET\n",
        "\n",
        "ê°€ì¥ ê¸°ë³¸ì ì¸ ì´ëª¨ì§€ ë¶„ë¥˜ê¸°ë¥¼ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "ì´ë²ˆì— ì œê³µë˜ëŠ” ë°ì´í„°ì…‹ (X,Y)ëŠ” ê½¤ ì‘ì€ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.\n",
        "- XëŠ” 127ê°œì˜ ë¬¸ì¥(ë¬¸ìì—´)ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤\n",
        "- YëŠ” 0ë¶€í„° 4ê¹Œì§€ ê° ë¬¸ì¥ì— ëŒ€ì‘ë˜ëŠ” ì´ëª¨ì§€ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "<img src=\"arts/data_set.png\" style=\"width:700px;height:300px;\">\n",
        "<center>ê·¸ë¦¼ 1: EMOJISET - \n",
        "5 ê°œ í´ë˜ìŠ¤ì˜ ë¶„ë¥˜ ë¬¸ì œ. ì—¬ê¸°ì— ëª‡ ê°€ì§€ ë¬¸ì¥ì— ëŒ€í•œ ì˜ˆì‹œê°€ ë‚˜ì™€ ìˆìŠµë‹ˆë‹¤.</center>\n",
        "\n",
        "\n",
        "ì•„ë˜ ì½”ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë¶ˆëŸ¬ì™€ ë³´ê² ìŠµë‹ˆë‹¤. í›ˆë ¨ (127 ê°œ ì˜ˆì‹œ)ê³¼ í…ŒìŠ¤íŠ¸ (56 ê°œ ì˜ˆì‹œ)ë¡œ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë¶„í• í–ˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx8MDgG3ODF7"
      },
      "source": [
        "X_train, Y_train = read_csv('data/train_emoji.csv')\n",
        "X_test, Y_test = read_csv('data/tesss.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMOKNOY7P2v5"
      },
      "source": [
        "maxLen = len(max(X_train, key=len).split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GXIUGsvP6Ef"
      },
      "source": [
        "ë‹¤ìŒ ì…€ì„ ì‹¤í–‰í•˜ì—¬ X_trainì˜ ë¬¸ì¥ê³¼ Y_trainì˜ í•´ë‹¹ ë ˆì´ë¸”ì„ ì¸ì‡„í•©ë‹ˆë‹¤.\n",
        "\n",
        "* ë‹¤ë¥¸ ì˜ˆë¥¼ ë³´ë ¤ë©´ `idx`ë¥¼ ë³€ê²½í•˜ì‹­ì‹œì˜¤.\n",
        "* iPython ë…¸íŠ¸ë¶ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê¸€ê¼´ë¡œ ì¸í•´ í•˜íŠ¸ ì´ëª¨í‹°ì½˜ì´ ë¹¨ê°„ìƒ‰ì´ ì•„ë‹Œ ê²€ì€ ìƒ‰ìœ¼ë¡œ í‘œì‹œ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-mUAKl5P-cw"
      },
      "source": [
        "for idx in range(10):\n",
        "    print(X_train[idx], label_to_emoji(Y_train[idx]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG10zJIcQCEA"
      },
      "source": [
        "### 1.2 - Overview of the Emojifier-V1\n",
        "\n",
        "ì´ íŒŒíŠ¸ì—ì„œ, ì—¬ëŸ¬ë¶„ì€ \"Emojifer-v1\"ì´ë¼ëŠ” ê¸°ë³¸ì ì¸ ëª¨ë¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
        "\n",
        "<img src=\"arts/image_1.png\" style=\"width:900px;height:300px;\">\n",
        "<center>ê·¸ë¦¼ 2 : Baseline model(Emojifier-V1)</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYs-wYudQO-a"
      },
      "source": [
        "#### Inputs and outputs\n",
        "- ì´ ëª¨ë¸ì˜ ì…ë ¥ì€ íŠ¹ì • ë¬¸ì¥ì„ ë‚˜íƒ€ë‚´ëŠ” ë¬¸ìì—´ì…ë‹ˆë‹¤(ì˜ˆë¥¼ ë“¤ì–´, \"I love you\")\n",
        "- ì¶œë ¥ì€ shapeê°€ (1,5)ì¸ í™•ë¥  ë²¡í„°ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì„ íƒí•  ìˆ˜ ìˆëŠ” ì´ëª¨ì§€ì˜ ì¢…ë¥˜ë¡œ ì´ 5ê°œê°€ ìˆìŠµë‹ˆë‹¤.\n",
        "- (1,5) shapeì˜ í™•ë¥  ë°±í„°ëŠ” argmax ë ˆì´ì–´ë¥¼ í†µê³¼í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ ì´ëª¨ì§€ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo7oEzJxQmaC"
      },
      "source": [
        "#### One-hot encoding\n",
        "- ë¼ë²¨ë§ëœ ë°ì´í„°ë¥¼ softmax ë¶„ë¥˜ í•¨ìˆ˜ì— ì í•©í•˜ê²Œ ë§ì¶”ê¸° ìœ„í•´ì„œ, $(m, 1)$ì˜ shapeë¥¼ í•˜ê³  ìˆëŠ” $Y$ ë°ì´í„°ë¥¼ $(m, 5)$ í˜•íƒœì˜ one-hot í‘œí˜„ ë°©ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "  - ê° í–‰ì€ ë°ì´í„° ì…‹ í•œ ê°œì˜ ë ˆì´ë¸”ì„ ì œê³µí•˜ëŠ” one-hot ë²¡í„°ì…ë‹ˆë‹¤.\n",
        "  - ì—¬ê¸°ì„œ `Y_oh`ëŠ” ë³€ìˆ˜ ì´ë¦„ `Y_oh_train`ë° `Y_oh_test`ì—ì„œ \"Y-one-hot\"ì„ ì˜ë¯¸í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1-Ux7gcQOh9"
      },
      "source": [
        "Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
        "Y_oh_test = convert_to_one_hot(Y_test, C = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi-fGDfpRF4W"
      },
      "source": [
        "`convert_to_one_hot()` í•¨ìˆ˜ê°€ ë¬´ì—‡ì„ í•˜ëŠ”ì§€ ì‚´í´ ë³´ê² ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê°’ì„ ì¸ì‡„í•˜ë ¤ë©´ `index`ë¥¼ ììœ ë¡­ê²Œ ë³€ê²½í•˜ì‹­ì‹œì˜¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7ppn0pnRLU-"
      },
      "source": [
        "idx = 50\n",
        "print(f\"Sentence '{X_train[50]}' has label index {Y_train[idx]}, which is emoji {label_to_emoji(Y_train[idx])}\", )\n",
        "print(f\"Label index {Y_train[idx]} in one-hot encoding format is {Y_oh_train[idx]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_bNl8F2RTUe"
      },
      "source": [
        "ì´ì œ ëª¨ë“  ë°ì´í„°ë¥¼ Emojify-V1 ëª¨ë¸ì— ì…ë ¥ í•  ì¤€ë¹„ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë¸ì„ êµ¬í˜„í•©ì‹œë‹¤!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZIhe32aRUIv"
      },
      "source": [
        "### 1.3 - Implementing Emojifier-V1\n",
        "\n",
        "ê·¸ë¦¼ 2ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ”,\n",
        "- ì…ë ¥ ë¬¸ì¥ì˜ ê° ë‹¨ì–´ë¥¼ word vector í‘œí˜„ ë°©ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "- ì´í›„ word vectorsì˜ í‰ê· ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "- ì´ì „ ê³¼ì œì™€ ë™ì¼í•˜ê²Œ, ìš°ë¦¬ëŠ” ë¯¸ë¦¬ í›ˆë ¨ëœ 50 ì°¨ì›ì˜ GloVe embeddingì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "\n",
        "ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰ì‹œì¼œ ë‹¨ì–´ì— ëŒ€í•œ ëª¨ë“  ë²¡í„° í‘œí˜„ì„ ë‹´ê³  ìˆëŠ” `word_to_vec_map`ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LORnn2FWRT3c"
      },
      "source": [
        "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('../../readonly/glove.6B.50d.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrdOl5SmRuUR"
      },
      "source": [
        "ë‹¤ìŒì˜ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.\n",
        "- `word_to_index` : ì–´íœ˜ ì„¸íŠ¸ì—ì„œ íŠ¹ì • ë‹¨ì–´ì™€ ë‹¨ì–´ì˜ indexë¥¼ ë§µí•‘í•˜ëŠ” ë”•ì…”ë„ˆë¦¬\n",
        "- `index_to_word` : indexì™€ ë‹¨ì–´ë¥¼ ë§µí•‘í•˜ëŠ” ë”•ì…”ë„ˆë¦¬\n",
        "- `word_to_vec_map` : ë‹¨ì–´ì™€ GloVe ë²¡í„° í‘œí˜„ì„ ë§µí•‘í•˜ëŠ” ë”•ì…”ë„ˆë¦¬\n",
        "\n",
        "ì•„ë˜ ì½”ë“œë¥¼ í†µí•´ ìœ„ì˜ í•¨ìˆ˜ê°€ ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸í•´ë´…ì‹œë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-X4-PbSSGC9"
      },
      "source": [
        "word = \"cucumber\"\n",
        "idx = 289846\n",
        "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
        "print(\"the\", str(idx) + \"th word in the vocabulary is\", index_to_word[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poyBE_k6SHOq"
      },
      "source": [
        "**ì—°ìŠµ ë¬¸ì œ** : `sentence_to_avg()` í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”. ë‘ ë‹¨ê³„ë¥¼ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.\n",
        "1. ëª¨ë“  ë¬¸ì¥ì„ ì†Œë¬¸ìë¡œ ë°”ê¾¸ì„¸ìš”. ê·¸ë¦¬ê³  ë¬¸ì¥ì„ ë‹¨ì–´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ìª¼ê°œì•¼ í•©ë‹ˆë‹¤.\n",
        "  - `X.lower()` í•¨ìˆ˜ì™€ `X.split()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\n",
        "2. ê° ë‹¨ì–´ë§ˆë‹¤, GloVe ë²¡í„° ê°’ì„ ë°›ì•„ì™€ì„œ ë‹¤ìŒì„ ìˆ˜í–‰í•˜ì„¸ìš”.\n",
        "  - ëª¨ë“  GloVe word vectorì— ëŒ€í•œ í‰ê· ì„ ê³„ì‚°í•˜ì„¸ìš”.\n",
        "  - `numpy.zeros()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\n",
        "\n",
        "#### ì¶”ê°€ íŒíŠ¸\n",
        "- 0ìœ¼ë¡œ êµ¬ì„±ëœ `avg` ë°°ì—´ì„ ìƒì„±í•  ë•Œ, `word_to_vec_map` ì— ìˆëŠ” ë‹¤ë¥¸ word vectorì™€ ë™ì¼í•œ shapeì˜ ë²¡í„°ê°€ ë˜ê¸¸ ì›í•  ê²ƒì…ë‹ˆë‹¤.\n",
        "  - `word_to_vec_map`ì— ìˆëŠ” ë‹¨ì–´ë¥¼ ì„ íƒí•˜ê³  `.shape` í•„ë“œë¥¼ ì‚¬ìš©í•  ìˆ˜ ì´ìŠµë‹ˆë‹¤.\n",
        "  - íŠ¹ì • ë‹¨ì–´ì— ì—‘ì„¸ìŠ¤í•  ë•Œ í•´ë‹¹ ë‹¨ì–´ë¥¼ í•˜ë“œì½”ë”©í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ì„¸ìš”. ë‹¤ì‹œ ë§í•´, ì´ ê³¼ì œì— `word_to_vec_map`ì— `the`ë¼ëŠ” ë‹¨ì–´ê°€ ë³´ì¸ë‹¤ë©´, ì±„ì  ì•Œê³ ë¦¬ì¦˜ì´ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•  ë•Œ `the`ë¼ëŠ” ë‹¨ì–´ê°€ `word_to_vec_map`ì— ìˆì„ ê²ƒì´ë¼ê³  ê°€ì •í•´ì„  ì•ˆë©ë‹ˆë‹¤.\n",
        "  - íŒíŠ¸ : ì…ë ¥ ë¬¸ì¥ì—ì„œ ê²€ìƒ‰í•œ word vector ì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ìš©í•˜ì—¬ word vectorì˜ shapeë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVPfhByOTLIM"
      },
      "source": [
        "# GRADED FUNCTION: sentence_to_avg\n",
        "\n",
        "def sentence_to_avg(sentence, word_to_vec_map):\n",
        "    \"\"\"\n",
        "    Converts a sentence (string) into a list of words (strings). Extracts the GloVe representation of each word\n",
        "    and averages its value into a single vector encoding the meaning of the sentence.\n",
        "    \n",
        "    Arguments:\n",
        "    sentence -- string, one training example from X\n",
        "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
        "    \n",
        "    Returns:\n",
        "    avg -- average vector encoding information about the sentence, numpy-array of shape (50,)\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Step 1: Split sentence into list of lower case words (â‰ˆ 1 line)\n",
        "    words = None\n",
        "\n",
        "    # Initialize the average word vector, should have the same shape as your word vectors.\n",
        "    avg = None\n",
        "    \n",
        "    # Step 2: average the word vectors. You can loop over the words in the list \"words\".\n",
        "    total = 0\n",
        "    for w in None:\n",
        "        total += None\n",
        "    avg = None\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5swI_dDTNCR"
      },
      "source": [
        "avg = sentence_to_avg(\"Morrocan couscous is my favorite dish\", word_to_vec_map)\n",
        "print(\"avg = \\n\", avg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZVMCnKzTPbb"
      },
      "source": [
        "**ëª¨ë²” ë‹µì•ˆ**:\n",
        "\n",
        "```Python\n",
        "avg =\n",
        "[-0.008005    0.56370833 -0.50427333  0.258865    0.55131103  0.03104983\n",
        " -0.21013718  0.16893933 -0.09590267  0.141784   -0.15708967  0.18525867\n",
        "  0.6495785   0.38371117  0.21102167  0.11301667  0.02613967  0.26037767\n",
        "  0.05820667 -0.01578167 -0.12078833 -0.02471267  0.4128455   0.5152061\n",
        "  0.38756167 -0.898661   -0.535145    0.33501167  0.68806933 -0.2156265\n",
        "  1.797155    0.10476933 -0.36775333  0.750785    0.10282583  0.348925\n",
        " -0.27262833  0.66768    -0.10706167 -0.283635    0.59580117  0.28747333\n",
        " -0.3366635   0.23393817  0.34349183  0.178405    0.1166155  -0.076433\n",
        "  0.1445417   0.09808667]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my0Fxpn2TPVB"
      },
      "source": [
        "#### Model\n",
        "\n",
        "ì´ì œ `model()` í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ê¸° ìœ„í•œ ëª¨ë“  ì¡°ê°ë“¤ì„ ì™„ì„±í–ˆìŠµë‹ˆë‹¤. `sentence_to_avg()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê³  ë‚œ ë‹¤ìŒì—, ì—¬ëŸ¬ë¶„ì€ ë‹¤ìŒì˜ ì‘ì—…ì„ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "- í‰ê· ê°’ì„ forward propagationì— ì „ë‹¬í•˜ì„¸ìš”.\n",
        "- ë¹„ìš©ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "- softmax íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ê¸° ìœ„í•œ back propagationì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
        "\n",
        "**ì—°ìŠµ ë¬¸ì œ**: ê·¸ë¦¼ 2ë²ˆì— í‘œí˜„ëœ `model()` í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”.\n",
        "- forward pass ë° í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ê³µì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "- $Y_{oh}$(\"Y one hot\") ë³€ìˆ˜ëŠ” ë¼ë²¨ë§ëœ ì¶œë ¥ê°’ì˜ one-hot ì¸ì½”ë”© ë²¡í„° ê°’ì…ë‹ˆë‹¤.\n",
        "\n",
        "$$ z^{(i)} = W . avg^{(i)} + b$$\n",
        "\n",
        "$$ a^{(i)} = softmax(z^{(i)})$$\n",
        "\n",
        "$$ \\mathcal{L}^{(i)} = - \\sum_{k = 0}^{n_y - 1} Y_{oh,k}^{(i)} * log(a^{(i)}_k)$$\n",
        "\n",
        "**ì°¸ê³ ** : ë³´ë‹¤ íš¨ìœ¨ì ì¸ ë²¡í„°í™” ëœ êµ¬í˜„ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì§€ê¸ˆì€ ì•Œê³ ë¦¬ì¦˜ì„ ë” ì˜ ì´í•´í•˜ê³  ë””ë²„ê¹…ì„ ë” ì‰½ê²Œí•˜ê¸° ìœ„í•´ ì¤‘ì²© ëœ for ë£¨í”„ë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "ìœ„ì—ì„œ ë¶ˆëŸ¬ì˜¨ í•¨ìˆ˜ `softmax()`ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bChqsSWiUtcT"
      },
      "source": [
        "# GRADED FUNCTION: model\n",
        "\n",
        "def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):\n",
        "    \"\"\"\n",
        "    Model to train word vector representations in numpy.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data, numpy array of sentences as strings, of shape (m, 1)\n",
        "    Y -- labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1)\n",
        "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
        "    learning_rate -- learning_rate for the stochastic gradient descent algorithm\n",
        "    num_iterations -- number of iterations\n",
        "    \n",
        "    Returns:\n",
        "    pred -- vector of predictions, numpy-array of shape (m, 1)\n",
        "    W -- weight matrix of the softmax layer, of shape (n_y, n_h)\n",
        "    b -- bias of the softmax layer, of shape (n_y,)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(1)\n",
        "\n",
        "    # Define number of training examples\n",
        "    m = Y.shape[0]                          # number of training examples\n",
        "    n_y = 5                                 # number of classes  \n",
        "    n_h = 50                                # dimensions of the GloVe vectors \n",
        "    \n",
        "    # Initialize parameters using Xavier initialization\n",
        "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
        "    b = np.zeros((n_y,))\n",
        "    \n",
        "    # Convert Y to Y_onehot with n_y classes\n",
        "    Y_oh = convert_to_one_hot(Y, C = n_y) \n",
        "    \n",
        "    # Optimization loop\n",
        "    for t in range(num_iterations): # Loop over the number of iterations\n",
        "        for i in range(m):          # Loop over the training examples\n",
        "            \n",
        "            ### START CODE HERE ### (â‰ˆ 4 lines of code)\n",
        "            # Average the word vectors of the words from the i'th training example\n",
        "            avg = None\n",
        "\n",
        "            # Forward propagate the avg through the softmax layer\n",
        "            z = None\n",
        "            a = None\n",
        "\n",
        "            # Compute cost using the i'th training label's one hot representation and \"A\" (the output of the softmax)\n",
        "            cost = None\n",
        "            ### END CODE HERE ###\n",
        "            \n",
        "            # Compute gradients \n",
        "            dz = a - Y_oh[i]\n",
        "            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
        "            db = dz\n",
        "\n",
        "            # Update parameters with Stochastic Gradient Descent\n",
        "            W = W - learning_rate * dW\n",
        "            b = b - learning_rate * db\n",
        "        \n",
        "        if t % 100 == 0:\n",
        "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
        "            pred = predict(X, Y, W, b, word_to_vec_map) #predict is defined in emo_utils.py\n",
        "\n",
        "    return pred, W, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxYjQ99LUwKe"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(np.eye(5)[Y_train.reshape(-1)].shape)\n",
        "print(X_train[0])\n",
        "print(type(X_train))\n",
        "Y = np.asarray([5,0,0,5, 4, 4, 4, 6, 6, 4, 1, 1, 5, 6, 6, 3, 6, 3, 4, 4])\n",
        "print(Y.shape)\n",
        "\n",
        "X = np.asarray(['I am going to the bar tonight', 'I love you', 'miss you my dear',\n",
        " 'Lets go party and drinks','Congrats on the new job','Congratulations',\n",
        " 'I am so happy for you', 'Why are you feeling bad', 'What is wrong with you',\n",
        " 'You totally deserve this prize', 'Let us go play football',\n",
        " 'Are you down for football this afternoon', 'Work hard play harder',\n",
        " 'It is suprising how people can be dumb sometimes',\n",
        " 'I am very disappointed','It is the best day in my life',\n",
        " 'I think I will end up alone','My life is so boring','Good job',\n",
        " 'Great so awesome'])\n",
        "\n",
        "print(X.shape)\n",
        "print(np.eye(5)[Y_train.reshape(-1)].shape)\n",
        "print(type(X_train))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAlMkvpeUyIh"
      },
      "source": [
        "ë‹¤ìŒ ì…€ì„ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  softmax íŒŒë¼ë¯¸í„° (W, b)ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3bgh0I5U1_c"
      },
      "source": [
        "pred, W, b = model(X_train, Y_train, word_to_vec_map)\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RMzxZr_U65E"
      },
      "source": [
        "**ëª¨ë²” ë‹µì•ˆ** (on a subset of iterations):\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **Epoch: 0**\n",
        "        </td>\n",
        "        <td>\n",
        "           cost = 1.95204988128\n",
        "        </td>\n",
        "        <td>\n",
        "           Accuracy: 0.348484848485\n",
        "        </td>\n",
        "    </tr>\n",
        "<tr>\n",
        "        <td>\n",
        "            **Epoch: 100**\n",
        "        </td>\n",
        "        <td>\n",
        "           cost = 0.0797181872601\n",
        "        </td>\n",
        "        <td>\n",
        "           Accuracy: 0.931818181818\n",
        "        </td>\n",
        "    </tr>\n",
        "<tr>\n",
        "        <td>\n",
        "            **Epoch: 200**\n",
        "        </td>\n",
        "        <td>\n",
        "           cost = 0.0445636924368\n",
        "        </td>\n",
        "        <td>\n",
        "           Accuracy: 0.954545454545\n",
        "        </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **Epoch: 300**\n",
        "        </td>\n",
        "        <td>\n",
        "           cost = 0.0343226737879\n",
        "        </td>\n",
        "        <td>\n",
        "           Accuracy: 0.969696969697\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMC9LhLIVAxs"
      },
      "source": [
        "í›Œë¥­í•©ë‹ˆë‹¤! ëª¨ë¸ì€ í›ˆë ¨ ì„¸íŠ¸ì—ì„œ ë§¤ìš° ë†’ì€ ì •í™•ë„ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì´ì œ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ì‚´í´ ë³´ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLTj5zlPV_kf"
      },
      "source": [
        "### 1.4 - Examining test set performance\n",
        "- `em_util.spy`ì— ì •ì˜ë˜ì–´ ìˆëŠ” `predict` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ifp2nZkWIUP"
      },
      "source": [
        "print(\"Training set:\")\n",
        "pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)\n",
        "print('Test set:')\n",
        "pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2Rr_WMOWKty"
      },
      "source": [
        "**ëª¨ë²” ë‹µì•ˆ**:\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **Train set accuracy**\n",
        "        </td>\n",
        "        <td>\n",
        "           97.7\n",
        "        </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **Test set accuracy**\n",
        "        </td>\n",
        "        <td>\n",
        "           85.7\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxqe7cuCWQAL"
      },
      "source": [
        "* ë¬´ì‘ìœ„ ì¶”ì¸¡ì€ 5 ê°œì˜ í´ë˜ìŠ¤ê°€ ìˆëŠ” ê²½ìš° 20% ì˜ ì •í™•ë„ë¥¼ ê°€ì§‘ë‹ˆë‹¤. (1/5 = 20 %).\n",
        "* 127 ê°œì˜ ì˜ˆì œì— ëŒ€í•œ í›ˆë ¨í•œ ê²ƒì— ë¹„í•´ ê½¤ë‚˜ ì¢‹ì€ ì„±ëŠ¥ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDvECC5gWXwc"
      },
      "source": [
        "#### The model matches emojis to relevant words\n",
        "\n",
        "í›ˆë ¨ ì„¸íŠ¸ì—ì„œ, ì•Œê³ ë¦¬ì¦˜ì€ ë‹¤ìŒ ë¬¸ì¥ì„ ë°›ì•„ì„œ\n",
        "> \"I love you\"\n",
        "\n",
        "ë‹¤ìŒì˜ ì´ëª¨ì§€ë¥¼ ì¶œë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "> â¤ï¸\n",
        "\n",
        "- íŠ¸ë ˆì´ë‹ ì„¸íŠ¸ì— \"adore\"ë¼ëŠ” ë‹¨ì–´ê°€ ì—†ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "- ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  \"I adore you\"ë¼ëŠ” ë¬¸ì¥ì„ ì…ë ¥í•˜ë©´ ì–´ë–»ê²Œ ë˜ëŠ”ì§€ í™•ì¸í•´ë´…ì‹œë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GigpgI5ZWL10"
      },
      "source": [
        "X_my_sentences = np.array([\"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy\"])\n",
        "Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])\n",
        "\n",
        "pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n",
        "print_predictions(X_my_sentences, pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNbuhSRuW83N"
      },
      "source": [
        "ë†€ëìŠµë‹ˆë‹¤!\n",
        "\n",
        "\n",
        "* adoreì—ëŠ” loveì™€ ìœ ì‚¬í•œ ì„ë² ë”©ì´ ìˆê¸° ë•Œë¬¸ì— ì•Œê³ ë¦¬ì¦˜ì€ ì´ì „ì— ë³¸ ì ì´ ì—†ëŠ” ë‹¨ì–´ë¡œë„ ì˜¬ë°”ë¥¸ ê°’ì„ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
        "* heart, dear, beloved ë˜ëŠ” adoreì™€ ê°™ì€ ë‹¨ì–´ì—ëŠ” loveì™€ ìœ ì‚¬í•œ ì„ë² ë”© ë²¡í„°ê°€ ìˆìŠµë‹ˆë‹¤.\n",
        "  * ìœ„ì˜ ì…ë ¥ì„ ììœ ë¡­ê²Œ ìˆ˜ì •í•˜ê³  ë‹¤ì–‘í•œ ì…ë ¥ ë¬¸ì¥ì„ ì‹œë„í•´ë³´ì„¸ìš”.\n",
        "  * ì–¼ë§ˆë‚˜ ì˜ ì‘ë™í•©ë‹ˆê¹Œ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQPExDiuXhJb"
      },
      "source": [
        "#### Word ordering isn't considered in this model\n",
        "\n",
        "* ëª¨ë¸ì€ ë‹¤ìŒ ë¬¸ì¥ì— ëŒ€í•œ ì˜¬ë°”ë¥¸ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
        "> \"not feeling happy\"\n",
        "\n",
        "* ì´ ì•Œê³ ë¦¬ì¦˜ì€ ë‹¨ì–´ ìˆœì„œë¥¼ ë¬´ì‹œí•˜ë¯€ë¡œ, \"not happy\"ì™€ ê°™ì€ êµ¬ë¬¸ì„ ì´í•´ëŠ”ë° ì í•©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7bcFujiXxaS"
      },
      "source": [
        "#### Confusion matrix\n",
        "\n",
        "- Confusion matrixë¥¼ ì¶œë ¥í•˜ë©´ ëª¨ë¸ì—ì„œ ì–´ë–¤ í´ë˜ìŠ¤ê°€ ë” ì–´ë ¤ìš´ì§€ ì´í•´í•˜ëŠ”ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.\n",
        "- Confusion matrixëŠ” ë ˆì´ë¸”ì´ í•˜ë‚˜ì˜ í´ë˜ìŠ¤(\"ì‹¤ì œ\" í´ë˜ìŠ¤)ì¸ ì˜ˆì œê°€, ë‹¤ë¥¸ í´ë˜ìŠ¤(\"ì˜ˆì¸¡ëœ\" í´ë˜ìŠ¤)ì˜ ì•Œê³ ë¦¬ì¦˜ì— ì˜í•´ ì–¼ë§ˆë‚˜ ìì£¼ ì˜ëª» í‘œì‹œë˜ëŠ”ì§€ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IduG77UcXgDr"
      },
      "source": [
        "print(Y_test.shape)\n",
        "print('           '+ label_to_emoji(0)+ '    ' + label_to_emoji(1) + '    ' +  label_to_emoji(2)+ '    ' + label_to_emoji(3)+'   ' + label_to_emoji(4))\n",
        "print(pd.crosstab(Y_test, pred_test.reshape(56,), rownames=['Actual'], colnames=['Predicted'], margins=True))\n",
        "plot_confusion_matrix(Y_test, pred_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vg-zD2FTYDS_"
      },
      "source": [
        "## What you should remember from this section\n",
        "\n",
        "-127 ê°œì˜ ë°ì´í„° ì„¸íŠ¸ë¡œë„ Emojifyingì„ ìˆ˜í–‰í•˜ëŠ” ì¢‹ì€ ëª¨ë¸ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "  - ì´ê²ƒì€ ì¼ë°˜í™”ë¥¼ ìœ„í•œ ê°•ë ¥í•œ word vectorë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
        "- Emojify-V1ì€ \"This movie is not good and not enjoyable\"ê³¼ ê°™ì€ ë¬¸ì¥ì—ì„œ ì„±ëŠ¥ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤.\n",
        "  - ë‹¨ì–´ì˜ ì¡°í•©ì„ ì´í•´í•˜ì§€ ëª»í•©ë‹ˆë‹¤.\n",
        "  - ë‹¨ì–´ì˜ ìˆœì„œë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê³  ëª¨ë“  ë‹¨ì–´ì˜ embedding ë²¡í„°ë¥¼ í‰ê· í™”í•©ë‹ˆë‹¤.\n",
        "    \n",
        "**ë‹¤ìŒ ì„¹ì…˜ì—ì„œ ë” ë‚˜ì€ ì•Œê³ ë¦¬ì¦˜ì„ ë§Œë“¤ ê²ƒì…ë‹ˆë‹¤!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mddp4CFBYbn_"
      },
      "source": [
        "## 2 - Emojifier-V2 : Using LSTMs in Keras\n",
        "\n",
        "ë‹¨ì–´ **ì‹œí€€ìŠ¤**ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” LSTM ëª¨ë¸ì„ êµ¬ì¶•í•´ ë³´ê² ìŠµë‹ˆë‹¤!\n",
        "* ì´ ëª¨ë¸ì€ ë‹¨ì–´ì˜ ìˆœì„œë¥¼ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "* Emojifier-V2ëŠ” ì‚¬ì „ í•™ìŠµ ëœ ë‹¨ì–´ ì„ë² ë”©ì„ ê³„ì† ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
        "* LSTMì— word embeddingì„ ì œê³µí•©ë‹ˆë‹¤.\n",
        "* LSTMì€ ê°€ì¥ ì í•©í•œ ì´ëª¨í‹°ì½˜ì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
        "\n",
        "ë‹¤ìŒ ì…€ì„ ì‹¤í–‰í•˜ì—¬ Keras íŒ¨í‚¤ì§€ë¥¼ë¡œë“œí•˜ì‹­ì‹œì˜¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DfNCXANYaor"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.initializers import glorot_uniform\n",
        "np.random.seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLX10W31YqqK"
      },
      "source": [
        "### 2.1 - Overview of the model\n",
        "\n",
        "ì•„ë˜ëŠ” ì—¬ëŸ¬ë¶„ì´ êµ¬í˜„í•  Emojifier-v2 ëª¨ë¸ì˜ êµ¬ì¡°ì…ë‹ˆë‹¤.\n",
        "\n",
        "<img src=\"arts/emojifier-v2.png\" style=\"width:700px;height:400px;\">\n",
        "<center>ê·¸ë¦¼ 3 : Emojfifier-V2. 2-layerì˜ LSTM ì‹œí€¸ìŠ¤ ë¶„ë¥˜ ëª¨ë¸</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhvdDFm1ZBib"
      },
      "source": [
        "### 2.2 Keras and mini-batching\n",
        "\n",
        "- ì´ë²ˆ ê³¼ì œì—ì„œëŠ”, ë¯¸ë‹ˆ ë°°ì¹˜ë¥¼ ì‚¬ìš©í•´ ì¼€ë¼ìŠ¤ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³ ì í•©ë‹ˆë‹¤.\n",
        "- í•˜ì§€ë§Œ ëŒ€ë¶€ë¶„ì˜ ë”¥ ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ëŠ” ë™ì¼í•œ ë¯¸ë‹ˆ ë°°ì¹˜ì˜ ëª¨ë“  ìŠ¤í€¸ìŠ¤ê°€ ë™ì¼í•œ ê¸¸ì´ë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤.\n",
        "  - ì´ë¥¼ í†µí•´ ë²¡í„°í™”ê°€ ê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤ : 3ê°œ ë‹¨ì–´ ë¬¸ì¥ê³¼ 4ê°œ ë‹¨ì–´ ë¬¸ì¥ì´ ìˆëŠ” ê²½ìš° í•„ìš”í•œ ê³„ì‚°ì´ ë‹¤ë¦…ë‹ˆë‹¤(í•˜ë‚˜ëŠ” LSTMì˜ 3ë‹¨ê³„, í•˜ë‚˜ëŠ” LSTMì˜ 4ë‹¨ê³„ë¥¼ ê±°ì³ì•¼ í•©ë‹ˆë‹¤). ë‘˜ ë‹¤ ë™ì‹œì— í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1bTXd59ZAyC"
      },
      "source": [
        "#### Padding handles sequences of varying length\n",
        "\n",
        "* ë‹¤ë¥¸ ê¸¸ì´ì˜ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¼ë°˜ì ì¸ í•´ê²°ì±…ì€ íŒ¨ë”©ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ íŠ¹ë³„íˆ:\n",
        "  * ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ ì„¤ì •\n",
        "  * ë™ì¼í•œ ê¸¸ì´ë¥¼ ê°–ë„ë¡ ëª¨ë“  ì‹œí€€ìŠ¤ë¥¼ ì±„ ì›ë‹ˆë‹¤.\n",
        "\n",
        "##### Example of padding\n",
        "\n",
        "* ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ 20 ì¸ ê²½ìš° ê° ì…ë ¥ ë¬¸ì¥ì˜ ê¸¸ì´ê°€ 20ì´ë˜ë„ë¡ ëª¨ë“  ë¬¸ì¥ì„ \"0\"ìœ¼ë¡œ ì±„ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "* ë”°ë¼ì„œ \"I love you\"ë¼ëŠ” ë¬¸ì¥ì€ $ (e_ {I}, e_ {love}, e_ {you}, \\vec {0}, \\vec {0}, \\ldots, \\vec {0 }) $.\n",
        "*ì´ ì˜ˆì—ì„œëŠ” 20 ë‹¨ì–´ë³´ë‹¤ ê¸´ ë¬¸ì¥ì€ ì˜ ë ¤ì•¼í•©ë‹ˆë‹¤.\n",
        "* ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì„ íƒí•˜ëŠ” í•œ ê°€ì§€ ë°©ë²•ì€ í›ˆë ¨ ì„¸íŠ¸ì—ì„œ ê°€ì¥ ê¸´ ë¬¸ì¥ì˜ ê¸¸ì´ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVbtbesPZArY"
      },
      "source": [
        "### 2.3 - The Embedding layer\n",
        "\n",
        "- Kerasì—ì„œ, embedding matrixëŠ” \"layer\" ë¼ëŠ” ë‹¨ì–´ë¡œ í‘œí˜„ë©ë‹ˆë‹¤.\n",
        "- Embedding matrixëŠ” ë‹¨ì–´ indexì™€ embedding ë²¡í„°ë¥¼ ë§µí•‘í•©ëŠ¬ë‹¤.\n",
        "  - ë‹¨ì–´ ì¸ë±ìŠ¤ëŠ” ì–‘ì˜ ì •ìˆ˜ì…ë‹ˆë‹¤.\n",
        "  - Embedding vectorëŠ” ê³ ì •ëœ ì‚¬ì´ì¦ˆì˜ dense ë²¡í„° ì…ë‹ˆë‹¤.\n",
        "  - íŠ¹ì • ë²¡í„°ê°€ \"dense\" í•˜ë‹¤ëŠ” ê²ƒì€, ë²¡í„° ë‚´ì˜ ëŒ€ë¶€ë¶„ì˜ ê°’ì´ 0ì´ ì•„ë‹ˆë¼ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ì™€ ë°˜ëŒ€ë˜ëŠ” \"dense\"ê°€ ì•„ë‹Œ ë²¡í„°ì˜ ì˜ˆì‹œë¡œ one-hot ì¸ì½”ë”© ë²¡í„°ë¥¼ ë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "- Embedding matrixëŠ” ë‘ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "  - ë°ì´í„°ë¥¼ ê¸ì–´ ëª¨ì•„, ì§ì ‘ embeddingì„ í•™ìŠµí•˜ì—¬ êµ¬í•©ë‹ˆë‹¤.\n",
        "  - ì‚¬ì „ì— í›ˆë ¨ëœ embedding ë²¡í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LG-bwkuZ3dF"
      },
      "source": [
        "#### Using and updateing pre-trained embeddings\n",
        "\n",
        "- ì´ íŒŒíŠ¸ì—ì„œ, ì—¬ëŸ¬ë¶„ì€ ì¼€ë¼ìŠ¤ë¥¼ ì‚¬ìš©í•´ [Embedding()](https://keras.io/layers/embeddings/) ë ˆì´ì–´ë¥¼ ë§Œë“œëŠ” ë°©ë²•ì„ ë°°ìš°ê²Œ ë©ë‹ˆë‹¤.\n",
        "- ë¨¼ì € Embedding ë ˆì´ì–´ë¥¼ 50 ì°¨ì›ì˜ GloVe ë²¡í„°ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
        "- ì•„ë˜ ì½”ë“œì—ì„œ Kerasë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ ë ˆì´ì–´ë¥¼ í›ˆë ¨í•˜ê±°ë‚˜ ê³ ì • ëœ ìƒíƒœë¡œ ë‘ëŠ” ë°©ë²•ì„ ë³´ì—¬ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n",
        "- í›ˆë ¨ ì„¸íŠ¸ê°€ ë§¤ìš° ì‘ê¸° ë•Œë¬¸ì— GloVe ì„ë² ë”©ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ëŒ€ì‹  ê³ ì • ëœ ìƒíƒœë¡œ ë‘¡ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLI-l-guaNt1"
      },
      "source": [
        "#### Inputs and outputs to the embedding layer\n",
        "\n",
        "\n",
        "- `Embedding ()`ë ˆì´ì–´ì˜ ì…ë ¥ì€ **(ë°°ì¹˜ í¬ê¸°, ìµœëŒ€ ì…ë ¥ ê¸¸ì´)** í¬ê¸°ì˜ ì •ìˆ˜ í–‰ë ¬ì…ë‹ˆë‹¤.\n",
        "  - ì´ ì…ë ¥ì€ ì¸ë±ìŠ¤ ëª©ë¡ (ì •ìˆ˜)ìœ¼ë¡œ ë³€í™˜ ëœ ë¬¸ì¥ì— í•´ë‹¹í•©ë‹ˆë‹¤.\n",
        "  - ì…ë ¥ì—ì„œ ê°€ì¥ í° ì •ìˆ˜ (ê°€ì¥ ë†’ì€ ë‹¨ì–´ ì¸ë±ìŠ¤)ëŠ” ì–´íœ˜ í¬ê¸°ë³´ë‹¤ í¬ì§€ ì•Šì•„ì•¼í•©ë‹ˆë‹¤.\n",
        "- Embedding ë ˆì´ì–´ëŠ” (ë°°ì¹˜ í¬ê¸°, ìµœëŒ€ ì…ë ¥ ê¸¸ì´, ë‹¨ì–´ ë²¡í„°ì˜ ì°¨ì›) shapeì˜ ë°°ì—´ì„ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "- ê·¸ë¦¼ì€ ì„ë² ë”© ë ˆì´ì–´ë¥¼ í†µí•´ ë‘ ê°œì˜ ì˜ˆì œ ë¬¸ì¥ì´ ì „íŒŒë˜ëŠ” ëª¨ìŠµì…ë‹ˆë‹¤.\n",
        "  - ë‘ ì˜ˆì œ ëª¨ë‘`max_len = 5` ê¸¸ì´ë¡œ zero paddingë˜ì—ˆìŠµë‹ˆë‹¤.\n",
        "  - ë‹¨ì–´ ì„ë² ë”©ì€ ê¸¸ì´ê°€ 50 ë‹¨ìœ„ì…ë‹ˆë‹¤.\n",
        "  - shapeëŠ” `(2, max_len, 50)`ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC94QeKPaxR7"
      },
      "source": [
        "#### Prepare to input sentenses\n",
        "\n",
        "**ì—°ìŠµ ë¬¸ì œ**:\n",
        "- ì•„ë¬´ ë¬¸ì¥ (X)ë¥¼ ë°›ì•„ì„œ embedding layerì˜ ì…ë ¥ìœ¼ë¡œ ë³€í™˜ì‹œì¼œì£¼ëŠ” `sentences_to_indices` í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.\n",
        "  - ê° í›ˆë ¨ìš© ë¬¸ì¥ë“¤ì„ ì¸ë±ìŠ¤(ê° ì¸ë±ìŠ¤ë“¤ì€ ë¬¸ì¥ì—ì„œì˜ ê° ë‹¨ì–´ì— ëŒ€ì‘ë©ë‹ˆë‹¤)ë¡œ ë³€í™˜í•˜ì„¸ìš”.\n",
        "  - ê°€ì¥ ê¸´ ë¬¸ì¥ì˜ ê¸¸ì´ì— ë§ê²Œ ëª¨ë“  ë¦¬ìŠ¤íŠ¸ë“¤ì„ zero padding í•˜ì„¸ìš”.\n",
        "\n",
        "##### ì¶”ê°€ íŒíŠ¸\n",
        "- ë°˜ë³µë¬¸ì—ì„œ `enumerate()`ë¥¼ í•¨ìˆ˜ ì‚¬ìš©í•  ìˆ˜ ìˆì§€ë§Œ ìë™ ì±„ì  ì•Œê³ ë¦¬ì¦˜ì— ì›í™œí•œ ê°’ì„ ì „ë‹¬í•˜ë ¤ë©´ jë¥¼ ëª…ì‹œ ì ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ê³  ì¦ê°€ ì‹œì¼œê°€ë©° ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2TZTFB7Z3Iv"
      },
      "source": [
        "for idx, val in enumerate([\"I\", \"like\", \"learning\"]):\n",
        "    print(idx,val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEiikd5fbYFl"
      },
      "source": [
        "# GRADED FUNCTION: sentences_to_indices\n",
        "\n",
        "def sentences_to_indices(X, word_to_index, max_len):\n",
        "    \"\"\"\n",
        "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
        "    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n",
        "    \n",
        "    Arguments:\n",
        "    X -- array of sentences (strings), of shape (m, 1)\n",
        "    word_to_index -- a dictionary containing the each word mapped to its index\n",
        "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
        "    \n",
        "    Returns:\n",
        "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[0]                                   # number of training examples\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (â‰ˆ 1 line)\n",
        "    X_indices = None\n",
        "    \n",
        "    for i in range(m):                               # loop over training examples\n",
        "        \n",
        "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
        "        sentence_words =None\n",
        "        \n",
        "        # Initialize j to 0\n",
        "        j = None\n",
        "        \n",
        "        # Loop over the words of sentence_words\n",
        "        for w in None:\n",
        "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
        "            X_indices[i, j] = None\n",
        "            # Increment j to j + 1\n",
        "            j = None\n",
        "            \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return X_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngmd6PCmbZ-b"
      },
      "source": [
        "ë‹¤ìŒ ì…€ì„ ì‹¤í–‰í•˜ì—¬ `sentences_to_indices()`ì˜ ê¸°ëŠ¥ì„ í™•ì¸í•˜ê³  ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzy0fy9cbeH7"
      },
      "source": [
        "X1 = np.array([\"funny lol\", \"lets play baseball\", \"food is ready for you\"])\n",
        "X1_indices = sentences_to_indices(X1,word_to_index, max_len = 5)\n",
        "print(\"X1 =\", X1)\n",
        "print(\"X1_indices =\\n\", X1_indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BqWSiJfbfPS"
      },
      "source": [
        "**ëª¨ë²” ë‹µì•ˆ**:\n",
        "\n",
        "```Python\n",
        "X1 = ['funny lol' 'lets play baseball' 'food is ready for you']\n",
        "X1_indices =\n",
        " [[ 155345.  225122.       0.       0.       0.]\n",
        " [ 220930.  286375.   69714.       0.       0.]\n",
        " [ 151204.  192973.  302254.  151349.  394475.]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ie9HCAxbhQK"
      },
      "source": [
        "#### Build embedding layer\n",
        "- ì´ì œ ì¼€ë¼ìŠ¤ì™€, ì‚¬ì „ í›ˆë ¨ëœ word-vectorë¥¼ ì‚¬ìš©í•´ `Embedding()` ë ˆì´ì–´ë¥¼ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "- Embedding ë ˆì´ì–´ëŠ” ë‹¨ì–´ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.\n",
        "  - `sentences_to_indices()` í•¨ìˆ˜ëŠ” ì´ ë‹¨ì–´ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "- Embedding ë ˆì´ì–´ëŠ” í•´ë‹¹ ë¬¸ì¥ì˜ word embeddingì„ ë¦¬í„´í•©ë‹ˆë‹¤.\n",
        "\n",
        "**Exercise** : ì•„ë˜ ë‹¨ê³„ì— ë”°ë¼, `pretrained_embedding layer()` í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”.\n",
        "\n",
        "1. Numpy arrayì¸ embedding matrixë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ì„¸ìš”.\n",
        "  - ì´ embedding matrixì˜ ì—´(row)ì€ ê° ì–´íœ˜ ì„¸íŠ¸ì—ì„œ ìœ ì¼í•œ í•œ ê°œì˜ ë‹¨ì–´ì…ë‹ˆë‹¤.\n",
        "    - ì¶”ê°€ì ìœ¼ë¡œ \"unknown\" wordë¼ê³  í•˜ëŠ” ì—´ì´ ìˆìŠµë‹ˆë‹¤.\n",
        "    - ë”°ë¼ì„œ `vocab_len` ê°’ì€ ì–´íœ˜ì„¸íŠ¸ì˜ ë‹¨ì–´ì˜ ê°œìˆ˜ì—, 1ì„ ë”í•œ ê°’ì…ë‹ˆë‹¤.\n",
        "  - ê° ì—´ì€ ë‹¨ì–´ì˜ ë²¡í„° í‘œí˜„ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "    - ì˜ˆë¥¼ ë“¤ì–´ GloVe ë‹¨ì–´ ë²¡í„°ë¥¼ ì‚¬ìš©í•œë‹¤ë©´ í•œ ê°œì˜ ì—´ì´ 50ê°œì˜ í¬ì§€ì…˜ì„ ê°–ê³  ìˆì„ ê²ƒì…ë‹ˆë‹¤.\n",
        "  - ì•„ë˜ ì½”ë“œì—ì„œ, `emb_dim`ì€ word embeddingì˜ ê¸¸ì´ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
        "2. Embedding matrixì˜ ê° í–‰ì„ ë‹¨ì–´ì˜ ë²¡í„° í‘œí˜„ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.\n",
        "  - `word_to_index` ê° ë‹¨ì–´ëŠ” ë¬¸ìì—´ì…ë‹ˆë‹¤.\n",
        "  - `word_to_vec_map`ì€ keyê°€ ë¬¸ìì—´ì´ê³ , valueê°€ ë‹¨ì–´ ë²¡í„°ì¸ ë”•ì…”ë„ˆë¦¬ì…ë‹ˆë‹¤.\n",
        "3. Keras Embedding layerë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "  - [Embedding()](https://keras.io/layers/embeddings/)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "  - ì…ë ¥ ì°¨ì›ì€ ì–´íœ˜ì˜ ê¸¸ì´(ê³ ìœ  ë‹¨ì–´ ìˆ˜ì— 1ì„ ë”í•œ ê°’)ì…ë‹ˆë‹¤.\n",
        "  - ì´ ë ˆì´ì–´ì˜ embeddingì„ ê³ ì •ì‹œí‚µë‹ˆë‹¤.\n",
        "    - `trainable = True`ë¡œ ì„¤ì •í•˜ë©´, ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì´ word embedding ê°’ì„ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "    - ì´ ê²½ìš° ìš°ë¦¬ëŠ” ëª¨ë¸ì´ word embedding ê°’ì„ ë³€ê²½í•˜ì§€ ì•Šê¸¸ ì›í•©ë‹ˆë‹¤.\n",
        "4. Word embeddingê³¼ ê°™ë„ë¡ embeddingì˜ ê°€ì¤‘ì¹˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "  - ì´ëŠ” ì´ë¯¸ ì™„ì„±ëœ ì½”ë“œì˜ ì¼ë¶€ë¼ì„œ, ë³„ë„ë¡œ ìˆ˜ì •í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laP_exzXcn1S"
      },
      "source": [
        "# GRADED FUNCTION: pretrained_embedding_layer\n",
        "\n",
        "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
        "    \"\"\"\n",
        "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
        "    \n",
        "    Arguments:\n",
        "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
        "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
        "\n",
        "    Returns:\n",
        "    embedding_layer -- pretrained layer Keras instance\n",
        "    \"\"\"\n",
        "    \n",
        "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
        "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Step 1\n",
        "    # Initialize the embedding matrix as a numpy array of zeros.\n",
        "    # See instructions above to choose the correct shape.\n",
        "    emb_matrix = None\n",
        "    \n",
        "    # Step 2\n",
        "    # Set each row \"idx\" of the embedding matrix to be \n",
        "    # the word vector representation of the idx'th word of the vocabulary\n",
        "    for word, idx in word_to_index.items():\n",
        "        emb_matrix[idx, :] = None\n",
        "\n",
        "    # Step 3\n",
        "    # Define Keras embedding layer with the correct input and output sizes\n",
        "    # Make it non-trainable.\n",
        "    embedding_layer = None\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Step 4 (already done for you; please do not modify)\n",
        "    # Build the embedding layer, it is required before setting the weights of the embedding layer. \n",
        "    embedding_layer.build((None,)) # Do not modify the \"None\".  This line of code is complete as-is.\n",
        "    \n",
        "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
        "    embedding_layer.set_weights([emb_matrix])\n",
        "    \n",
        "    return embedding_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xC1jzUZeGEV"
      },
      "source": [
        "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
        "print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsU_jGdneGVD"
      },
      "source": [
        "**ëª¨ë²” ë‹µì•ˆ**:\n",
        "\n",
        "```Python\n",
        "weights[0][1][3] = -0.3403\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWL_n3gneIta"
      },
      "source": [
        "## 2.3 Building the Emojifier-V2\n",
        "\n",
        "\n",
        "ì´ì œ Emojifier-V2 ëª¨ë¸ì„ ë¹Œë“œ í•´ ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "* Embedding ë ˆì´ì–´ì˜ ì¶œë ¥ì„ LSTM ë„¤íŠ¸ì›Œí¬ì— ê³µê¸‰í•©ë‹ˆë‹¤.\n",
        "\n",
        "<img src=\"arts/emojifier-v2.png\" style=\"width:700px;height:400px;\">\n",
        "<center>ê·¸ë¦¼ 3 : Emojifier-v2. 2 ê³„ì¸µ LSTM ì‹œí€€ìŠ¤ ë¶„ë¥˜ ëª¨ë¸</center>\n",
        "\n",
        "\n",
        "**ì—°ìŠµ ë¬¸ì œ** : ê·¸ë¦¼ 3ì— í‘œì‹œëœ ì•„í‚¤í…ì²˜ì˜ Keras ê·¸ë˜í”„ë¥¼ ì‘ì„±í•˜ëŠ” `Emojify_V2()`ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
        "* ëª¨ë¸ì€ `input_shape`ë¡œ ì •ì˜ ëœ shape (`m`,`max_len`,)ì˜ ë¬¸ì¥ ë°°ì—´ì„ ì…ë ¥ìœ¼ë¡œë°›ìŠµë‹ˆë‹¤.\n",
        "* ëª¨ë¸ì€ (`m`,`C = 5`) shapeì˜ softmax í™•ë¥  ë²¡í„°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "* ë‹¤ìŒ Keras ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "    * [input()](https://keras.io/layers/core/#input)\n",
        "        * `shape` ë° `dtype` ë§¤ê°œ ë³€ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "        * ì…ë ¥ì€ ì •ìˆ˜ì´ë¯€ë¡œ ë°ì´í„° ìœ í˜•ì„ ë¬¸ìì—´ 'int32'ë¡œ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "    * [LSTM()](https://keras.io/layers/recurrent/#lstm)\n",
        "        * `units` ë°`return_sequences` ë§¤ê°œ ë³€ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "    * [Dropout()](https://keras.io/layers/core/#dropout)\n",
        "        * `rate` ë§¤ê°œ ë³€ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "    * [Dense()](https://keras.io/layers/core/#dense)\n",
        "        * 'ë‹¨ìœ„'ì„¤ì •,\n",
        "        * `Dense()`ì—ëŠ” `activation`ë§¤ê°œ ë³€ìˆ˜ê°€ ìˆìŠµë‹ˆë‹¤. ìë™ ì±„ì ê¸°ì— ì˜¬ë°”ë¥¸ ê°’ì„ ì „ë‹¬í•˜ê¸° ìœ„í•´ `Dense()`ë‚´ì— í™œì„±í™”ë¥¼ ì„¤ì •í•˜ì§€ ë§ˆì‹­ì‹œì˜¤. ì´ë¥¼ ìœ„í•´ ë³„ë„ì˜ 'í™œì„±í™”' ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "    * [Activation()](https://keras.io/activations/).\n",
        "        * ì„ íƒí•œ í™œì„±í™”ë¥¼ ì†Œë¬¸ì ë¬¸ìì—´ë¡œ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "    * [Model](https://keras.io/models/model/)\n",
        "        'ì…ë ¥'ê³¼ 'ì¶œë ¥'ì„ ì„¤ì •í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAb2Rxg7e3Qz"
      },
      "source": [
        "\n",
        "#### ì¶”ê°€ íŒíŠ¸\n",
        "* ì´ëŸ¬í•œ Keras ë ˆì´ì–´ëŠ” ê°œì²´ë¥¼ ë°˜í™˜í•˜ê³  ì´ì „ ë ˆì´ì–´ì˜ ì¶œë ¥ì„ í•´ë‹¹ ê°œì²´ì— ëŒ€í•œ ì…ë ¥ ì¸ìˆ˜ë¡œ ì œê³µí•œë‹¤ëŠ” ì ì„ ê¸°ì–µí•˜ì‹­ì‹œì˜¤. ë°˜í™˜ ëœ ê°ì²´ëŠ” ë™ì¼í•œ ì¤„ì—ì„œ ìƒì„±ë˜ê³  í˜¸ì¶œ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "  ```python\n",
        "  # How to use Keras layers in two lines of code\n",
        "  dense_object = Dense(units = ...)\n",
        "  X = dense_object(inputs)\n",
        "\n",
        "  # How to use Keras layers in one line of code\n",
        "  X = Dense(units = ...)(inputs)\n",
        "  ```\n",
        "\n",
        "* `pretrained_embedding_layer`ê°€ ë°˜í™˜í•˜ëŠ”`embedding_layer`ëŠ” ë‹¨ì¼ ì¸ìˆ˜ (ë¬¸ì¥ ì¸ë±ìŠ¤)ë¥¼ ì „ë‹¬í•˜ì—¬ í•¨ìˆ˜ë¡œ í˜¸ì¶œ í•  ìˆ˜ìˆëŠ” ë ˆì´ì–´ ê°ì²´ì…ë‹ˆë‹¤.\n",
        "\n",
        "* ë‹¤ìŒì€ ë¬¸ì œê°€ ë°œìƒí•  ê²½ìš°ë¥¼ ëŒ€ë¹„ í•œ ìƒ˜í”Œ ì½”ë“œì…ë‹ˆë‹¤.\n",
        "  ```python\n",
        "  raw_inputs = Input(shape=(maxLen,), dtype='int32')\n",
        "  preprocessed_inputs = ... # some pre-processing\n",
        "  X = LSTM(units = ..., return_sequences= ...)(processed_inputs)\n",
        "  X = Dropout(rate = ..., )(X)\n",
        "  ...\n",
        "  X = Dense(units = ...)(X)\n",
        "  X = Activation(...)(X)\n",
        "  model = Model(inputs=..., outputs=...)\n",
        "  ...\n",
        "  ```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYcTA0tmeIOk"
      },
      "source": [
        "# GRADED FUNCTION: Emojify_V2\n",
        "\n",
        "def Emojify_V2(input_shape, word_to_vec_map, word_to_index):\n",
        "    \"\"\"\n",
        "    Function creating the Emojify-v2 model's graph.\n",
        "    \n",
        "    Arguments:\n",
        "    input_shape -- shape of the input, usually (max_len,)\n",
        "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
        "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
        "\n",
        "    Returns:\n",
        "    model -- a model instance in Keras\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Define sentence_indices as the input of the graph.\n",
        "    # It should be of shape input_shape and dtype 'int32' (as it contains indices, which are integers).\n",
        "    sentence_indices = None\n",
        "    \n",
        "    # Create the embedding layer pretrained with GloVe Vectors (â‰ˆ1 line)\n",
        "    embedding_layer = None\n",
        "    \n",
        "    # Propagate sentence_indices through your embedding layer\n",
        "    # (See additional hints in the instructions).\n",
        "    embeddings = None   \n",
        "    \n",
        "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
        "    # The returned output should be a batch of sequences.\n",
        "    X = None\n",
        "    # Add dropout with a probability of 0.5\n",
        "    X = None\n",
        "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
        "    # The returned output should be a single hidden state, not a batch of sequences.\n",
        "    X = None\n",
        "    # Add dropout with a probability of 0.5\n",
        "    X = None\n",
        "    # Propagate X through a Dense layer with 5 units\n",
        "    X = None\n",
        "    # Add a softmax activation\n",
        "    X = None\n",
        "    \n",
        "    # Create Model instance which converts sentence_indices into X.\n",
        "    model = None\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gzpdMgBfPc2"
      },
      "source": [
        "ë‹¤ìŒ ì…€ì„ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ ë§Œë“¤ê³  ìš”ì•½ì„ í™•ì¸í•©ë‹ˆë‹¤. ë°ì´í„° ì„¸íŠ¸ì˜ ëª¨ë“  ë¬¸ì¥ì´ 10 ë‹¨ì–´ ë¯¸ë§Œì´ë¯€ë¡œ `max_len = 10`ì„ ì„ íƒí–ˆìŠµë‹ˆë‹¤. ì•„í‚¤í…ì²˜ê°€ í‘œì‹œë˜ì–´ì•¼ í•©ë‹ˆë‹¤. \"20,223,927\" ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ì¤‘ 20,000,050(Embeddingì´ë¼ëŠ” ë‹¨ì–´)ì€ í•™ìŠµ í•  ìˆ˜ ì—†ê³  ë‚˜ë¨¸ì§€ 223,877ê°œ ì˜ íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ ì–´íœ˜ í¬ê¸°ëŠ” 400,001 ë‹¨ì–´ (0 ~ 400,000ì˜ ìœ íš¨í•œ ì¸ë±ìŠ¤ í¬í•¨)ê°€ ìˆê¸° ë•Œë¬¸ì— 400,001 \\ * 50 = 20,000,050 ê°œì˜ í›ˆë ¨ ë¶ˆê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ê°€ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktU3_jlHfaxa"
      },
      "source": [
        "model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7_uqcGkfcCu"
      },
      "source": [
        "í‰ì†Œì™€ ê°™ì´ Kerasì—ì„œ ëª¨ë¸ì„ ìƒì„± í•œ í›„ì—ëŠ” ëª¨ë¸ì„ ì»´íŒŒì¼í•˜ê³  ì‚¬ìš©í•  ì†ì‹¤, ìµœì í™” ë„êµ¬ ë° ë©”íŠ¸ë¦­ì„ ì •ì˜í•´ì•¼í•©ë‹ˆë‹¤. categorical_crossentropy loss, adam Optimizer ë° [ 'accuracy'] ì¸¡ì • í•­ëª©ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì»´íŒŒì¼í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyLjMhoAffQy"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "In8K2BbAfhL4"
      },
      "source": [
        "ëª¨ë¸ì„ í›ˆë ¨ í•  ì‹œê°„ì…ë‹ˆë‹¤. Emojifier-V2 `model`ì€ shape (`m`,`max_len`)ì˜ ë°°ì—´ì„ ì…ë ¥ìœ¼ë¡œ ì·¨í•˜ê³  (`m`,`number of classes`) shapeì˜ í™•ë¥  ë²¡í„°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤. ë”°ë¼ì„œ X_train (ë¬¸ìì—´ë¡œ ëœ ë¬¸ì¥ ë°°ì—´)ì„ X_train_indices (ë‹¨ì–´ ìƒ‰ì¸ ëª©ë¡ìœ¼ë¡œ ë¬¸ì¥ ë°°ì—´)ë¡œ, Y_train (ì¸ë±ìŠ¤ë¡œ ë ˆì´ë¸”)ì„ Y_train_oh (ì›-í•« ë²¡í„°ë¡œ ë ˆì´ë¸”)ë¡œ ë³€í™˜í•´ì•¼í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igVgoCmYfn7Y"
      },
      "source": [
        "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
        "Y_train_oh = convert_to_one_hot(Y_train, C = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORQ1ez6KfsEF"
      },
      "source": [
        "\n",
        "Keras ëª¨ë¸ì„ `X_train_indices` ë° `Y_train_oh`ì— ë§ì¶¥ë‹ˆë‹¤. `epochs = 50` ê³¼ `batch_size = 32`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cps8xRHWfr66"
      },
      "source": [
        "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ__yRYdfx4D"
      },
      "source": [
        "ëª¨ë¸ì€ í•™ìŠµ ì„¸íŠ¸ì—ì„œ ì•½ ** 90 % ~ 100 % ì •í™•ë„ **ë¥¼ ìˆ˜í–‰í•´ì•¼í•©ë‹ˆë‹¤. ë‹¹ì‹ ì´ ì–»ëŠ” ì •í™•í•œ ì •í™•ë„ëŠ” ì•½ê°„ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ëª¨ë¸ì„ í‰ê°€í•˜ë ¤ë©´ ë‹¤ìŒ ì…€ì„ ì‹¤í–‰í•˜ì‹­ì‹œì˜¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2Qc-gL1f0Yv"
      },
      "source": [
        "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
        "Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
        "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
        "print()\n",
        "print(\"Test accuracy = \", acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tbj4AQ1Qf4Jv"
      },
      "source": [
        "\n",
        "í…ŒìŠ¤íŠ¸ ì •í™•ë„ëŠ” 80 %ì—ì„œ 95 % ì‚¬ì´ ì—¬ì•¼í•©ë‹ˆë‹¤. ì•„ë˜ ì…€ì„ ì‹¤í–‰í•˜ì—¬ ë¼ë²¨ì´ ì˜ëª» ì§€ì •ëœ ì˜ˆë¥¼ í™•ì¸í•˜ì„¸ìš”."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BXXRu4Sf368"
      },
      "source": [
        "# This code allows you to see the mislabelled examples\n",
        "C = 5\n",
        "y_test_oh = np.eye(C)[Y_test.reshape(-1)]\n",
        "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
        "pred = model.predict(X_test_indices)\n",
        "for i in range(len(X_test)):\n",
        "    x = X_test_indices\n",
        "    num = np.argmax(pred[i])\n",
        "    if(num != Y_test[i]):\n",
        "        print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqjkZnLGf7fL"
      },
      "source": [
        "ì´ì œ ì§ì ‘ ë‹¤ë¥¸ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•´ì„œ ì‹œë„í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì— ìì‹ ì˜ ë¬¸ì¥ì„ ì‘ì„±í•˜ì‹­ì‹œì˜¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRyH6XyOgBdz"
      },
      "source": [
        "# Change the sentence below to see your prediction. Make sure all the words are in the Glove embeddings.  \n",
        "x_test = np.array(['not feeling happy'])\n",
        "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
        "print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGxn67yegBS6"
      },
      "source": [
        "## LSTM version accounts for word order\n",
        "\n",
        "* ì´ì „ì—ëŠ” Emojify-V1 ëª¨ë¸ì´ \"not feeling happy\"ë¼ëŠ” ë ˆì´ë¸”ì„ ì˜¬ë°”ë¥´ê²Œ í‘œì‹œí•˜ì§€ ì•Šì•˜ì§€ë§Œ Emojiy-V2 êµ¬í˜„ì—ì„œëŠ” í•´ë‹¹ ë‚´ìš©ì´ ì˜¬ë°”ë¥´ê²Œ ì‘ë™í–ˆìŠµë‹ˆë‹¤.\n",
        "  * (Kerasì˜ ì¶œë ¥ì€ ë§¤ë²ˆ ì•½ê°„ ë¬´ì‘ìœ„ì´ë¯€ë¡œ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì–»ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)\n",
        "* í˜„ì¬ ëª¨ë¸ì€ ì—¬ì „íˆ â€‹â€‹ë¶€ì • í‘œí˜„ì„ ì´í•´í•˜ëŠ” ë° ë§¤ìš° ê°•ë ¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (ì˜ˆ : \"not happy\").\n",
        "  * ì´ê²ƒì€ í›ˆë ¨ ì„¸íŠ¸ê°€ ì‘ê³  ë¶€ì • í‘œí˜„ì˜ ì˜ˆê°€ ë§ì§€ ì•Šê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
        "  * ê·¸ëŸ¬ë‚˜ í›ˆë ¨ ì„¸íŠ¸ê°€ ë” í¬ë©´ LSTM ëª¨ë¸ì´ ì´ëŸ¬í•œ ë³µì¡í•œ ë¬¸ì¥ì„ ì´í•´í•˜ëŠ” ë° Emojify-V1 ëª¨ë¸ë³´ë‹¤ í›¨ì”¬ ë‚«ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7dps2qjf9pv"
      },
      "source": [
        "### Congratulations!\n",
        "\n",
        "ì´ ê³¼ì œë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤! â¤ï¸â¤ï¸â¤ï¸\n",
        "\n",
        "\n",
        "## What you should remember\n",
        "- í›ˆë ¨ ì„¸íŠ¸ê°€ ì‘ì€ NLP ì‘ì—…ì´ìˆëŠ” ê²½ìš° word embeddingì„ ì‚¬ìš©í•˜ë©´ ì•Œê³ ë¦¬ì¦˜ì— í¬ê²Œ ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "- Word embeddingì„ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ì´ í•™ìŠµ ì„¸íŠ¸ì— ë‚˜íƒ€ë‚˜ì§€ ì•Šì„ ìˆ˜ë„ ìˆëŠ” í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ë‹¨ì–´ì— ëŒ€í•´ ì‘ì—… í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "- Keras (ë° ëŒ€ë¶€ë¶„ì˜ ë‹¤ë¥¸ ë”¥ ëŸ¬ë‹ í”„ë ˆì„ ì›Œí¬)ì˜ í›ˆë ¨ ì‹œí€€ìŠ¤ ëª¨ë¸ì—ëŠ” ëª‡ ê°€ì§€ ì¤‘ìš”í•œ ì„¸ë¶€ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n",
        "  - ë¯¸ë‹ˆ ë°°ì¹˜ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ë¯¸ë‹ˆ ë°°ì¹˜ì˜ ëª¨ë“  ì˜ˆì œê°€ **ë™ì¼í•œ ê¸¸ì´**ë¥¼ ê°–ë„ë¡ ì‹œí€€ìŠ¤ë¥¼ **íŒ¨ë”©**í•´ì•¼í•©ë‹ˆë‹¤.\n",
        "  - `Embedding()` ë ˆì´ì–´ëŠ” ì‚¬ì „ í›ˆë ¨ ëœ ê°’ìœ¼ë¡œ ì´ˆê¸°í™” í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "    - ì´ëŸ¬í•œ ê°’ì€ ê³ ì •ë˜ê±°ë‚˜ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ì¶”ê°€ í•™ìŠµ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "    - ê·¸ëŸ¬ë‚˜ ë¼ë²¨ì´ ì§€ì •ëœ ë°ì´í„° ì„¸íŠ¸ê°€ ì‘ë‹¤ë©´ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ì „ í•™ìŠµ ëœ ëŒ€ê·œëª¨ ì„ë² ë”© ì„¸íŠ¸ë¥¼ í•™ìŠµì‹œí‚¬ ê°€ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤.\n",
        "    - `LSTM ()`ì—ëŠ”`return_sequences`ë¼ëŠ” í”Œë˜ê·¸ê°€ìˆì–´ ëª¨ë“  ìˆ¨ê²¨ì§„ ìƒíƒœë¥¼ ë°˜í™˜í• ì§€ ì•„ë‹ˆë©´ ë§ˆì§€ë§‰ ìƒíƒœ ë§Œ ë°˜í™˜ í• ì§€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.\n",
        "    - `LSTM ()`ë°”ë¡œ ë’¤ì—`Dropout ()`ì„ ì‚¬ìš©í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ë¥¼ ì •ê·œí™” í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8O2lF_rgpgn"
      },
      "source": [
        "#### Input sentences:\n",
        "```Python\n",
        "\"Congratulations on finishing this assignment and building an Emojifier.\"\n",
        "\"We hope you're happy with what you've accomplished in this notebook!\"\n",
        "```\n",
        "#### Output emojis:\n",
        "# ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig7JaUJKgrVq"
      },
      "source": [
        "## Acknowledgments\n",
        "\n",
        "Thanks to Alison Darcy and the Woebot team for their advice on the creation of this assignment. \n",
        "* Woebot is a chatbot friend that is ready to speak with you 24/7. \n",
        "* Part of Woebot's technology uses word embeddings to understand the emotions of what you say. \n",
        "* You can chat with Woebot by going to http://woebot.io\n",
        "\n",
        "<img src=\"arts/woebot.png\" style=\"width:600px;height:300px;\">"
      ]
    }
  ]
}