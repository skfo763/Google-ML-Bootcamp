{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_machine_translation_with_attention_v4a",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_6SWQg0KcSd"
      },
      "source": [
        "# Neural Machine Translation\n",
        "\n",
        "이번 주차 첫 번째 과제에 오신것을 환영합니다!\n",
        "\n",
        "- 여러분은 이번 과제를 통해서 인간이 읽을 수 있는 날짜 표현(\"25th of June, 2009\")을 기계가 인식할 수 있는 날짜 표현(\"2009-06-25\")으로 변환하는 Neural Machine Translation(NMT) 모델을 구현합니다.\n",
        "- 위 내용은 \"attention model\" 개념을 사용해 구현할 수 있으며, 이는 보다 정교한 sequence-to-sequence 모델의 한 종류입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS4jcMvLK4zg"
      },
      "source": [
        "이번 과제에 필요한 패키지를 불러와 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGZJpjBJK6tn"
      },
      "source": [
        "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
        "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import load_model, Model\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "\n",
        "from faker import Faker\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from babel.dates import format_date\n",
        "from nmt_utils import *\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjjKzwOYK88q"
      },
      "source": [
        "## 1 - Translating human readable dates into machine dates\n",
        "\n",
        "- 여기서 구축 할 모델은 영어에서 힌디어로 번역하는 것과 같이 한 언어에서 다른 언어로 번역하는 데 사용할 수 있습니다.\n",
        "- 그러나 언어 번역에는 방대한 데이터 세트가 필요하며 일반적으로 GPU 교육에 며칠이 걸립니다.\n",
        "- 대규모 데이터 세트를 사용하지 않고 이러한 모델을 실험하기 위해 더 간단한 \"날짜 변환\" 작업을 수행합니다.\n",
        "- 인공 신경망에는 다양한 형식으로 작성된 날짜가 입력됩니다 (*예 : '1958 년 8 월 29 일', '1968 년 3 월 30 일', '1987 년 6 월 24 일'*)\n",
        "- 신경망은 이 날짜를 기계가 읽을 수 있는 표준화 된 날짜 형식으로 변환합니다 (*예 : \"1958-08-29\", \"1968-03-30\", \"1987-06-24\"*).\n",
        "- 우리는 신경망이 YYYY-MM-DD의 일반적인 기계 판독 형식으로 날짜를 출력하는 방법을 배우게 할 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzWwF92dLcvb"
      },
      "source": [
        "### 1.1 - Dataset\n",
        "\n",
        "사람이 읽을 수있는 10,000개의 날짜와 이에 상응하는 표준화 된 기계가 읽을 수 있는 날짜로 구성된 데이터 세트를 통해 모델을 학습합니다. 다음 셀을 실행하여 데이터 세트를 로드하고 몇 가지 예시를 출력 해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq4QFB_rLcbM"
      },
      "source": [
        "m = 10000\n",
        "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3EH2U46ME0W"
      },
      "source": [
        "dataset[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geJrUYyHMGY3"
      },
      "source": [
        "불러온 데이터는 다음과 같습니다.\n",
        "- `dataset` : (human readable date, machine readable date) 형태의 튜플 데이터들을 담고 있는 리스트입니다.\n",
        "- `human_vocab` : human-readable date와 정수 값의 인덱스를 맵핑하고 있는 파이썬 딕셔너리입니다.\n",
        "- `machine_vocab` : machine-readable date와 정수 값인 인덱스를 맵핑하고 있는 파이썬 딕셔너리 입니다.\n",
        "  - **주의** : 이 인덱스는 반드시 `human_vocab`의 인덱스와 일치하지 않습니다.\n",
        "- `inv_matchine_vocab` : 문자열과 인덱스들을 거꾸로 맵핑하는, `machine_vocab`의 뒤집어진 딕셔너리입니다.\n",
        "\n",
        "데이터를 전처리하고 원시 텍스트 데이터를 인덱스 값에 매핑 해 보겠습니다.\n",
        "- Tx = 30으로 설정하겠습니다.\n",
        "  - Tx는 사람이 읽을 수있는 날짜의 최대 길이라고 가정합니다.\n",
        "  - 더 긴 입력이 들어오면 잘라야합니다.\n",
        "- Ty = 10으로 설정하겠습니다.\n",
        "  - \"YYYY-MM-DD\"는 10 자입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsNuyEm8M8BS"
      },
      "source": [
        "Tx = 30\n",
        "Ty = 10\n",
        "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
        "\n",
        "print(\"X.shape:\", X.shape)\n",
        "print(\"Y.shape:\", Y.shape)\n",
        "print(\"Xoh.shape:\", Xoh.shape)\n",
        "print(\"Yoh.shape:\", Yoh.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BRIetqxM-qv"
      },
      "source": [
        "이제 우리는 다음의 데이터를 가지고 있습니다.\n",
        "- `X` : 훈련 데이터 세트에서, 인간이 읽을 수 있는 형식의 날짜를 전처리한 데이터입니다.\n",
        "  - X의 각 문자열은 정수형 index로 대체되었습니다. `human_vocab`를 사용하여 이 index에 맵핑된 문자열을 얻을 수 있습니다.\n",
        "  - 각각의 날짜는 고정된 길이 $T_x$에 맞추기 위해, (<pad>) 라고 하는 특별한 문자로 패딩되었습니다.\n",
        "  - `X.shape = (m, Tx)`에서, m은 한 배치에 들어 있는 훈련 데이터의 갯수입니다.\n",
        "- `Y` : 기계가 읽을 수 있는 형식의 날짜를 전처리한 데이터 입니다.\n",
        "  - Y의 각 문자열은 `machine_vocab`에 맵핑된 index 값으로 대체되었습니다.\n",
        "  - `Y.shape = (m, Ty)`\n",
        "- `Xoh` : `X`의 one-hot 인코딩 벡터입니다.\n",
        "  - X의 각 index는 one-hot 인코딩 버전으로 변환되었습니다.(예를 들어 index가 2라면, one-hot 버전은 2번째 포지션이 1이고 나머지 포지션이 0인 형식을 취합니다.)\n",
        "  - `Xoh.shape = (m, Tx, len(human_vocab))`\n",
        "- `Yoh` : `Y`의 one-hot 인코딩 벡터입니다.\n",
        "  - Y의 각 index를 one-hot 인코딩 버전으로 변환시킵니다.\n",
        "  - `Yoh.shape = (m, Tx, len(machine_vocab))`\n",
        "  - `len(machine_vocab) = 11`에서, 기계는 0-9까지의 10가지 숫자와 '-'를 포함한 총 11개의 텍스트 심볼을 읽을 수 있습니다.\n",
        "\n",
        "\n",
        "- 전처리된 데이터 셋의 몇 가지 다른 예시도 살펴 보겠습니다.\n",
        "- 데이터 세트를 탐색하고 소스/타겟 날짜가 전처리되는 방식을 확인하려면 아래 셀의 'index'를 자유롭게 변경하세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGa_WzSeOV-5"
      },
      "source": [
        "index = 0\n",
        "print(\"Source date:\", dataset[index][0])\n",
        "print(\"Target date:\", dataset[index][1])\n",
        "print()\n",
        "print(\"Source after preprocessing (indices):\", X[index])\n",
        "print(\"Target after preprocessing (indices):\", Y[index])\n",
        "print()\n",
        "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
        "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T97iD2x6OiXS"
      },
      "source": [
        "## 2 - Neural machine translation with attention\n",
        "\n",
        "* 책의 문단을 프랑스어에서 영어로 번역해야한다면 전체 문단을 읽지 않고 책을 닫고 번역합니다.\n",
        "* 번역 과정 중에도, 여러분은 읽고/다시 읽고 적어 놓은 영어 부분에 해당하는 프랑스어 단락 부분에 집중합니다.\n",
        "* attention 매커니즘은 Neural Machine Translation 모델에 어느 단계에서든 주의를 기울여야하는 위치를 알려줍니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc8QAgQeO5OP"
      },
      "source": [
        "### 2.1 - Attention mechanism\n",
        "\n",
        "이 파트에서, 여러분은 동영상 강의에서 소개된 attention 매커니즘을 구현합니다.\n",
        "\n",
        "- 아래에 모델이 어떻게 동작하는지에 대한 그림이 있습니다.\n",
        "  - 왼쪽 그림은 attention 모델을 보여줍니다.\n",
        "  - 오른쪽 그림은, 하나의 \"attention\" 단계가 변수 $\\alpha^{\\langle t, t' \\rangle}$를 계산하는 과정을 보여줍니다.\n",
        "  - attention 변수 $\\alpha^{\\langle t, t' \\rangle}$는 매 time step ($t=1, \\ldots, T_y$)마다, 맥락 변수 $context^{\\langle t \\rangle}$를 구하는데 사용됩니다.\n",
        "\n",
        "<table>\n",
        "<td> \n",
        "<img src=\"arts/attn_model.png\" style=\"width:500;height:500px;\"> <br>\n",
        "</td> \n",
        "<td> \n",
        "<img src=\"arts/attn_mechanism.png\" style=\"width:500;height:500px;\"> <br>\n",
        "</td> \n",
        "</table>\n",
        "<center>그림 1 - attention 모델을 사용한 Neural machine translation</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynMJFiqtPeQU"
      },
      "source": [
        "이 모델에서 여러분이 기억할 몇 가지 특성들이 있습니다.\n",
        "\n",
        "#### Pre-attention and Post-attention LSTMs on both sides of attention mechanism\n",
        "\n",
        "- 왼쪽 그림에서 볼 수 있듯이, 이 모델에는 두 가지 독립적인 LSTM 모델이 있습니다 : Pre-attention과 Post-attention LSTM 모델입니다.\n",
        "- **Pre-attention Bi-LSTM** : 이 모델은 사진 하단에 있는 양방향의 LSTM 모델이고, attention 매커니즘 이전에 계산됩니다.\n",
        "  - attention mechanism은 왼쪽 다이어그램의 중간에 보이는 그림입니다.\n",
        "  - Bi-LSTM인 pre-attention은 $T_x$ time step을 거쳐서 수행됩니다.\n",
        "- **Post-attention LSTM** : 이 모델은 hidden state $s^{\\langle t \\rangle}$와 cell state $c^{\\langle t \\rangle}$를 한 time step에서 다른 time step으로 전달합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osjGI-siQnRX"
      },
      "source": [
        "#### An LSTM has both a hidden state and cell state\n",
        "\n",
        "\n",
        "* 강의 비디오에서는 Post-attention 시퀀스 모델에 기본적인 RNN 만 사용했습니다.\n",
        "  * 이것은 RNN에 의해 ​​캡처 된 상태가 hidden state $ s ^ {\\langle t \\rangle} $ 만 출력했음을 의미합니다.\n",
        "* 이 과제에서는 기본 RNN 대신 LSTM을 사용하고 있습니다.\n",
        "  * 따라서 LSTM은 hidden state $ s ^ {\\langle t \\rangle} $와 cell state $ c ^ {\\langle t \\rangle} $를 모두 갖습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcE8YSTyQ2eM"
      },
      "source": [
        "#### Each time step does not use predictions from the previous time step\n",
        "\n",
        "\n",
        "* 코스 초반의 텍스트 생성 예제와 달리, 이 모델에서는 $ t $ time step의 post-attention LSTM이 이전 time step의 예측 $ y ^ {\\langle t-1 \\rangle} $을 입력으로 사용하지 않습니다.\n",
        "* Time step $t$에서의 post-attention LSTM 는 hidden state $ s ^ {\\langle t \\rangle} $ 및 cell state $ c ^ {\\langle t \\rangle} $ 만 입력으로 사용합니다.\n",
        "* 언어 생성 (인접한 문자가 높은 상관 관계가 있는 경우)과 달리 YYYY-MM-DD 날짜의 경우 이전 문자와 다음 문자 사이에 강한 종속성이 없기 때문에 이러한 방식으로 모델을 설계했습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08GdlATgRTSy"
      },
      "source": [
        "#### Concatenation of hidden states from the forward and backward pre-attention LSTMs\n",
        "\n",
        "\n",
        "- $ \\overrightarrow {a} ^ {\\langle t \\rangle} $ : 정방향 pre-attention LSTM의 hidden state.\n",
        "- $ \\overleftarrow {a} ^ {\\langle t \\rangle} $ : 역방향, pre-attention LSTM의 hidden state.\n",
        "- $ a ^ {\\langle t \\rangle} = [\\overrightarrow {a} ^ {\\langle t \\rangle}, \\overleftarrow {a} ^ {\\langle t \\rangle}] $ : Pre-attention Bi-LSTM의 정방향 $ \\ \\overrightarrow {a} ^ {\\langle t \\rangle} $ 및 역방향 $ \\overleftarrow {a} ^ {\\langle t \\rangle} $를 연결한 최종 활성화 값."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5NHSpWoR1a7"
      },
      "source": [
        "#### Computing \"energies\" $e^{\\langle t, t' \\rangle}$ as a function of $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t' \\rangle}$\n",
        "\n",
        "- \"Attention Model\" 동영상 강의의 6:45부터 8:16 사이의 내용을 기억해봅시다. \"e\"라는 변수는 $a^{\\langle t \\rangle}$ 와 $s^{\\langle t-1 \\rangle}$의 함수로 정의합니다.\n",
        "  - \"e\"는 \"energy\" 변수로 부릅니다.\n",
        "  - $s^{\\langle t-1 \\rangle}$ post-attention LSTM의 hidden state입니다.\n",
        "  - $a^{\\langle t' \\rangle}$ 는 pre-attention LSTM의 hidden state입니다.\n",
        "  - $s^{\\langle t-1 \\rangle}$과 $a^{\\langle t \\rangle}$는 간단한 인공 신경망에 투입되어, 출력값인 $e^{\\langle t, t' \\rangle}$를 구하는 함수를 학습합니다.\n",
        "  - $e^{\\langle t, t' \\rangle}$ 값은 $y^{\\langle t \\rangle}$가 특정 $a^{\\langle t' \\rangle}$를 고려해야 하는 크기를 나타내는 attention 변수인 $a^{\\langle t, t' \\rangle}$를 계산하는데 사용됩니다.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V30KSPMDajW6"
      },
      "source": [
        "- 그림 1의 오른쪽에 나타난 다이어그램에선, $s^{\\langle t-1 \\rangle}$를 $T_x$ 번 만큼 복사하기 위해 `RepeatVector` 노드를 사용합니다.\n",
        "- 이후 `Concatnation`을 사용하여 $s^{\\langle t-1 \\rangle}$과 $a^{\\langle t \\rangle}$를 이어붙입니다.\n",
        "- $s^{\\langle t-1 \\rangle}$과 $a^{\\langle t \\rangle}$가 이어붙여진 값은 \"Dense\" 레이어에 투입되고, 출력으로 $e^{\\langle t, t' \\rangle}$를 계산합니다.\n",
        "- $e^{\\langle t, t' \\rangle}$는 softmax 함수를 통과하여 $\\alpha^{\\langle t, t' \\rangle}$를 계산하는데 사용됩니다.\n",
        "- 위의 다이어그램은 명시적으로 $e^{\\langle t, t' \\rangle}$를 표시하지 않지만, $e^{\\langle t, t' \\rangle}$는 그림 1의 오른쪽에 있는 Dense 레이어와 softmax 레이어 아래에 있습니다.\n",
        "- 케라스에서 `RepeatVector`와 `Concatenation`를 사용하는 방법을 아래에서 설명하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPKrvnsVbn7r"
      },
      "source": [
        "### Implementation Details\n",
        "\n",
        "Neural Translator를 구해봅시다. `one_step_attention()`과 `model()` 두 함수를 구현하는 것으로 시작해 보겠습니다.\n",
        "\n",
        "#### one_set_attention()\n",
        "- 특정 time step $t$에서의 `one_step_attention()` 함수의 입력은 다음과 같습니다.\n",
        "  - $[a^{<1>},a^{<2>}, ..., a^{<T_x>}]$ : pre-attention Bi-LSTM의 모든 hidden state\n",
        "  - $s^{<t-1>}$ : post-attention LSTM의 이전 hidden state\n",
        "- `one_step_attention()` 함수는 다음을 계산합니다.\n",
        "  - $[\\alpha^{<t,1>},\\alpha^{<t,2>}, ..., \\alpha^{<t,T_x>}]$ : attention 가중치\n",
        "  - $context^{ \\langle t \\rangle }$ : context 벡터\n",
        "  $$context^{<t>} = \\sum_{t' = 1}^{T_x} \\alpha^{<t,t'>}a^{<t'>}\\tag{1}$$ \n",
        "\n",
        "\n",
        "#### Clarifing 'context' and 'c'\n",
        "- 강의에서, 컨텍스트는 $c^{\\langle t \\rangle}$로 표기됩니다.\n",
        "- 이번 과제에서는, 컨텍스트 변수를 we are calling the context $context^{\\langle t \\rangle}$로 표기하겠습니다.\n",
        "  - 이는 post-attention LSTM의 내부 메모리 cell 변수 $c^{\\langle t \\rangle}$와 구분하기 위해서입니다. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZLupkHZc7lP"
      },
      "source": [
        "#### Implement `one_step_attention`\n",
        "\n",
        "**연습 문제** : `one_step_attention()` 함수를 구현하세요.\n",
        "\n",
        "- `model()` 함수는 반복문을 사용해서 `one_step_attention()` $T_y$의 레이어를 호출합니다.\n",
        "- 모든 $T_y$의 복사본이 동일한 가중치를 가지는 것이 중요합니다.\n",
        "  - 매번 가중치를 다시 초기화해선 안됩니다.\n",
        "  - 다시 말해, 모든 $T_y$ 단계에서 가중치가 공유되어야 합니다.\n",
        "- 케라스에서 공유 가능한 가중치로 레이어를 구현하는 방법은 다음과 같습니다.\n",
        "  1. `one_step_attention` 함수 외부에 있는 범위에서 레이어를 정의합니다. 예를 들어 객체를 전역 변수로 정의하세요.\n",
        "    - `model` 함수 내부에서 레이어 객체를 정의해도 기술적으로는 잘 동작합니다. `model`함수 내부에서 `one_step_attention`함수를 호출하기 때문입니다. 그러나 이번 과제에선 채점의 편의를 위해 레이어 객체를 전역변수로 선언하겠습니다. 자동 채점 앨고리즘은 이 변수를 전역 변수로 예상하고 채점합니다.\n",
        "  2. 입력을 넘길 때 이 전역 변수 객체를 호출합니다.\n",
        "- 필요한 레이어를 전역 변수로 정의했습니다.\n",
        "  - 전역 변수 객체를 생성하려면 아래 코드를 실행하세요.\n",
        "  - 채점 알고리즘은 주어진 변수 이름을 그대로 사용합니다. 따라서 전역 변수의 이름을 임의로 바꾸지 마세요.\n",
        "- 이 공유 가능 레이어에 대해 자세히 알아보려면 케라스의 문서를 확인하세요. 레이어는 함수입니다. 아래는 일련의 함수를 어떻게 호출하는지에 대한 내용입니다.\n",
        "    - [RepeatVector()](https://keras.io/layers/core/#repeatvector)\n",
        "```Python\n",
        "var_repeated = repeat_layer(var1)\n",
        "```\n",
        "    - [Concatenate()](https://keras.io/layers/merge/#concatenate)   \n",
        "```Python\n",
        "concatenated_vars = concatenate_layer([var1,var2,var3])\n",
        "```\n",
        "    - [Dense()](https://keras.io/layers/core/#dense)  \n",
        "```Python\n",
        "var_out = dense_layer(var_in)\n",
        "```\n",
        "    - [Activation()](https://keras.io/layers/core/#activation)  \n",
        "```Python\n",
        "activation = activation_layer(var_in)  \n",
        "```\n",
        "    - [Dot()](https://keras.io/layers/merge/#dot)  \n",
        "```Python\n",
        "dot_product = dot_layer([var1,var2])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o1mn8EUPb9P"
      },
      "source": [
        "# Defined shared layers as global variables\n",
        "repeator = RepeatVector(Tx)\n",
        "concatenator = Concatenate(axis=-1)\n",
        "densor1 = Dense(10, activation = \"tanh\")\n",
        "densor2 = Dense(1, activation = \"relu\")\n",
        "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
        "dotor = Dot(axes = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pl7TQ6sFeVEg"
      },
      "source": [
        "def one_step_attention(a, s_prev):\n",
        "    \"\"\"\n",
        "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
        "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
        "    \n",
        "    Arguments:\n",
        "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
        "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
        "    \n",
        "    Returns:\n",
        "    context -- context vector, input of the next (post-attention) LSTM cell\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
        "    s_prev = None\n",
        "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
        "    # For grading purposes, please list 'a' first and 's_prev' second, in this order.\n",
        "    concat = None\n",
        "    # Use densor1 to propagate concat through a small fully-connected neural network to compute the \"intermediate energies\" variable e. (≈1 lines)\n",
        "    e = None\n",
        "    # Use densor2 to propagate e through a small fully-connected neural network to compute the \"energies\" variable energies. (≈1 lines)\n",
        "    energies = None\n",
        "    # Use \"activator\" on \"energies\" to compute the attention weights \"alphas\" (≈ 1 line)\n",
        "    alphas = None\n",
        "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
        "    context = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K85oJqquflb0"
      },
      "source": [
        "`model()`함수를 구현한 이후 `one_step_attention()`의 예상 출력을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH3W-nUhfs0g"
      },
      "source": [
        "#### model\n",
        "\n",
        "- `model`은 먼저 Bi-LSTM을 통해 입력을 실행하여 $[a ^ {<1>}, a ^ {<2>}, ..., a ^ {<T_x>}] $를 얻습니다.\n",
        "- 이후 `model` 함수는 반복문을 사용하여 `one_step_attention`을 $T_y$번 호출합니다. 이 루프가 반복될 때마다.\n",
        "  - 계산된 컨텍스트 벡터 $context^{<t>}$를 post-attention LSTM에 투입합니다.\n",
        "  - softmax 활성화 함수와 dense layer를 통해서 post-attention LSTM의 출력을 실행합니다.\n",
        "  - softmax 함수는 예측 결과 $\\hat{y}^{<t>}$를 계산합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2lGHT3_gYlz"
      },
      "source": [
        "**연습 문제** : 그림 1과 위의 텍스트에 설명 된대로 `model()` 함수를 구현합니다. 다시 말하지만,`model()` 에서 사용할 가중치를 공유 할 전역 레이어를 정의했습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXG9-lMLgV28"
      },
      "source": [
        "n_a = 32 # number of units for the pre-attention, bi-directional LSTM's hidden state 'a'\n",
        "n_s = 64 # number of units for the post-attention LSTM's hidden state \"s\"\n",
        "\n",
        "# Please note, this is the post attention LSTM cell.  \n",
        "# For the purposes of passing the automatic grader\n",
        "# please do not modify this global variable.  This will be corrected once the automatic grader is also updated.\n",
        "post_activation_LSTM_cell = LSTM(n_s, return_state = True) # post-attention LSTM \n",
        "output_layer = Dense(len(machine_vocab), activation=softmax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78NzWWprgmnC"
      },
      "source": [
        "이제 반복문에서 이러한 레이어를 $ T_y $ 번 사용하여 출력을 생성 할 수 있으며 해당 파라미터는 매 반복마다 다시 초기화되지 않습니다. 다음 단계를 수행해야합니다.\n",
        "\n",
        "1. 입력`X`를 양방향 LSTM으로 전파합니다.\n",
        "  * [양방향](https://keras.io/layers/wrappers/#bidirectional)\n",
        "  * [LSTM](https://keras.io/layers/recurrent/#lstm)\n",
        "  * LSTM이 마지막 hidden state 대신 전체 시퀀스를 반환하기를 원한다는 것을 기억하십시오.\n",
        "  * 샘플 코드 :\n",
        "  ```python\n",
        "  sequence_of_hidden_states = Bidirectional(LSTM(units=..., return_sequences=...))(the_input_X)\n",
        "  ```\n",
        "2. $ t = 0, \\cdots, T_y-1 $에 대해 반복합니다.\n",
        "  1. `one_step_attention ()`을 호출하여 hidden state를 전달합니다. $context^{<t>}$를 계산하기 위해 양방향 LSTM인 pre-attention LSTM의 hideen state $[a^{\\langle 1 \\rangle},a^{\\langle 2 \\rangle}, ..., a^{ \\langle T_x \\rangle}]$와 post-attention LSTM의 이전 hidden state $s^{<t-1>}$를 입력받습니다.\n",
        "  2. $context^{<t>}$를 post-attention LSTM cell에 입력합니다.\n",
        "    - 이 LSTM의 이전 hidden state $ s ^ {\\langle t-1 \\rangle} $ 및 cell state $ c ^ {\\langle t-1 \\rangle} $를 전달해야합니다.\n",
        "    - 아래 샘플 코드와 같이 하면 새로운 hidden state $s^{\\langle t \\rangle}$와 cell state $c^{\\langle t \\rangle}$가 출력됩니다.\n",
        "    - 샘플 코드 :\n",
        "    ```python\n",
        "    next_hidden_state, _ , next_cell_state = \n",
        "            post_activation_LSTM_cell(inputs=..., initial_state=[prev_hidden_state, prev_cell_state])\n",
        "    ```\n",
        "  여기서 레이어는 실제 post attention LSTM cell입니다. 자동 채점을 위해 전역 변수의 이름을 임의로 수정하지 마세요. \n",
        "  3. $ s ^ {<t>} $에 dense 레이어와 softmax 레이어를 적용하고 결과를 얻습니다.\n",
        "    - 샘플 코드 : \n",
        "    ```python\n",
        "        output = output_layer(inputs=...)\n",
        "    ```\n",
        "  4. 이 결과를 `outputs` 리스트에 추가합니다.\n",
        "3. 케라스 모델 인스턴스를 생성합니다.\n",
        "  - 세 가지 인자를 입력합니다.\n",
        "    - `X` : $(T_x, humanVocabSize)$의 shape를 가진 one-hot 인코딩 벡터입니다.\n",
        "    - $ s ^ {\\langle 0 \\rangle} $ : post-attention LSTM의 초기 hidden state\n",
        "    - $ c ^ {\\langle 0 \\rangle} $) : post-attention LSTM의 초기 cell state\n",
        "  - 이 모델의 출력 결과는 `outputs` 리스트입니다.\n",
        "  - 샘플 코드 :\n",
        "  ```python\n",
        "    model = Model(inputs=[...,...,...], outputs=...)\n",
        "  ```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUstikXojr-F"
      },
      "source": [
        "# GRADED FUNCTION: model\n",
        "\n",
        "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    Tx -- length of the input sequence\n",
        "    Ty -- length of the output sequence\n",
        "    n_a -- hidden state size of the Bi-LSTM\n",
        "    n_s -- hidden state size of the post-attention LSTM\n",
        "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
        "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
        "\n",
        "    Returns:\n",
        "    model -- Keras model instance\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define the inputs of your model with a shape (Tx,)\n",
        "    # Define s0 (initial hidden state) and c0 (initial cell state)\n",
        "    # for the decoder LSTM with shape (n_s,)\n",
        "    X = Input(shape=(Tx, human_vocab_size))\n",
        "    s0 = Input(shape=(n_s,), name='s0')\n",
        "    c0 = Input(shape=(n_s,), name='c0')\n",
        "    s = s0\n",
        "    c = c0\n",
        "    \n",
        "    # Initialize empty list of outputs\n",
        "    outputs = []\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    \n",
        "    # Step 1: Define your pre-attention Bi-LSTM. (≈ 1 line)\n",
        "    a = None\n",
        "    \n",
        "    # Step 2: Iterate for Ty steps\n",
        "    for t in range(None):\n",
        "    \n",
        "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
        "        context = None\n",
        "        \n",
        "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
        "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
        "        s, _, c = None\n",
        "        \n",
        "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
        "        out = None\n",
        "        \n",
        "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
        "        None\n",
        "    \n",
        "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
        "    model = None\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGQQqnsdjwF7"
      },
      "source": [
        "다음 셀을 실행하여 모델을 만듭니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLmxpD-ZjxA7"
      },
      "source": [
        "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2I_cPd9Mjycw"
      },
      "source": [
        "#### Troubleshooting Note\n",
        "\n",
        "* 처음에 \"모델\"을 잘못 구현 한 후 반복되는 오류가 발생하지만 오류를 수정했다고 생각되는 경우에도 모델을 빌드 할 때 오류 메시지가 계속 표시 될 수 있습니다.\n",
        "* 해결책은 모든 데이터를 저장하고 커널을 다시 시작 (또는 노트북을 종료 한 다음 다시 시작)하고 셀을 다시 실행하는 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHbn5VZ9kHKt"
      },
      "source": [
        "예상 출력과 일치하는지 확인하기 위해 완성된 모델에 대한 요약을 가져옵니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IfjZvz5kIXS"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sNC0D2ikLTG"
      },
      "source": [
        "**모범 답안**:\n",
        "\n",
        "아래는 여러분이 확인하실 수 있는 모델의 요약 자료입니다.\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **Total params:**\n",
        "        </td>\n",
        "        <td>\n",
        "         52,960\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **Trainable params:**\n",
        "        </td>\n",
        "        <td>\n",
        "         52,960\n",
        "        </td>\n",
        "    </tr>\n",
        "            <tr>\n",
        "        <td>\n",
        "            **Non-trainable params:**\n",
        "        </td>\n",
        "        <td>\n",
        "         0\n",
        "        </td>\n",
        "    </tr>\n",
        "                    <tr>\n",
        "        <td>\n",
        "            **bidirectional_1's output shape **\n",
        "        </td>\n",
        "        <td>\n",
        "         (None, 30, 64)  \n",
        "        </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **repeat_vector_1's output shape **\n",
        "        </td>\n",
        "        <td>\n",
        "         (None, 30, 64) \n",
        "        </td>\n",
        "    </tr>\n",
        "                <tr>\n",
        "        <td>\n",
        "            **concatenate_1's output shape **\n",
        "        </td>\n",
        "        <td>\n",
        "         (None, 30, 128) \n",
        "        </td>\n",
        "    </tr>\n",
        "            <tr>\n",
        "        <td>\n",
        "            **attention_weights's output shape **\n",
        "        </td>\n",
        "        <td>\n",
        "         (None, 30, 1)  \n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **dot_1's output shape **\n",
        "        </td>\n",
        "        <td>\n",
        "         (None, 1, 64)\n",
        "        </td>\n",
        "    </tr>\n",
        "           <tr>\n",
        "        <td>\n",
        "            **dense_3's output shape **\n",
        "        </td>\n",
        "        <td>\n",
        "         (None, 11) \n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uyu7fXMRkRy_"
      },
      "source": [
        "#### Compile the model\n",
        "\n",
        "* Keras에서 모델을 생성 한 후 이를 컴파일하고 사용할 손실 함수, 최적화 함수 및 메트릭을 정의해야합니다.\n",
        "  * 손실 함수 : `categoical_crossentropy`\n",
        "  * 최적화 함수 : [Adam](https://keras.io/optimizers/#adam) [optimizer](https://keras.io/optimizers/#usage-of-optimizers)\n",
        "    - learning rate = 0.005 \n",
        "    - $\\beta_1 = 0.9$\n",
        "    - $\\beta_2 = 0.999$\n",
        "    - decay = 0.01  \n",
        "  * metric: 'accuracy'\n",
        "\n",
        "<br>\n",
        "\n",
        "샘플 코드\n",
        "```Python\n",
        "optimizer = Adam(lr=..., beta_1=..., beta_2=..., decay=...)\n",
        "model.compile(optimizer=..., loss=..., metrics=[...])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3Vcl0jckP1p"
      },
      "source": [
        "### START CODE HERE ### (≈2 lines)\n",
        "opt = None\n",
        "None\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQRQBMG9nUl9"
      },
      "source": [
        "#### Define inputs and outputs, and fit the model\n",
        "\n",
        "마지막 단계는 모든 입력과 출력을 모델에 맞게 정의하는 것입니다.\n",
        "- 훈련 데이터 세트가 포함 된 $ (m = 10000, T_x = 30) $ 모양의 입력 X가 있습니다.\n",
        "- 'post_attention_LSTM_cell'을 0으로 초기화하려면 's0'과 'c0'을 만들어야합니다.\n",
        "- 구현한 `model ()`이 주어지면 10 개의 데이터를 가지는 shape (m, T_y)리스트를 출력해야 합니다.\n",
        "     - `outputs [i] [0], ..., outputs [i] [Ty]`목록은 $ i ^ {th} $ 훈련 데이터 (`X [i]`에 해당하는 실제 레이블 (문자))을 나타냅니다. ).\n",
        "     - `outputs [i] [j]`는 $ i ^ {th} $ 훈련 데이터에서 $ j ^ {th} $ 문자의 실제 레이블입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRyGYVymnnNm"
      },
      "source": [
        "s0 = np.zeros((m, n_s))\n",
        "c0 = np.zeros((m, n_s))\n",
        "outputs = list(Yoh.swapaxes(0,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLwfgOnpnpaB"
      },
      "source": [
        "이제 모델을 맞추고 1 epoch 동안 실행 해 보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP2luqkony0-"
      },
      "source": [
        "훈련하는 동안 출력의 10 개 위치 각각에 대한 손실과 정확도를 볼 수 있습니다. 아래 표는 배치에 2 개의 훈련 데이터가 있는 경우 정확도가 어떻게 될 수 있는지에 대한 예를 제공합니다.\n",
        "\n",
        "<img src=\"arts/table.png\" style=\"width:700;height:200px;\">\n",
        "<center>따라서 `dense_2_acc_8 : 0.89`는 출력의 7 번째 문자를 현재 데이터 배치에서 89%의 시간 동안 정확하게 예측하고 있음을 의미합니다.</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ8gDVupoCED"
      },
      "source": [
        "이 모델을 더 오래 실행하고 가중치를 저장했습니다. 다음 셀을 실행하여 가중치를 로드합니다. (몇 분 동안 모델을 훈련하면 비슷한 정확도의 모델을 얻을 수 있지만 미리 훈련된 모델을 불러오면 시간이 절약됩니다.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aCH7gYFnpNB"
      },
      "source": [
        "model.load_weights('models/model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMQMb9fYoKzE"
      },
      "source": [
        "이제 새로운 데이터에서 결과를 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scC36evYoMlc"
      },
      "source": [
        "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
        "for example in EXAMPLES:\n",
        "    \n",
        "    source = string_to_int(example, Tx, human_vocab)\n",
        "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n",
        "    prediction = model.predict([source, s0, c0])\n",
        "    prediction = np.argmax(prediction, axis = -1)\n",
        "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
        "    \n",
        "    print(\"source:\", example)\n",
        "    print(\"output:\", ''.join(output),\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg_4t5oboWvp"
      },
      "source": [
        "이러한 훈련 데이터를 변경하여 자신만의 예제로 테스트 할 수도 있습니다. 다음 부분에서는 attention mechanism이 수행하는 작업, 즉 특정 출력 문자를 생성 할 때 네트워크가 주의를 기울이는 입력 부분에 대해 더 잘 이해할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8mM7DMhohHC"
      },
      "source": [
        "## 3 - Visualizing Attention (Optional / Ungraded)\n",
        "\n",
        "문제의 출력 길이가 10으로 고정되어 있으므로 10 개의 서로 다른 softmax unit을 사용하여 이 작업을 수행하여 10 개의 출력 문자를 생성 할 수도 있습니다. 그러나주의 모델의 한 가지 장점은 출력의 각 부분 (예 : 월)이 입력의 작은 부분(월을 나타내는 입력의 문자)에만 의존해야 한다는 것을 알고 있다는 것입니다. 출력의 각 부분이 입력의 어느 부분을 보고 있는지 시각화 할 수 있습니다.\n",
        "\n",
        "\"2018 년 5 월 9 일 토요일\"을 \"2018-05-09\"로 번역하는 작업이 있다고 생각해 보세요. 계산 된 $ \\alpha ^ {\\langle t, t '\\rangle} $를 시각화하면 다음과 같이됩니다.\n",
        "\n",
        "<img src=\"arts/date_attention.png\" style=\"width:600;height:300px;\"> \n",
        "<caption><center>그림 8 : Full Attention Map</center></caption>\n",
        "\n",
        "<br>\n",
        "\n",
        "출력이 입력의 \"토요일\"부분을 어떻게 무시하는지 확인하십시오. 출력 time step 중 어느 것도 입력의 해당 부분에 많은 주의를 기울이지 않습니다. 또한 9가 09로 번역되었고 5월이 05로 올바르게 번역되었음을 알 수 있습니다. 출력은 번역에 필요한 입력 부분에 주의를 기울입니다. \"년도\" 값은 대부분 \"2018\"을 생성하기 위해 입력의 \"18\"에 주의를 기울여야합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzlY5dgTpGV5"
      },
      "source": [
        "### 3.1 - Getting the attention weights from the network\n",
        "\n",
        "이제 신경망의 attention value를 시각화 해 보겠습니다. 신경망을 통해 데이터를 정방향 계산 한 다음 $ \\alpha ^ {\\langle t, t '\\rangle} $의 값을 시각화합니다.\n",
        "\n",
        "attention value가 어디에 있는지 알아 내기 위해 먼저 모델 요약을 출력 해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FssbXeYowea"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR4s1t9TpVn4"
      },
      "source": [
        "위의 `model.summary()`출력을 살펴보세요. 'dot_2'가 $ t = 0, \\ldots, T_y-1 $ 단계마다 컨텍스트 벡터를 계산하기 전에 'attention_weights'라는 레이어가 shape (m, 30, 1)의 'alpha'를 출력하는 것을 볼 수 있습니다. 이 레이어에서 attention weight를 가져 오겠습니다.\n",
        "\n",
        "`attention_map()`함수는 모델에서 주의 값을 가져 와서 그래프를 그리는 역할을 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK6vJ-9ypojI"
      },
      "source": [
        "attention_map = plot_attention_map(model, human_vocab, inv_machine_vocab, \"Tuesday 09 Oct 1993\", num = 7, n_s = 64);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9V9ce_dpu2J"
      },
      "source": [
        "생성 된 그래프에서 예측 된 출력의 각 문자에 대한 attention weight 값을 관찰 할 수 있습니다. 이 플롯을 조사하고 신경망이 주의를 기울이고있는 위치가 실제로 의미가 있는지 확인하십시오.\n",
        "\n",
        "날짜 번역 응용 프로그램에서 대부분의 시간 관련 attention은 연도를 예측하는 데 도움이 되며 날짜 나 월을 예측하는 데 큰 영향을 주지 않습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVzam7s_p7jB"
      },
      "source": [
        "### Congratulations!\n",
        "\n",
        "이 과제를 끝마쳤습니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEHgyK1lqAjp"
      },
      "source": [
        "## Here's what you should remember\n",
        "\n",
        "- Machine translation model을 사용하여 한 시퀸스를 사용해 다른 시퀸스로 맵핑할 수 있습니다. 이는 언어 번역(예 : 프랑스어 -> 영어) 번역 뿐만 아니라 날짜 형식과 같은 작업에도 유용합니다.\n",
        "- Attention mechanism은 출력의 특정 부분을 생성할 때 입력의 가장 관련성 있는 부분에 집중할 수 있습니다.\n",
        "- Attention mechanism을 사용하는 인공 신경망은 $T_x$ 길이의 입력에서, $T_y$ 길이의 출력으로 변환할 수 있습니다. 여기서 $T_x$와 $T_y$는 다를 수 있습니다.\n",
        "- Attention weight $\\alpha^{\\langle t,t' \\rangle}$를 시각화하여, 각 출력을 생성하는 동안 신경망이 주의를 기울이고 있는 부분을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hbanes--qswo"
      },
      "source": [
        "이 과제를 마치신 것을 축하드립니다! 이제 attention model을 구현하고 이를 사용하여 한 시퀀스에서 다른 시퀀스로의 복잡한 매핑을 학습 할 수 있습니다."
      ]
    }
  ]
}