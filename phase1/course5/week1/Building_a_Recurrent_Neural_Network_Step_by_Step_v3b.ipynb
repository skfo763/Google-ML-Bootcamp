{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Building_a_Recurrent_Neural_Network_Step_by_Step_v3b",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skfo763/Google-ML-Bootcamp-phase1/blob/main/course5/week1/Building_a_Recurrent_Neural_Network_Step_by_Step_v3b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuKYvLC7RVKB"
      },
      "source": [
        "# Building your Recurrent Neural Network - Step by Step\n",
        "\n",
        "5주차 첫 번째 과제에 오신것을 환영합니다! 이번 과제에서, 여러분들은 numpy를 사용하여 Recurrent Neural Network(순환 신경망: RNN) 의 핵심 구성 요소를 구현해볼 것입니다.\n",
        "\n",
        "RNN(Recurrent Neural Networks)은 \"메모리\"가 있기 때문에 자연어 처리 및 기타 시퀀스 작업에 매우 효과적입니다. 한 번에 하나씩 입력 $x ^ {\\langle t \\rangle}$ (예 : 단어)를 읽고 한 time step에서 다음 time step으로 전달되는 은닉층 활성화 값을 통해 해당 time step의 데이터에 대한 정보/컨텍스트를 기억할 수 있습니다. 이를 통해 단방향 RNN이 과거의 정보를 가져와 추후에 들어오는 입력을 처리 할 수 있습니다. 양방향 RNN은 과거와 미래 모두에서 정보 혹은 컨텍스트를 가져올 수 있습니다.\n",
        "\n",
        "**표기법**:\n",
        "- 위첨자 $[l]$는 $l$번째 레이어와 관련된 개체를 나타냅니다.\n",
        "- 위첨자 $(i)$는 훈련 데이터 세트의 $i$번째 데이터와 관련된 객체를 나타냅니다.\n",
        "- 위첨자 $\\langle t \\rangle$은 $t$ 번째 time step에 있는 객체를 나타냅니다.\n",
        "- 아래첨자 $i$는 벡터의 $i$ 번째 요소를 나타냅니다.\n",
        "\n",
        "예를 들어,\n",
        "- $a^{(2)[3]<4>}_5$ 라는 값은 는 두 번째 훈련 예제(2), 세 번째 레이어[3], 네 번째 time stop<4> 및 벡터의 다섯 번째 항목의 활성화 변수를 나타냅니다.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Pre-requisites\n",
        "- 이 과제는 여러분들이 이미 `numpy` 라이브러리에 익숙하다고 가정하고 진행됩니다.\n",
        "- numpy에 대한 지식을 리프레시하고 싶다면, 이 Specialization 코스의 1번 코스인 \"Neural Networks and Deep Learning\"를 검토 할 수 있습니다.\n",
        "  - 특히, 2 주차 과제 [ \"Python Basics with numpy (선택 사항)\"](https://www.coursera.org/learn/neural-networks-deep-learning/item/Zh0CU)를 다시 수행하세요.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### 시작 코드를 수정할 때주의하세요\n",
        "- 채점 대상이 되는 함수를 작업 할 때는 다음 주석 사이에 있는 코드 만 수정해야합니다.\n",
        "```python\n",
        "#### START CODE HERE\n",
        "```\n",
        "과\n",
        "```python\n",
        "#### END CODE HERE\n",
        "```\n",
        "* 특히, 채점 대상이 되는 루틴의 첫 번째 줄을 수정하지 않도록 주의하십시오. 다음으로 시작합니다.\n",
        "```python\n",
        "# GRADED FUNCTION : routine_name\n",
        "```\n",
        "- 자동 채점 알고리즘(autograder)은 특정 함수를 찾기 위해 다음 사항이 필요합니다.\n",
        "- 줄 간격/텍스트 간격이 변경되어오 자동 채점 알고리즘에 문제가 발생합니다.\n",
        "- 위의 사항들을 수정했거나, 필수 사항이 일부 누락 된 경우 'failed'를 반환합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flnGB0t3VOJj"
      },
      "source": [
        "이번 과제를 수행하는데 필요한 패키지들을 불러오는 것으로 이번 과제를 시작해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0euRsQ0TEGI"
      },
      "source": [
        "import numpy as np\n",
        "from rnn_utils import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQgQZvEvWVjl"
      },
      "source": [
        "## 1 - Forward propagation for the basic Recurrent Neural Network\n",
        "\n",
        "이번 주차 마지막에, 여러분은 RNN을 사용한 음악을 만들어볼 예정입니다. 앞으로 여러분이 구현할 RNN의 기본적인 구조는 아래 그림과 같습니다. 아래의 예시는 $T_x = T_y$인 예시입니다.\n",
        "\n",
        "<img src=\"arts/RNN.png\" style=\"width:500;height:300px;\">\n",
        "<center>그림 1 : 기본적인 RNN 모델</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpA9m8RXWw8r"
      },
      "source": [
        "### Dimensions of input $x$\n",
        "\n",
        "#### Input with $n_x$ number of units\n",
        "\n",
        "- 단일 입력 데이터의 단일 time step의 경우, $x^{(i) \\langle t \\rangle}$는 1차원 input 벡터입니다.\n",
        "- 언어를 하나의 예시로 삼아보자면, 5000개의 단어를 가진 언어는 5000개의 unit을 가진 one-hot encoding 벡터로 나타낼 수 있습니다. 따라서 $x^{(i) \\langle t \\rangle}$는 `(5000, )` 의 shape를 갖습니다.\n",
        "- $n_x$ 표기를 사용해 단일 학습 데이터의 단일 time step에서의 unit의 개수를 표현합니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxJ7yS50XrsZ"
      },
      "source": [
        "#### Time steps of size $T_{x}$\n",
        "- 순환 신경망 RNN에는 여러 time step이 있으며 특정 time step을 $t$로 나타냅니다.\n",
        "- 영상 강의에서, $ x^{(i)}$ 가 여러 time step $T_x$로 구성된 단일 학습 데이터를 보았습니다. 예를 들어 10 개의 time step이 있는 경우 $ T_{x} = 10$ 입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCW2jHGEYDrH"
      },
      "source": [
        "#### Batches of size $m$\n",
        "- 각각 20 개의 훈련 데이타가있는 미니 배치가 있다고 가정 해 보겠습니다.\n",
        "- 벡터화의 이점을 얻기 위해 $x^{(i)}$ 예제의 열 20 개를 쌓아 올립니다.\n",
        "- 예를 들어, 위 경우의 텐서는 `(5000, 20, 10)`의 shape를 가집니다.\n",
        "- 미니 배치에 포함된 훈련 데이터의 수를 나타 내기 위해 $m$를 사용합니다.\n",
        "- 따라서 미니 배치의 shape는 `(n_x, m, T_x)`입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhuXw5_hYdER"
      },
      "source": [
        "#### 3D Tensor of shape $(n_{x},m,T_{x})$\n",
        "- $(n_x, m, T_x)$ 모양의 3 차원 텐서 $x$는 RNN에 투입되는 $x$ 입력을 나타냅니다.\n",
        "\n",
        "#### Taking a 2D slice for each time step: $x^{\\langle t \\rangle}$\n",
        "- 각 time step에서 훈련 데이터의 미니 배치를 사용합니다 (단일 예제를 사용하지 않는다는 것에 주의하세요).\n",
        "- 따라서 각 시간 단계 $t$에 대해 $(n_x, m)$ shape의 2D 슬라이스를 사용합니다.\n",
        "- 우리는이 2D 슬라이스를 $x^{\\langle t \\rangle} $ 라고 합니다. 코드 변수 이름은 `xt` 입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEgnKkZc-vL2"
      },
      "source": [
        "### Definition of hidden state $a$\n",
        "- 한 time step에서 다른 time step까지 RNN으로 전달되는 활성화 $a^{\\langle t \\rangle}$ 값을 \"hidden state\"라고합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6lcHrsa_Wen"
      },
      "source": [
        "### Dimensions of hidden state $a$\n",
        "\n",
        "- 입력 텐서 $ x $와 유사하게 단일 학습 예제의 hidden state는 길이가 $ n_ {a} $ 인 벡터입니다.\n",
        "- 미니 배치가 $m$개의 훈련 데이터를 가지는 경우, 미니 배치의 shape는 $ (n_ {a}, m) $입니다.\n",
        "- time step 차원을 포함 할 때 hidden state의 shape는 $ (n_ {a}, m, T_x) $입니다.\n",
        "- 인덱스 $ t $를 사용하여 time step를 반복하고 3D 텐서의 2D 슬라이스로 작업합니다.\n",
        "- 이 2D 슬라이스를 $ a ^ {\\langle t \\rangle} $라고 합니다.\n",
        "- 코드에서 사용하는 변수 이름은 구현되는 함수에 따라 `a_prev` 또는 `a_next`입니다.\n",
        "- 이 2D 슬라이스의 shape는 $ (n_ {a}, m) $입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk_yna6p_3c6"
      },
      "source": [
        "### Dimensions of prediction $\\hat{y}$\n",
        "\n",
        "- 입력 및 hidden state와 유사하게 $ \\ hat {y} $는 $ (n_ {y}, m, T_ {y}) $ 모양의 3D 텐서입니다.\n",
        "     - $ n_ {y} $ : 예측을 나타내는 벡터의 단위 수.\n",
        "     - $ m $ : 미니 배치의 예 수.\n",
        "     - $ T_ {y} $ : 예측의 시간 단계 수.\n",
        "- 단일 time step $ t $의 경우 2D 슬라이스 $ \\hat {y} ^ {\\langle t \\rangle} $의 모양은 $ (n_ {y}, m) $입니다.\n",
        "- 코드에서 사용하는 변수 이름은 다음과 같습니다.\n",
        "     - `y_pred` : $ \\hat {y} $\n",
        "     - `yt_pred` : $ \\hat {y} ^ {\\langle t \\rangle} $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZFBLaRJAKSK"
      },
      "source": [
        "RNN을 구현하는 방법은 다음과 같습니다.\n",
        "\n",
        "**Step** :\n",
        "1. RNN의 한 time step에 필요한 계산을 구현합니다.\n",
        "2. 모든 입력을 한 번에 하나씩 처리하기 위해 $T_x$ time step에 대한 루프를 구현합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRVs7DZvAYi9"
      },
      "source": [
        "### 1.1 - RNN cell\n",
        "\n",
        "순환 신경망 RNN은 단일 cell을 반복적으로 사용한다는 것을 볼 수 있습니다. 먼저 단일 time step에 대한 계산을 구현합니다. 다음 그림은 RNN 셀의 단일 time step에 대한 작업을 설명합니다.\n",
        "\n",
        "<img src=\"arts/rnn_step_forward_figure2_v3a.png\" style=\"width:700px;height:300px;\">\n",
        "<center>그림 2 : 기본적인 RNN cell. $ x ^ {\\langle t \\rangle} $ (현재 입력) 및 $ a ^ {\\langle t-1 \\rangle} $ (과거 정보를 포함하는 이전 hidden state)를 입력으로 취하고 $ a ^ {\\langle t \\rangle} $을 출력합니다. 이는 다음 RNN cell에 입력으로 제공되며 $ \\hat {y} ^ {\\langle t \\rangle} $ 예측에도 사용됩니다.</center>\n",
        "\n",
        "<br>\n",
        "\n",
        "#### RNN cell versus `rnn_cell_forward`\n",
        "- RNN 셀은 hidden state $ a ^ {\\langle t \\rangle} $을 출력합니다.\n",
        "  - RNN cell은 아래 그림에서 실선이있는 안쪽 상자로 나타낼 수 있습니다.\n",
        "- 우리가 구현할 함수`rnn_cell_forward`는 예측 결과인 $ \\hat {y} ^ {\\langle t \\rangle} $을 계산합니다.\n",
        "  - `rnn_cell_forward` 는 그림에서 파선이 있는 바깥쪽의 상자로 나타낼 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Vko9ZeUBg-7"
      },
      "source": [
        "**연습 문제** : 그림 2에 설명된 RNN 셀을 구현합니다.\n",
        "\n",
        "**지시 사항**:\n",
        "1. tanh 활성화로 hidden unit을 계산합니다. $ a ^ {\\langle t \\rangle} = \\tanh (W_ {aa} a ^ {\\langle t-1 \\rangle} + W_ {ax} x ^ {\\langle t \\rangle} + b_a) $.\n",
        "2. 새로운 hidden unit $ a ^ {\\langle t \\rangle} $을 사용하여 $ \\hat {y} ^ {\\langle t \\rangle} = softmax (W_ {ya} a ^ {\\langle t \\rangle} + b_y) $를 계산하세요. 사전에 제공되는 `softmax` 함수를 사용하세요.\n",
        "3. $ (a ^ {\\langle t \\rangle}, a ^ {\\langle t-1 \\rangle}, x ^ {\\langle t \\rangle}, parameters) $를 `cache`에 저장합니다.\n",
        "4. 최종적으로 $ a ^ {\\langle t \\rangle} $, $ \\ hat {y} ^ {\\langle t \\rangle} $ 및 `cache`를 반환합니다.\n",
        "\n",
        "#### 추가 힌트\n",
        "* [numpy.tanh](https://www.google.com/search?q=numpy+tanh&rlz=1C5CHFA_koUS854US855&oq=numpy+tanh&aqs=chrome..69i57j0l5.1340j0j7&sourceid=chrome&ie=UTF-8)\n",
        "* 제공되는 `softmax`함수를 사용하세요. `rnn_utils.py`파일에 선언되어 있으며, 앞서 불러온 바 있습니다.\n",
        "* 행렬 곱셈의 경우 [numpy.dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html) 함수를 사용하세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQjxZdxqA5es"
      },
      "source": [
        "# GRADED FUNCTION: rnn_cell_forward\n",
        "\n",
        "def rnn_cell_forward(xt, a_prev, parameters):\n",
        "    \"\"\"\n",
        "    Implements a single forward step of the RNN-cell as described in Figure (2)\n",
        "\n",
        "    Arguments:\n",
        "    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
        "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
        "    parameters -- python dictionary containing:\n",
        "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
        "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
        "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
        "                        ba --  Bias, numpy array of shape (n_a, 1)\n",
        "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
        "    Returns:\n",
        "    a_next -- next hidden state, of shape (n_a, m)\n",
        "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
        "    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve parameters from \"parameters\"\n",
        "    Wax = parameters[\"Wax\"]\n",
        "    Waa = parameters[\"Waa\"]\n",
        "    Wya = parameters[\"Wya\"]\n",
        "    ba = parameters[\"ba\"]\n",
        "    by = parameters[\"by\"]\n",
        "    \n",
        "    ### START CODE HERE ### (≈2 lines)\n",
        "    # compute next activation state using the formula given above\n",
        "    a_next = None\n",
        "    # compute output of the current cell using the formula given above\n",
        "    yt_pred = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # store values you need for backward propagation in cache\n",
        "    cache = (a_next, a_prev, xt, parameters)\n",
        "    \n",
        "    return a_next, yt_pred, cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia3jimflCPYu"
      },
      "source": [
        "np.random.seed(1)\n",
        "xt_tmp = np.random.randn(3,10)\n",
        "a_prev_tmp = np.random.randn(5,10)\n",
        "parameters_tmp = {}\n",
        "parameters_tmp['Waa'] = np.random.randn(5,5)\n",
        "parameters_tmp['Wax'] = np.random.randn(5,3)\n",
        "parameters_tmp['Wya'] = np.random.randn(2,5)\n",
        "parameters_tmp['ba'] = np.random.randn(5,1)\n",
        "parameters_tmp['by'] = np.random.randn(2,1)\n",
        "\n",
        "a_next_tmp, yt_pred_tmp, cache_tmp = rnn_cell_forward(xt_tmp, a_prev_tmp, parameters_tmp)\n",
        "print(\"a_next[4] = \\n\", a_next_tmp[4])\n",
        "print(\"a_next.shape = \\n\", a_next_tmp.shape)\n",
        "print(\"yt_pred[1] =\\n\", yt_pred_tmp[1])\n",
        "print(\"yt_pred.shape = \\n\", yt_pred_tmp.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGcUVIcECNtO"
      },
      "source": [
        "**모범 답안**: \n",
        "```Python\n",
        "a_next[4] = \n",
        " [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978\n",
        " -0.18887155  0.99815551  0.6531151   0.82872037]\n",
        "a_next.shape = \n",
        " (5, 10)\n",
        "yt_pred[1] =\n",
        " [ 0.9888161   0.01682021  0.21140899  0.36817467  0.98988387  0.88945212\n",
        "  0.36920224  0.9966312   0.9982559   0.17746526]\n",
        "yt_pred.shape = \n",
        " (2, 10)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHN3nsiyC165"
      },
      "source": [
        "### 1.2 - RNN forward pass \n",
        "\n",
        "- RNN (Recurrent Neural Network)은 바로 이전 단계에서 구현한 RNN cell의 반복입니다.\n",
        "  - 입력 데이터 시퀀스가 10개의 time step을 가지는경우 RNN cell을 10번 반복합니다.\n",
        "- 각 cell은 각 time step에서 두 가지 입력을 받습니다.\n",
        "  - $ a ^ {\\langle t-1 \\rangle} $ : 이전 cell의 hidden unit.\n",
        "  - $ x ^ {\\langle t \\rangle} $ : 현재 time step의 입력 데이터.\n",
        "- 마찬가지로 각 time step마다 두 개의 출력이 있습니다.\n",
        "  - hidden unit ($ a ^ {\\langle t \\rangle} $)\n",
        "  - 예측 결과 ($ y ^ {\\langle t \\rangle} $)\n",
        "- 가중치 및 bias $ (W_ {aa}, b_ {a}, W_ {ax}, b_ {x}) $는 각 time step에서 재사용됩니다.\n",
        "  - `parameters` 딕셔너리는 `rnn_cell_forward` 함수가 호출되는 사이에도 값이 유지됩니다.\n",
        "\n",
        "\n",
        "<img src=\"arts/rnn_forward_sequence_figure3_v3a.png\" style=\"width:800px;height:180px;\">\n",
        "\n",
        "<center>그림 3 : 기본적인 RNN 구조. 입력 시퀀스 $ x = (x ^ {\\langle 1 \\rangle}, x ^ {\\langle 2 \\rangle}, ..., x ^ {\\langle T_x \\rangle}) $는 $ T_x $ 각 time step에 걸쳐 전달됩니다. 인공신경망은 $ y = (y ^ {\\langle 1 \\rangle}, y ^ {\\langle 2 \\rangle}, ..., y ^ {\\langle T_x \\rangle}) $를 출력합니다.</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uJS0BaTEI9w"
      },
      "source": [
        "**연습 문제** : 그림 3에 설명 된 RNN의 forward propagation을 구현하세요.\n",
        "\n",
        "**지시 사항**:\n",
        "- RNN에 의해 ​​계산 된 모든 hidden unit를 저장할, shape $(n_{a}, m, T_ {x})$를 가지는 3D 배열 $a$를 만들고 이를 0으로 초기화합니다.\n",
        "- 예측 결과를 저장할 $ (n_ {y}, m, T_ {x}) $ shape의 3D 배열 $ \\hat {y} $를 만들고 이를 0으로 초기화합니다.\n",
        "  - 이 경우 $ T_ {y} = T_ {x} $ (예측 결과 및 입력의 time step 수가 동일 함)에 유의하십시오.\n",
        "- 최초의 hidden state $ a_ {0} $와 같은 값으로 2D hidden state `a_next`를 초기화합니다.\n",
        "- 매 $t$ time step마다 :\n",
        "  - 단일 time step $ t $에 대한 $ x $의 2D 슬라이스 인 $ x ^ {\\langle t \\rangle} $를 가져옵니다.\n",
        "    - $ x ^ {\\langle t\\rangle} $의 shape는 $ (n_ {x}, m) $입니다.\n",
        "    - $ x $의 shape는 $ (n_ {x}, m, T_ {x}) $입니다.\n",
        "  - `rnn_cell_forward`를 실행하여 2D hidden state $ a ^ {\\langle t \\rangle} $ (변수 이름 = `a_next`), 예측 결과 $ \\hat {y} ^ {\\langle t \\rangle} $ 및 캐시를 업데이트합니다. .\n",
        "    - $ a ^ {\\ langle t \\ rangle} $ 모양이 $ (n_ {a}, m) $\n",
        "  - $ t ^ {th} $ 번째 위치의 3D 텐서 $ a $에 2D hidden state를 저장합니다.\n",
        "    - $ a $는 $ (n_ {a}, m, T_ {x}) $의 shape를 가집니다.\n",
        "  - $ t $ 번째 위치에 3D 텐서 $ \\hat {y} _ {pred} $에 2D $ \\hat {y} ^ {\\langle t \\rangle} $ 예측 (변수 이름`yt_pred`)을 저장합니다.\n",
        "    - $ \\hat {y} ^ {\\langle t \\rangle} $의 shape는 $ (n_ {y}, m) $입니다.\n",
        "    - $ \\hat {y} $의 shape는 $ (n_ {y}, m, T_x) $입니다.\n",
        "  - 캐시 목록에 캐시를 추가합니다.\n",
        "- 3D 텐서 $ a $ 및 $ \\hat {y} $와 캐시를 반환합니다.\n",
        "\n",
        "\n",
        "**추가 힌트**\n",
        "- [np.zeros](https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html)\n",
        "- 3 차원 numpy 배열이 있고 3 차원으로 인덱싱하는 경우 `var_name [:, :, i]` 과 같이 배열 슬라이싱을 사용할 수 있습니다. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPdXp5E6C9QU"
      },
      "source": [
        "# GRADED FUNCTION: rnn_forward\n",
        "\n",
        "def rnn_forward(x, a0, parameters):\n",
        "    \"\"\"\n",
        "    Implement the forward propagation of the recurrent neural network described in Figure (3).\n",
        "\n",
        "    Arguments:\n",
        "    x -- Input data for every time-step, of shape (n_x, m, T_x).\n",
        "    a0 -- Initial hidden state, of shape (n_a, m)\n",
        "    parameters -- python dictionary containing:\n",
        "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
        "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
        "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
        "                        ba --  Bias numpy array of shape (n_a, 1)\n",
        "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
        "\n",
        "    Returns:\n",
        "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
        "    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
        "    caches -- tuple of values needed for the backward pass, contains (list of caches, x)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize \"caches\" which will contain the list of all caches\n",
        "    caches = []\n",
        "    \n",
        "    # Retrieve dimensions from shapes of x and parameters[\"Wya\"]\n",
        "    n_x, m, T_x = x.shape\n",
        "    n_y, n_a = parameters[\"Wya\"].shape\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    \n",
        "    # initialize \"a\" and \"y_pred\" with zeros (≈2 lines)\n",
        "    a = None\n",
        "    y_pred = None\n",
        "    \n",
        "    # Initialize a_next (≈1 line)\n",
        "    a_next = None\n",
        "    \n",
        "    # loop over all time-steps of the input 'x' (1 line)\n",
        "    for t in range(None):\n",
        "        # Update next hidden state, compute the prediction, get the cache (≈2 lines)\n",
        "        xt = None\n",
        "        a_next, yt_pred, cache = None\n",
        "        # Save the value of the new \"next\" hidden state in a (≈1 line)\n",
        "        a[:,:,t] = None\n",
        "        # Save the value of the prediction in y (≈1 line)\n",
        "        y_pred[:,:,t] = None\n",
        "        # Append \"cache\" to \"caches\" (≈1 line)\n",
        "        None\n",
        "        \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # store values needed for backward propagation in cache\n",
        "    caches = (caches, x)\n",
        "    \n",
        "    return a, y_pred, caches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3We818j9GQUO"
      },
      "source": [
        "np.random.seed(1)\n",
        "x_tmp = np.random.randn(3,10,4)\n",
        "a0_tmp = np.random.randn(5,10)\n",
        "parameters_tmp = {}\n",
        "parameters_tmp['Waa'] = np.random.randn(5,5)\n",
        "parameters_tmp['Wax'] = np.random.randn(5,3)\n",
        "parameters_tmp['Wya'] = np.random.randn(2,5)\n",
        "parameters_tmp['ba'] = np.random.randn(5,1)\n",
        "parameters_tmp['by'] = np.random.randn(2,1)\n",
        "\n",
        "a_tmp, y_pred_tmp, caches_tmp = rnn_forward(x_tmp, a0_tmp, parameters_tmp)\n",
        "print(\"a[4][1] = \\n\", a_tmp[4][1])\n",
        "print(\"a.shape = \\n\", a_tmp.shape)\n",
        "print(\"y_pred[1][3] =\\n\", y_pred_tmp[1][3])\n",
        "print(\"y_pred.shape = \\n\", y_pred_tmp.shape)\n",
        "print(\"caches[1][1][3] =\\n\", caches_tmp[1][1][3])\n",
        "print(\"len(caches) = \\n\", len(caches_tmp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grW1Ny5MGSs7"
      },
      "source": [
        "**모범 답안**:\n",
        "\n",
        "```Python\n",
        "a[4][1] = \n",
        " [-0.99999375  0.77911235 -0.99861469 -0.99833267]\n",
        "a.shape = \n",
        " (5, 10, 4)\n",
        "y_pred[1][3] =\n",
        " [ 0.79560373  0.86224861  0.11118257  0.81515947]\n",
        "y_pred.shape = \n",
        " (2, 10, 4)\n",
        "caches[1][1][3] =\n",
        " [-1.1425182  -0.34934272 -0.20889423  0.58662319]\n",
        "len(caches) = \n",
        " 2\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VGdM1sTIHh0"
      },
      "source": [
        "축하합니다! Recurrent Neural Network의 forward propagation을 성공적으로 구현했습니다.\n",
        "\n",
        "#### Situations when this RNN will perform better:\n",
        "- 위 구현의 경우 그라디언트가 사라지는 문제(Gradient vanishing)가 있습니다.\n",
        "- RNN은 각 출력 $ \\hat {y} ^ {\\langle t \\rangle} $을 \"local\" 컨텍스트를 사용하여 추정 할 수있을 때 가장 잘 작동합니다.\n",
        "- \"local\" 컨텍스트는 예측의 time step $ t $에 가까운 정보를 나타냅니다.\n",
        "- 보다 공식적으로, local 컨텍스트는 입력 $ x ^ {\\langle t '\\rangle} $ 및 예측 $ \\hat {y} ^ {\\langle t \\rangle} $를 참조합니다. 여기서 $t'$는 $t$에 가깝습니다.\n",
        "\n",
        "다음 부분에서는 더 복잡한 LSTM 모델을 구축 할 것입니다. LSTM은 정보를 더 잘 기억하고 여러 시간 단계 동안 저장해 둘 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PStcAaWcIHtQ"
      },
      "source": [
        "## 2 - Long Short-Term Memory (LSTM) network\n",
        "\n",
        "다음 그림은 LSTM 셀의 작동을 보여줍니다.\n",
        "\n",
        "<img src=\"arts/LSTM_figure4_v3a.png\" style=\"width : 500; height : 400px;\">\n",
        "<center>그림 4 : LSTM-cell. 이는 모든 time step에서 \"셀 상태\" 또는 메모리 변수 $ c ^ {\\langle t \\rangle} $을 추적하고 업데이트하며, 이 값은 $a ^ {\\langle t \\rangle}$와 다를 수 있습니다.\n",
        "$ softmax ^ {*} $에는 dense layer와 softmax가 포함되어 있습니다.</center>\n",
        "\n",
        "위의 기본적인 RNN을 구현했던 방법과 비슷하게 단일 time step에 대해 LSTM 셀을 구현하는 것으로 시작합니다. 그런 다음 반복문내에서 그 함수를 반복적으로 호출하여 $ T_x $ time step에 대해 주어진 입력을 처리하도록 확장하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ltgOA1TJbLL"
      },
      "source": [
        "### Overview of gates and states\n",
        "\n",
        "#### Forget gate $\\mathbf{\\Gamma}_{f}$\n",
        "- 텍스트에서 단어를 읽고 문법 구조를 추적하기 위해 LSTM을 사용할 계획이라고 가정해 보겠습니다. 예를 들어 특정 명사가 단수 (\"강아지\")인지 복수 (\"강아지\")인지 인식하는 것입니다.\n",
        "- 주어의 상태가 변경됨면 (단수형에서 복수형으로) 이전 상태를 폐기하고, 그 값을 잊어버립니다.\n",
        "- \"Forget Gate\"는 0과 1 사이의 값을 포함하는 텐서입니다.\n",
        "  - Forget gate에 있는 유닛의 값이 0에 가까운 경우, LSTM은 이전 셀 상태의 해당 유닛에 저장된 상태를 \"잊습니다\".\n",
        "  - Forget gate에 있는 장치의 값이 1에 가까운 경우 LSTM은 대부분 저장된 상태 그대로의 값을 기억합니다.\n",
        "\n",
        "##### Equation\n",
        "$$\\mathbf{\\Gamma}_f^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_f[\\mathbf{a}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_f)\\tag{1} $$\n",
        "\n",
        "##### Explanation of the equation:\n",
        "\n",
        "- $ \\mathbf {W_ {f}} $에는 forget gate의 동작을 제어하는 ​​가중치가 포함되어 있습니다.\n",
        "- 이전 time step의 hidden state $ [a ^ {\\langle t-1 \\rangle} $ 와, 현재 time step의 입력 $ x ^ {\\langle t \\rangle}] $을 더하고 $ \\mathbf{W_{f}}$를 곱합니다.\n",
        "- 시그모이드 함수는 각 gate 텐서의 값 $ \\mathbf {\\Gamma} _f ^ {\\langle t \\rangle} $ 범위를 0에서 1까지 만드는 데 사용됩니다.\n",
        "- Forget gate $ \\mathbf {\\Gamma} _f ^ {\\langle t \\rangle} $은 이전 셀 상태 $ c ^ {\\langle t-1 \\rangle} $과 shape가 동일합니다.\n",
        "- 이것은 두 행렬을 요소별로 곱할 수 있음을 의미합니다.\n",
        "- 텐서 $ \\mathbf {\\Gamma} _f ^ {\\langle t \\rangle} * \\mathbf {c} ^ {\\langle t-1 \\rangle} $은 이전 셀 상태에 마스크를 적용하는 것과 같습니다.\n",
        "- $ \\mathbf {\\Gamma} _f ^ {\\langle t \\rangle} $의 값이 0이거나 0에 가까우면 곱셈 결과 역시 0에 가깝습니다.\n",
        "  - 이렇게 하면 $ \\mathbf {c} ^ {\\langle t-1 \\rangle} $에 저장된 정보가 다음 time step으로 전달되지(기억되지) 않습니다.\n",
        "- 마찬가지로 한 값이 1에 가까우면 곱셈 결과는 이전 셀 상태의 원래 값에 가깝습니다.\n",
        "  - LSTM은 다음 시간 단계에서 사용할 $ \\mathbf {c} ^ {\\langle t-1 \\rangle} $의 정보를 유지합니다(기억합니다).\n",
        "\n",
        "##### Variable names in the code\n",
        "코드의 변수 이름은 방정식과 유사합니다.\n",
        "- `Wf` : Forget gate 가중치 $ \\mathbf {W} _ {f} $\n",
        "- `bf` : Forget gate bias $ \\mathbf {b} _ {f} $\n",
        "- `ft` : Forget gate $ \\Gamma_f ^ {\\langle t \\rangle} $\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLMptiQaMkt7"
      },
      "source": [
        "#### Candidate value $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$\n",
        "\n",
        "- 후보 값은 현재 셀 상태 $ \\mathbf {c} ^ {\\langle t \\rangle} $에 **저장 될 수있는** 현재 time step의 정보를 포함하는 텐서입니다.\n",
        "- 후보 값의 어느 부분이 전달되는지는 Update gate에 따라 다릅니다.\n",
        "- 후보 값은 -1에서 1 사이의 값을 포함하는 텐서입니다.\n",
        "- 물결표 \"~(tilda)\"는 후보 값 $ \\tilde {\\mathbf {c}} ^ {\\langle t \\rangle} $를 셀 상태 $ \\mathbf {c} ^ {\\langle t \\rangle} $와 구별하기 위해 사용됩니다.\n",
        "\n",
        "##### Equation\n",
        "$$\\mathbf{\\tilde{c}}^{\\langle t \\rangle} = \\tanh\\left( \\mathbf{W}_{c} [\\mathbf{a}^{\\langle t - 1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{c} \\right) \\tag{3}$$\n",
        "\n",
        "##### Explanation of the equation\n",
        "- 'tanh' 함수는 -1 과 +1 사이의 값을 리턴합니다. \n",
        "\n",
        "##### Variable names in the code\n",
        "- `cct`: 후보 값 $\\mathbf{\\tilde{c}}^{\\langle t \\rangle}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaM7SzrwNbNM"
      },
      "source": [
        "#### Update gate $\\mathbf{\\Gamma}_{i}$\n",
        "- Update gate를 사용하여 후보 값인 $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$의 값을  셀 상태(메모리 변수) $c^{\\langle t \\rangle}$에 얼마만큼 반영할지 결정합니다.\n",
        "- Update gate는 후보 값 텐서 $ \\tilde {\\mathbf {c}} ^ {\\langle t \\rangle} $에서 셀 상태 $ \\mathbf {c} ^ {\\langle t \\rangle}$로 전달되는 부분을 결정합니다.\n",
        "- Update gate는 0과 1 사이의 값을 가지는 텐서입니다.\n",
        "  - Update gate의 단위가 1에 가까우면, 후보 값 $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$d의 값이 hidden state $ \\mathbf {c} ^ {\\langle t \\rangle}$로 더 많이 전달됩니다.\n",
        "  - Update gate의 단위가 0에 가까우면, 후보 값이 hidden state로 전달되는 것을 방지합니다.\n",
        "- 참고 문헌에 사용 된 표기법을 따르기 위해 \"u\"가 아닌 아래 첨자 \"i\"를 사용합니다.\n",
        "\n",
        "##### Equation\n",
        "$$\\mathbf{\\Gamma}_i^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_i[a^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_i)\\tag{2} $$ \n",
        "\n",
        "##### Explanation of the equation\n",
        "- Forget gate와 비슷하게, $ \\mathbf{\\Gamma} _i ^ {\\langle t \\rangle} $에서 시그모이드 함수는 0과 1 사이의 값을 출력합니다.\n",
        "- Update gate 값은 후보 값과 요소별로 곱하여($\\mathbf{\\Gamma}_{i}^{\\langle t \\rangle} * \\tilde{c}^{\\langle t \\rangle}$), 셀 상태 $\\mathbf{c}^{\\langle t \\rangle}$를 결정하는데 사용됩니다.\n",
        "\n",
        "##### Variable names in code (Please note that they're different than the equations)\n",
        "- `Wi` : Update gate 가중치 $\\mathbf{W}_i$ (not \"Wu\") \n",
        "- `bi` : Update gate bias $\\mathbf{b}_i$ (not \"bu\")\n",
        "- `it` : Forget gate $\\mathbf{\\Gamma}_i^{\\langle t \\rangle}$ (not \"ut\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjJaZDZOPo2W"
      },
      "source": [
        "#### - Cell state $\\mathbf{c}^{\\langle t \\rangle}$\n",
        "- 셀 상태는 미래의 시간 단계로 전달되는 \"메모리\"입니다.\n",
        "- 새 셀 상태 $ \\mathbf {c} ^ {\\langle t \\rangle} $은 이전 셀 상태와 후보 값의 조합입니다.\n",
        "\n",
        "##### Equation\n",
        "$$ \\mathbf{c}^{\\langle t \\rangle} = \\mathbf{\\Gamma}_f^{\\langle t \\rangle}* \\mathbf{c}^{\\langle t-1 \\rangle} + \\mathbf{\\Gamma}_{i}^{\\langle t \\rangle} *\\mathbf{\\tilde{c}}^{\\langle t \\rangle} \\tag{4} $$\n",
        "\n",
        "##### Explanation of equation\n",
        "- 이전 셀 상태 $ \\mathbf {c} ^ {\\langle t-1 \\rangle} $은 Forget gate $ \\mathbf {\\Gamma} _ {f} ^ {\\langle t \\rangle}$에 의해 조정(가중)됩니다.\n",
        "- 또한 후보 값 $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$는 Update gate $\\mathbf{\\Gamma}_{i}^{\\langle t \\rangle}$에 의하여 조정(가중)됩니다.\n",
        "\n",
        "##### Variable names and shapes in the code\n",
        "* `c`: 모든 time step에 대한 cell 상태, $\\mathbf{c}$ shape $(n_{a}, m, T)$\n",
        "* `c_next`: 다가오는 time step에 대한 cell 상태, $\\mathbf{c}^{\\langle t \\rangle}$ shape $(n_{a}, m)$\n",
        "* `c_prev`: 이전 time step에 대한 cell 상테, $\\mathbf{c}^{\\langle t-1 \\rangle}$, shape $(n_{a}, m)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JstAl4FqPpCm"
      },
      "source": [
        "#### Output gate $\\mathbf{\\Gamma}_{o}$\n",
        "* Output gate는 time step의 예측 (출력)으로 전송되는 내용을 결정합니다.\n",
        "* Output gate는 다른 게이트와 같습니다. 0에서 1 사이의 값을 포함합니다.\n",
        "\n",
        "##### Equation\n",
        "\n",
        "$$ \\mathbf{\\Gamma}_o^{\\langle t \\rangle}=  \\sigma(\\mathbf{W}_o[\\mathbf{a}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{o})\\tag{5}$$ \n",
        "\n",
        "##### Explanation of the equation\n",
        "* Output gate는 이전 hidden state $ \\mathbf {a} ^ {\\langle t-1 \\rangle} $ 및 현재 입력 $ \\mathbf {x} ^ {\\langle t \\rangle}$에 의해 결정됩니다.\n",
        "* 시그모이드 함수는 Output gate의 범위를 0에서 1까지로 만듭니다.\n",
        "\n",
        "##### Variable names in the code\n",
        "- `Wo` : Output gate 가중치, $ \\mathbf {W_o} $\n",
        "- `bo` : Output gate bias, $ \\mathbf {b_o} $\n",
        "- `ot` : Output gate, $ \\mathbf {\\Gamma} _ {o} ^ {\\langle t \\rangle}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbQJL53RVV5X"
      },
      "source": [
        "#### Hidden state $\\mathbf{a}^{\\langle t \\rangle}$\n",
        "- Hidden state는 LSTM 셀의 다음 시간 단계로 전달됩니다.\n",
        "- 다음 time step의 세가지 gate ($ \\mathbf {\\Gamma} _ {f}, \\mathbf {\\Gamma} _ {u}, \\mathbf {\\Gamma} _ {o} $)를 계산하는 데 사용됩니다.\n",
        "- Hidden state는 예측 결과인 $ y ^ {\\langle t \\rangle} $를 계산하는 데에도 사용됩니다.\n",
        "\n",
        "##### Equation\n",
        "$$ \\mathbf{a}^{\\langle t \\rangle} = \\mathbf{\\Gamma}_o^{\\langle t \\rangle} * \\tanh(\\mathbf{c}^{\\langle t \\rangle})\\tag{6} $$\n",
        "\n",
        "##### Explanation of equation\n",
        "- Hidden state $ \\mathbf {a} ^ {\\langle t \\rangle} $는 cell state $\\mathbf{c}^{\\langle t \\rangle}$와 output gate $\\mathbf{\\Gamma}_{o}$가 결합된 값에 의해 결정됩니다.\n",
        "- Cell state는 \"tanh\"함수를 통해 전달되어 -1과 +1 사이의 값으로 재조정됩니다.\n",
        "- Ouptut gate는 $\\tanh(\\mathbf{c}^{\\langle t \\rangle})$의 값을 보존하거나 이러한 값이 숨겨진 상태 $\\mathbf{a}^{\\langle t \\rangle}$에 포함되지 않도록 하는 \"마스크\"와 같은 역할을 수행합니다.\n",
        "\n",
        "##### Variable names  and shapes in the code\n",
        "* `a`: 모든 time step을 포괄하는 hidden state.  $\\mathbf{a}$의 shape는 $(n_{a}, m, T_{x})$입니다.\n",
        "* 'a_prev`: 이전 time step의 hidden state. $\\mathbf{a}^{\\langle t-1 \\rangle}$의 shape는 $(n_{a}, m)$입니다.\n",
        "* `a_next`: 현재 time step의 hidden state. $\\mathbf{a}^{\\langle t \\rangle}$의 shape는 $(n_{a}, m)$입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh8En0WlXCWU"
      },
      "source": [
        "#### Prediction $\\mathbf{y}^{\\langle t \\rangle}_{pred}$\n",
        "* 이번 과제에서의 예측 결과는 일종의 분류이므로 softmax 함수를 사용합니다.\n",
        "\n",
        "공식은 다음과 같습니다.:\n",
        "$$\\mathbf{y}^{\\langle t \\rangle}_{pred} = \\textrm{softmax}(\\mathbf{W}_{y} \\mathbf{a}^{\\langle t \\rangle} + \\mathbf{b}_{y})$$\n",
        "\n",
        "##### Variable names and shapes in the code\n",
        "* `y_pred`: 모든 time step에 대한 예측 결과. $\\mathbf{y}_{pred}$ has shape $(n_{y}, m, T_{x})$.  $(T_{y} = T_{x})$임에 주의하세요.\n",
        "* `yt_pred`: 현재 time step에 대한 예측 결과 $t$. $\\mathbf{y}^{\\langle t \\rangle}_{pred}$의 shape는 $(n_{y}, m)$입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un2c9amhd-2G"
      },
      "source": [
        "### 2.1 - LSTM cell\n",
        "\n",
        "**연습 문제** : 그림 4에 설명 된 LSTM cell을 구현합니다.\n",
        "\n",
        "**지시 사항**:\n",
        "1. Hidden state $ a ^ {\\langle t-1 \\rangle} $와 입력 $ x ^ {\\langle t \\rangle} $를 단일 행렬로 연결합니다.\n",
        "$$ concat = \\begin {bmatrix} a ^ {\\langle t-1 \\rangle} \\\\ x ^ {\\langle t \\rangle} \\end {bmatrix} $$\n",
        "2. Gate, hidden state 및 cell state에 대한 모든 공식 1 ~ 6을 계산합니다.\n",
        "3. $ y ^ {\\langle t \\rangle} $ 예측을 계산합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCQDO0Zreie_"
      },
      "source": [
        "#### Additional Hints\n",
        "\n",
        "* [numpy.concatenate](https://docs.scipy.org/doc/numpy/reference/generated/numpy.concatenate.html)를 사용할 수 있습니다. `axis` 매개 변수에 사용할 값을 확인하십시오.\n",
        "* `sigmoid ()` 및 `softmax` 함수는 `rnn_utils.py`에서 가져옵니다.\n",
        "* [numpy.tanh](https://docs.scipy.org/doc/numpy/reference/generated/numpy.tanh.html)\n",
        "* 행렬 곱셈에는 [np.dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html)을 사용합니다.\n",
        "* 변수 이름 `Wi`, `bi`는 **update** 게이트의 가중치와 편향을 나타냅니다. 이 함수에는 \"Wu\"또는 \"bu\"라는 변수가 없습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arVeKuQ-efzt"
      },
      "source": [
        "# GRADED FUNCTION: lstm_cell_forward\n",
        "\n",
        "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
        "    \"\"\"\n",
        "    Implement a single forward step of the LSTM-cell as described in Figure (4)\n",
        "\n",
        "    Arguments:\n",
        "    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
        "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
        "    c_prev -- Memory state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
        "    parameters -- python dictionary containing:\n",
        "                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
        "                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
        "                        Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
        "                        bc --  Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
        "                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        bo --  Bias of the output gate, numpy array of shape (n_a, 1)\n",
        "                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
        "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
        "                        \n",
        "    Returns:\n",
        "    a_next -- next hidden state, of shape (n_a, m)\n",
        "    c_next -- next memory state, of shape (n_a, m)\n",
        "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
        "    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)\n",
        "    \n",
        "    Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilde),\n",
        "          c stands for the cell state (memory)\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve parameters from \"parameters\"\n",
        "    Wf = parameters[\"Wf\"] # forget gate weight\n",
        "    bf = parameters[\"bf\"]\n",
        "    Wi = parameters[\"Wi\"] # update gate weight (notice the variable name)\n",
        "    bi = parameters[\"bi\"] # (notice the variable name)\n",
        "    Wc = parameters[\"Wc\"] # candidate value weight\n",
        "    bc = parameters[\"bc\"]\n",
        "    Wo = parameters[\"Wo\"] # output gate weight\n",
        "    bo = parameters[\"bo\"]\n",
        "    Wy = parameters[\"Wy\"] # prediction weight\n",
        "    by = parameters[\"by\"]\n",
        "    \n",
        "    # Retrieve dimensions from shapes of xt and Wy\n",
        "    n_x, m = xt.shape\n",
        "    n_y, n_a = Wy.shape\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # Concatenate a_prev and xt (≈1 line)\n",
        "    concat = None\n",
        "\n",
        "    # Compute values for ft (forget gate), it (update gate),\n",
        "    # cct (candidate value), c_next (cell state), \n",
        "    # ot (output gate), a_next (hidden state) (≈6 lines)\n",
        "    ft = None        # forget gate\n",
        "    it = None        # update gate\n",
        "    cct = None       # candidate value\n",
        "    c_next = None    # cell state\n",
        "    ot = None        # output gate\n",
        "    a_next = None    # hidden state\n",
        "    \n",
        "    # Compute prediction of the LSTM cell (≈1 line)\n",
        "    yt_pred = None\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # store values needed for backward propagation in cache\n",
        "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n",
        "\n",
        "    return a_next, c_next, yt_pred, cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEsbFouGeuHp"
      },
      "source": [
        "np.random.seed(1)\n",
        "xt_tmp = np.random.randn(3,10)\n",
        "a_prev_tmp = np.random.randn(5,10)\n",
        "c_prev_tmp = np.random.randn(5,10)\n",
        "parameters_tmp = {}\n",
        "parameters_tmp['Wf'] = np.random.randn(5, 5+3)\n",
        "parameters_tmp['bf'] = np.random.randn(5,1)\n",
        "parameters_tmp['Wi'] = np.random.randn(5, 5+3)\n",
        "parameters_tmp['bi'] = np.random.randn(5,1)\n",
        "parameters_tmp['Wo'] = np.random.randn(5, 5+3)\n",
        "parameters_tmp['bo'] = np.random.randn(5,1)\n",
        "parameters_tmp['Wc'] = np.random.randn(5, 5+3)\n",
        "parameters_tmp['bc'] = np.random.randn(5,1)\n",
        "parameters_tmp['Wy'] = np.random.randn(2,5)\n",
        "parameters_tmp['by'] = np.random.randn(2,1)\n",
        "\n",
        "a_next_tmp, c_next_tmp, yt_tmp, cache_tmp = lstm_cell_forward(xt_tmp, a_prev_tmp, c_prev_tmp, parameters_tmp)\n",
        "print(\"a_next[4] = \\n\", a_next_tmp[4])\n",
        "print(\"a_next.shape = \", a_next_tmp.shape)\n",
        "print(\"c_next[2] = \\n\", c_next_tmp[2])\n",
        "print(\"c_next.shape = \", c_next_tmp.shape)\n",
        "print(\"yt[1] =\", yt_tmp[1])\n",
        "print(\"yt.shape = \", yt_tmp.shape)\n",
        "print(\"cache[1][3] =\\n\", cache_tmp[1][3])\n",
        "print(\"len(cache) = \", len(cache_tmp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbYFnuNJevh4"
      },
      "source": [
        "**모범 답안**:\n",
        "\n",
        "```Python\n",
        "a_next[4] = \n",
        " [-0.66408471  0.0036921   0.02088357  0.22834167 -0.85575339  0.00138482\n",
        "  0.76566531  0.34631421 -0.00215674  0.43827275]\n",
        "a_next.shape =  (5, 10)\n",
        "c_next[2] = \n",
        " [ 0.63267805  1.00570849  0.35504474  0.20690913 -1.64566718  0.11832942\n",
        "  0.76449811 -0.0981561  -0.74348425 -0.26810932]\n",
        "c_next.shape =  (5, 10)\n",
        "yt[1] = [ 0.79913913  0.15986619  0.22412122  0.15606108  0.97057211  0.31146381\n",
        "  0.00943007  0.12666353  0.39380172  0.07828381]\n",
        "yt.shape =  (2, 10)\n",
        "cache[1][3] =\n",
        " [-0.16263996  1.03729328  0.72938082 -0.54101719  0.02752074 -0.30821874\n",
        "  0.07651101 -1.03752894  1.41219977 -0.37647422]\n",
        "len(cache) =  10\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LiKCMM8eyF-"
      },
      "source": [
        "### 2.2 - Forward pass for LSTM\n",
        "\n",
        "이제 LSTM의 한 단계를 구현 했으므로 반복문을 사용하여 단계를 반복하여 $ T_x $의 입력 시퀀스를 처리 할 수 있습니다.\n",
        "\n",
        "<img src=\"arts/LSTM_rnn.png\" style=\"width:500;height:300px;\">\n",
        "<center>그림 5 : 여러 시간 단계에 걸친 LSTM.</center>\n",
        "\n",
        "**연습 문제:** `lstm_forward()` 를 구현하여 $ T_x $의 time step에 걸쳐 LSTM을 실행합니다.\n",
        "\n",
        "**지시 사항**\n",
        "- 변수`x` 및 `parameters`의 shape를 참조해 $ n_x, n_a, n_y, m, T_x $ 차원을 가져옵니다.\n",
        "- 3D 텐서 $ a $, $ c $ 및 $ y $를 초기화합니다.\n",
        "  - $ a $ : hidden state, shape = $ (n_ {a}, m, T_ {x}) $\n",
        "  - $ c $ : cell state, shape = $ (n_ {a}, m, T_ {x}) $\n",
        "  - $ y $ : prediction, shape =  $ (n_ {y}, m, T_ {x}) $ (이 예에서 $ T_ {y} = T_ {x} $에 유의하십시오).\n",
        "  - **참고** 한 변수를 다른 변수와 동일하게 설정하는 것은 \"copy by reference\" 입니다. 즉,`c = a`의 코드를 사용해서는 안됩니다. 이렇게 코드를 작성하게 되면 두 변수가 동일한 기본 변수를 참조하게 됩니다.\n",
        "- 2D 텐서 $ a ^ {\\langle t \\rangle} $를 초기화 하세요.\n",
        "  - $ a ^ {\\langle t \\rangle} $은 time step $ t $에 대한 hidden state를 저장합니다. 변수 이름은 `a_next`입니다.\n",
        "  - $ a ^ {\\langle 0 \\rangle} $, 0번째 time step에서 hidden state 초기값입니다. 함수 호출시 전달됩니다. 변수 이름은 `a0`입니다.\n",
        "  - $ a ^ {\\langle t \\rangle} $ 및 $ a ^ {\\langle 0 \\rangle} $는 단일 time step을 나타내므로 둘 다 $ (n_ {a}, m) $ shape를 갖습니다.\n",
        "  - 함수에 전달되는 초기 hidden state ($ a ^ {\\langle 0 \\rangle} $)로 설정하여 $ a ^ {\\langle t \\rangle} $을 초기화합니다.\n",
        "- $ c ^ {\\langle t \\rangle} $을 0으로 초기화합니다.\n",
        "  - 변수 명은 `c_next`입니다.\n",
        "  - $ c ^ {\\langle t \\rangle} $는 단일 time step을 나타내므로 shape는 $ (n_ {a}, m) $입니다.\n",
        "  - **참고** :`c_next`를 메모리에 스스로의 영역이 있는 자체 변수로 선언하세요. 3D 텐서 $c$의 일부분으로 초기화해선 안됩니다. 다시 말해, `c_next = c [:, :, 0]` 과 같이 코드를 작성**하지 마세요**.\n",
        "- 각 time step에 대해 다음을 수행하십시오.\n",
        "  - 3D 텐서 $ x $에서 $ t $ time step에서 2D 슬라이스 $ x ^ {\\langle t \\rangle} $를 얻습니다.\n",
        "  - 이전에 정의한 `lstm_cell_forward` 함수를 호출하여 hidden state, cell state, prediction, cache를 가져옵니다.\n",
        "  - hidden state, cell state, prediction (2D 텐서)을 3D 텐서 안에 저장합니다.\n",
        "  - 또한 캐시 목록에 캐시를 추가합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "892v-L68ew6e"
      },
      "source": [
        "# GRADED FUNCTION: lstm_forward\n",
        "\n",
        "def lstm_forward(x, a0, parameters):\n",
        "    \"\"\"\n",
        "    Implement the forward propagation of the recurrent neural network using an LSTM-cell described in Figure (4).\n",
        "\n",
        "    Arguments:\n",
        "    x -- Input data for every time-step, of shape (n_x, m, T_x).\n",
        "    a0 -- Initial hidden state, of shape (n_a, m)\n",
        "    parameters -- python dictionary containing:\n",
        "                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
        "                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
        "                        Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
        "                        bc -- Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
        "                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        bo -- Bias of the output gate, numpy array of shape (n_a, 1)\n",
        "                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
        "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
        "                        \n",
        "    Returns:\n",
        "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
        "    y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
        "    c -- The value of the cell state, numpy array of shape (n_a, m, T_x)\n",
        "    caches -- tuple of values needed for the backward pass, contains (list of all the caches, x)\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize \"caches\", which will track the list of all the caches\n",
        "    caches = []\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    Wy = parameters['Wy'] # saving parameters['Wy'] in a local variable in case students use Wy instead of parameters['Wy']\n",
        "    # Retrieve dimensions from shapes of x and parameters['Wy'] (≈2 lines)\n",
        "    n_x, m, T_x = None\n",
        "    n_y, n_a = None\n",
        "    \n",
        "    # initialize \"a\", \"c\" and \"y\" with zeros (≈3 lines)\n",
        "    a = None\n",
        "    c = None\n",
        "    y = None\n",
        "    \n",
        "    # Initialize a_next and c_next (≈2 lines)\n",
        "    a_next = None\n",
        "    c_next = None\n",
        "    \n",
        "    # loop over all time-steps\n",
        "    for t in range(None):\n",
        "        # Get the 2D slice 'xt' from the 3D input 'x' at time step 't'\n",
        "        xt = None\n",
        "        # Update next hidden state, next memory state, compute the prediction, get the cache (≈1 line)\n",
        "        a_next, c_next, yt, cache = None\n",
        "        # Save the value of the new \"next\" hidden state in a (≈1 line)\n",
        "        a[:,:,t] = None\n",
        "        # Save the value of the next cell state (≈1 line)\n",
        "        c[:,:,t]  = None\n",
        "        # Save the value of the prediction in y (≈1 line)\n",
        "        y[:,:,t] = None\n",
        "        # Append the cache into caches (≈1 line)\n",
        "        None\n",
        "        \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # store values needed for backward propagation in cache\n",
        "    caches = (caches, x)\n",
        "\n",
        "    return a, y, c, caches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kudfhsI0gvRJ"
      },
      "source": [
        "np.random.seed(1)\n",
        "x_tmp = np.random.randn(3,10,7)\n",
        "a0_tmp = np.random.randn(5,10)\n",
        "parameters_tmp = {}\n",
        "parameters_tmp['Wf'] = np.random.randn(5, 5+3)\n",
        "parameters_tmp['bf'] = np.random.randn(5,1)\n",
        "parameters_tmp['Wi'] = np.random.randn(5, 5+3)\n",
        "parameters_tmp['bi']= np.random.randn(5,1)\n",
        "parameters_tmp['Wo'] = np.random.randn(5, 5+3)\n",
        "parameters_tmp['bo'] = np.random.randn(5,1)\n",
        "parameters_tmp['Wc'] = np.random.randn(5, 5+3)\n",
        "parameters_tmp['bc'] = np.random.randn(5,1)\n",
        "parameters_tmp['Wy'] = np.random.randn(2,5)\n",
        "parameters_tmp['by'] = np.random.randn(2,1)\n",
        "\n",
        "a_tmp, y_tmp, c_tmp, caches_tmp = lstm_forward(x_tmp, a0_tmp, parameters_tmp)\n",
        "print(\"a[4][3][6] = \", a_tmp[4][3][6])\n",
        "print(\"a.shape = \", a_tmp.shape)\n",
        "print(\"y[1][4][3] =\", y_tmp[1][4][3])\n",
        "print(\"y.shape = \", y_tmp.shape)\n",
        "print(\"caches[1][1][1] =\\n\", caches_tmp[1][1][1])\n",
        "print(\"c[1][2][1]\", c_tmp[1][2][1])\n",
        "print(\"len(caches) = \", len(caches_tmp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyfIlWzrgwds"
      },
      "source": [
        "**모범 답안**:\n",
        "\n",
        "```Python\n",
        "a[4][3][6] =  0.172117767533\n",
        "a.shape =  (5, 10, 7)\n",
        "y[1][4][3] = 0.95087346185\n",
        "y.shape =  (2, 10, 7)\n",
        "caches[1][1][1] =\n",
        " [ 0.82797464  0.23009474  0.76201118 -0.22232814 -0.20075807  0.18656139\n",
        "  0.41005165]\n",
        "c[1][2][1] -0.855544916718\n",
        "len(caches) =  2\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da5RnkQTg0z2"
      },
      "source": [
        "축하합니다! 이제 기본 RNN 및 LSTM에 대한 forward propagation을 구현했습니다. 딥 러닝 프레임 워크를 사용하여 forward propagation를 구현하면 뛰어난 성능의 시스템을 구축할 수 있습니다.\n",
        "\n",
        "아래 부분은 선택 사항이며 채점되지 않습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9Gd0xqJBHPq"
      },
      "source": [
        "## 3 - Backpropagation in recurrent neural networks (OPTIONAL / UNGRADED)\n",
        "\n",
        "최신 딥 러닝 프레임 워크는 forward pass만 구현하면 알아서 backward pass를 처리하므로, 대부분의 딥 러닝 엔지니어는 backward pass의 세부 사항에 신경 쓸 필요가 없습니다. 하지만 만약 여러분이 미적분에 대한 세부 지식을 알고 있고, RNN backprop의 구체적인 구현 방법을 확인해보고 싶다면 노트북의 이 선택 과제를 수행하면 됩니다.\n",
        "\n",
        "우리는 [이전 강좌](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/0VSHe/derivatives-with-a-computation-graph)에서 간단한 Fully connected 함수를 구현한 바 있습니다. 그 과정에서 비용 함수를 최소화시킬 수 있는 파라미터를 찾기 위해, backward propagation을 사용하여 대한 미분계수를 계산했습니다. 마찬가지로 RNN에서는 파라미터를 업데이트하기 위해 비용함수에 대한 미분계수를 계산할 수 있습니다. backward propagation 공식은 매우 복잡하기 때문에 동영상 강의에서 유도하지 않았습니다. 그러나 아래 파트에서 간략하게 설명하겠습니다.\n",
        "\n",
        "이 노트북은 손실 'J'에서 'a'로의 backward path를 구현하지 않습니다. 여기에는 forward path의 일부인 dense layer와 softmax가 포함되었을 것입니다. 이것은 다른 곳에서 계산되고 결과가 'da'의 rnn_backward에 전달되는 것으로 간주됩니다. 또한 손실은 배치 크기(m)에 대해 사전에 조정되었으며 여기에서는 훈련 세트에 의한 분할이 필요하지 않다고 가정합니다.\n",
        "\n",
        "이 파트는 선택 사항이며 채점되지 않습니다. 이 파트는 다른 부분보다 더 어렵고, 구현에 관한 세부 사항이 적습니다. 이 섹션은 전체 경로의 핵심 요소 만 구현합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm2qzigpDCOU"
      },
      "source": [
        "### 3.1 - Basic RNN backward pass\n",
        "\n",
        "기본적인 RNN-셀에 대한 backward path를 계산하는 것으로 시작하고 다음 섹션에서는 해당 셀에 대한 연산을 반복합니다.\n",
        "\n",
        "<img src=\"arts/rnn_backward_overview_3a_1.png\" style=\"width:500;height:300px;\">\n",
        "<center>그림 6 : RNN- 셀의 backward path. Fully connected layer에서와 마찬가지로 비용 함수 $ J $의 미분은 미적분의 연쇄 법칙을 따라 RNN의 time step을 통해 역 전파됩니다. 셀 내부에서 chain-rule은 $(\\frac {\\partial J} {\\partial W_ {ax}}, \\frac {\\partial J} {\\partial W_ {aa}}, \\frac {\\partial J} {\\partial b})$를 계산하는데도 사용됩니다. 매개 변수 $(W_ {ax}, W_ {aa}, b_a)$ 를 업데이트하기 위해 forward path의 캐시 된 결과를 활용할 수 있습니다.</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dQAeB4vEBbN"
      },
      "source": [
        "강의를 떠올려 보면 변수에 대한 비용의 편미분값은 dVariable입니다. 예를 들어 $ \\frac {\\partial J} {\\partial W_ {ax}} $는 $ dW_ {ax} $입니다. 이 표기법은 아래의 섹션에서 사용됩니다.\n",
        "\n",
        "<img src=\"images/rnn_cell_backward_3a_c.png\" style=\"width:800;height:500px;\"> \n",
        "\n",
        "<br>\n",
        "\n",
        "<center>그림 7 : 이 rnn_cell_backward 구현에는 rnn_cell_forward에 포함 된 출력 dense layer 및 softmax가 포함되지 않습니다.</center>\n",
        "\n",
        "$ da_ {next} $는 $ \\frac {\\partial {J}} {\\partial a ^ {\\langle t \\rangle}} $이며 이전 단계 및 현재 단계 출력 로직의 손실을 포함합니다. 녹색으로 표시된 추가 사항은 rnn_backward 구현의 일부입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulcHW7SNE0Oi"
      },
      "source": [
        "##### Equations\n",
        "\n",
        "rnn_cell_backward를 계산하려면 다음 공식들을 사용할 수 있습니다. 직접 쓰면서 공식을 유도해보는 것은 좋은 연습 방법이 될 수 있습니다. 여기서 $ * $ 기호는 행렬의 요소 별 곱셈을 나타내며 해당 기호가 없으면 행렬곱을 나타냅니다.\n",
        "\n",
        "\\begin{align}\n",
        "\\displaystyle a^{\\langle t \\rangle} &= \\tanh(W_{ax} x^{\\langle t \\rangle} + W_{aa} a^{\\langle t-1 \\rangle} + b_{a})\\tag{-} \\\\[8pt]\n",
        "\\displaystyle \\frac{\\partial \\tanh(x)} {\\partial x} &= 1 - \\tanh^2(x) \\tag{-} \\\\[8pt]\n",
        "\\displaystyle  {dW_{ax}} &= (da_{next} * ( 1-\\tanh^2(W_{ax}x^{\\langle t \\rangle}+W_{aa} a^{\\langle t-1 \\rangle} + b_{a}) )) x^{\\langle t \\rangle T}\\tag{1} \\\\[8pt]\n",
        "\\displaystyle dW_{aa} &= (da_{next} * ( 1-\\tanh^2(W_{ax}x^{\\langle t \\rangle}+W_{aa} a^{\\langle t-1 \\rangle} + b_{a}) ))  a^{\\langle t-1 \\rangle T}\\tag{2} \\\\[8pt]\n",
        "\\displaystyle db_a& = \\sum_{batch}( da_{next} * ( 1-\\tanh^2(W_{ax}x^{\\langle t \\rangle}+W_{aa} a^{\\langle t-1 \\rangle} + b_{a}) ))\\tag{3} \\\\[8pt]\n",
        "\\displaystyle dx^{\\langle t \\rangle} &= { W_{ax}}^T (da_{next} * ( 1-\\tanh^2(W_{ax}x^{\\langle t \\rangle}+W_{aa} a^{\\langle t-1 \\rangle} + b_{a}) ))\\tag{4} \\\\[8pt]\n",
        "\\displaystyle da_{prev} &= { W_{aa}}^T(da_{next} *  ( 1-\\tanh^2(W_{ax}x^{\\langle t \\rangle}+W_{aa} a^{\\langle t-1 \\rangle} + b_{a}) ))\\tag{5}\n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebwa7GsxFJvm"
      },
      "source": [
        "#### Implementing rnn_cell_backward\n",
        "\n",
        "위의 공식을 구현하여 결과를 직접 계산할 수 있습니다. 그러나 위의 내용은 'dz'를 계산하고 연쇄 법칙을 사용하여 추가적으로 단순화 할 수 있습니다.\n",
        "$ \\tanh (W_ {ax} x ^ {\\langle t \\rangle} + W_ {aa} a ^ {\\langle t-1 \\rangle} + b_ {a}) $가 forward에서 이미 계산된 바 있고, 캐시에 저장되었다는 것을 기억하면 위 공식은 더 간단해질 수 있습니다.\n",
        "\n",
        "`dba`를 계산하기 위해 위의 'batch'는 모든 'm'예제 (축 = 1)의 합계입니다. `keepdims = True` 옵션을 사용해야합니다.\n",
        "\n",
        "코스 1의 [Derivatives with a computaional graph](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/0VSHe/derivatives-with-a-computation-graph) 부터 [BackwardPropagation Intution](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/6dDj7/backpropagation-intuition-optional)을 검토하는 것이 좋습니다. \n",
        "\n",
        "행렬 벡터에 대한 미분은[여기](http://cs231n.stanford.edu/vecDerivs.pdf)에 설명되어 있지만 위의 공식에는 필요한 변환이 포함되어 있습니다.\n",
        "\n",
        "rnn_cell_backward는 $y \\langle t \\rangle $의 손실 계산을 포함하지 않습니다. 이것은 da_next를 계산하는 곳에 통합됩니다. 이는 dense_layer와 softmax를 모두 포함하는 rnn_cell_forward와 backward 사이의 작은 차이점입니다.\n",
        "\n",
        "참고 : 코드에서 :\n",
        "\n",
        "$\\displaystyle dx^{\\langle t \\rangle}$ is represented by dxt,   \n",
        "$\\displaystyle d W_{ax}$ is represented by dWax,   \n",
        "$\\displaystyle da_{prev}$ is represented by da_prev,    \n",
        "$\\displaystyle dW_{aa}$ is represented by dWaa,   \n",
        "$\\displaystyle db_{a}$ is represented by dba,   \n",
        "dz는 위에서 파생되지 않았지만 반복 계산을 단순화하기 위해 선택적으로 여러분이 직접 구해볼 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcZGplBtDBoR"
      },
      "source": [
        "def rnn_cell_backward(da_next, cache):\n",
        "    \"\"\"\n",
        "    Implements the backward pass for the RNN-cell (single time-step).\n",
        "\n",
        "    Arguments:\n",
        "    da_next -- Gradient of loss with respect to next hidden state\n",
        "    cache -- python dictionary containing useful values (output of rnn_cell_forward())\n",
        "\n",
        "    Returns:\n",
        "    gradients -- python dictionary containing:\n",
        "                        dx -- Gradients of input data, of shape (n_x, m)\n",
        "                        da_prev -- Gradients of previous hidden state, of shape (n_a, m)\n",
        "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
        "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
        "                        dba -- Gradients of bias vector, of shape (n_a, 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve values from cache\n",
        "    (a_next, a_prev, xt, parameters) = cache\n",
        "    \n",
        "    # Retrieve values from parameters\n",
        "    Wax = parameters[\"Wax\"]\n",
        "    Waa = parameters[\"Waa\"]\n",
        "    Wya = parameters[\"Wya\"]\n",
        "    ba = parameters[\"ba\"]\n",
        "    by = parameters[\"by\"]\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # compute the gradient of the loss with respect to z (optional) (≈1 line)\n",
        "    dz = None\n",
        "\n",
        "    # compute the gradient of the loss with respect to Wax (≈2 lines)\n",
        "    dxt = None\n",
        "    dWax = None\n",
        "\n",
        "    # compute the gradient with respect to Waa (≈2 lines)\n",
        "    da_prev = None\n",
        "    dWaa = None\n",
        "\n",
        "    # compute the gradient with respect to b (≈1 line)\n",
        "    dba = None\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Store the gradients in a python dictionary\n",
        "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dWax\": dWax, \"dWaa\": dWaa, \"dba\": dba}\n",
        "    \n",
        "    return gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt8ObfwwGia6"
      },
      "source": [
        "np.random.seed(1)\n",
        "xt_tmp = np.random.randn(3,10)\n",
        "a_prev_tmp = np.random.randn(5,10)\n",
        "parameters_tmp = {}\n",
        "parameters_tmp['Wax'] = np.random.randn(5,3)\n",
        "parameters_tmp['Waa'] = np.random.randn(5,5)\n",
        "parameters_tmp['Wya'] = np.random.randn(2,5)\n",
        "parameters_tmp['ba'] = np.random.randn(5,1)\n",
        "parameters_tmp['by'] = np.random.randn(2,1)\n",
        "\n",
        "a_next_tmp, yt_tmp, cache_tmp = rnn_cell_forward(xt_tmp, a_prev_tmp, parameters_tmp)\n",
        "\n",
        "da_next_tmp = np.random.randn(5,10)\n",
        "gradients_tmp = rnn_cell_backward(da_next_tmp, cache_tmp)\n",
        "print(\"gradients[\\\"dxt\\\"][1][2] =\", gradients_tmp[\"dxt\"][1][2])\n",
        "print(\"gradients[\\\"dxt\\\"].shape =\", gradients_tmp[\"dxt\"].shape)\n",
        "print(\"gradients[\\\"da_prev\\\"][2][3] =\", gradients_tmp[\"da_prev\"][2][3])\n",
        "print(\"gradients[\\\"da_prev\\\"].shape =\", gradients_tmp[\"da_prev\"].shape)\n",
        "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients_tmp[\"dWax\"][3][1])\n",
        "print(\"gradients[\\\"dWax\\\"].shape =\", gradients_tmp[\"dWax\"].shape)\n",
        "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients_tmp[\"dWaa\"][1][2])\n",
        "print(\"gradients[\\\"dWaa\\\"].shape =\", gradients_tmp[\"dWaa\"].shape)\n",
        "print(\"gradients[\\\"dba\\\"][4] =\", gradients_tmp[\"dba\"][4])\n",
        "print(\"gradients[\\\"dba\\\"].shape =\", gradients_tmp[\"dba\"].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_sTrwtRGmfI"
      },
      "source": [
        "**모범 답안**:\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **gradients[\"dxt\"][1][2]** =\n",
        "        </td>\n",
        "        <td>\n",
        "           -1.3872130506\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dxt\"].shape** =\n",
        "        </td>\n",
        "        <td>\n",
        "           (3, 10)\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"da_prev\"][2][3]** =\n",
        "        </td>\n",
        "        <td>\n",
        "           -0.152399493774\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"da_prev\"].shape** =\n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 10)\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWax\"][3][1]** =\n",
        "        </td>\n",
        "        <td>\n",
        "           0.410772824935\n",
        "        </td>\n",
        "    </tr>\n",
        "            <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWax\"].shape** =\n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 3)\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWaa\"][1][2]** = \n",
        "        </td>\n",
        "        <td>\n",
        "           1.15034506685\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWaa\"].shape** =\n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 5)\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dba\"][4]** = \n",
        "        </td>\n",
        "        <td>\n",
        "           [ 0.20023491]\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dba\"].shape** = \n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 1)\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGh-otWZGq8S"
      },
      "source": [
        "#### Backward pass through the RNN\n",
        "\n",
        "모든 time step $ t $에서 $ a ^ {\\langle t \\rangle} $에 대한 비용 함수의 기울기를 계산하는 것은 해당 기울기가 이전 RNN 셀로 역전파 되는데 도움이 되기 때문에 유용합니다. 이렇게하려면 마지막 셀에서 시작하여 모든 time step을 반복해야하며 각 step에서 전체 $ db_a $, $ dW_ {aa} $, $ dW_ {ax} $를 증가시키고 $ dx $를 저장합니다.\n",
        "\n",
        "**지시 사항**:\n",
        "\n",
        "`rnn_backward` 함수를 구현합니다. 리턴 값을 먼저 0으로 초기화 한 다음 각 time step에서`rnn_cell_backward`를 호출하면서 모든 time step을 반복하고 그에 따라 다른 변수를 업데이트합니다.\n",
        "\n",
        "* 이 노트북은 Loss 'J'에서 'a'로의 backward pass를 구현하지 않습니다.\n",
        "  * 여기에는 forward pass의 일부인 dense layer와 softmax가 포함되었을 것입니다.\n",
        "  * 이것은 다른 곳에서 계산되고 결과가 'da'의 rnn_backward에 전달되는 것으로 가정합니다.\n",
        "  * rnn_cell_backward를 호출 할 때 이전 단계의 손실과 결합해야합니다 (위의 그림 7 참조).\n",
        "* 또한 배치 크기 (m)에 대해 손실이 조정 된 것으로 가정합니다.\n",
        "  * 따라서 여기에서는 훈련 세트 수로 나눌 필요가 없습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZ8_5lNPGoVk"
      },
      "source": [
        "def rnn_backward(da, caches):\n",
        "    \"\"\"\n",
        "    Implement the backward pass for a RNN over an entire sequence of input data.\n",
        "\n",
        "    Arguments:\n",
        "    da -- Upstream gradients of all hidden states, of shape (n_a, m, T_x)\n",
        "    caches -- tuple containing information from the forward pass (rnn_forward)\n",
        "    \n",
        "    Returns:\n",
        "    gradients -- python dictionary containing:\n",
        "                        dx -- Gradient w.r.t. the input data, numpy-array of shape (n_x, m, T_x)\n",
        "                        da0 -- Gradient w.r.t the initial hidden state, numpy-array of shape (n_a, m)\n",
        "                        dWax -- Gradient w.r.t the input's weight matrix, numpy-array of shape (n_a, n_x)\n",
        "                        dWaa -- Gradient w.r.t the hidden state's weight matrix, numpy-arrayof shape (n_a, n_a)\n",
        "                        dba -- Gradient w.r.t the bias, of shape (n_a, 1)\n",
        "    \"\"\"\n",
        "        \n",
        "    ### START CODE HERE ###\n",
        "    \n",
        "    # Retrieve values from the first cache (t=1) of caches (≈2 lines)\n",
        "    (caches, x) = None\n",
        "    (a1, a0, x1, parameters) = None\n",
        "    \n",
        "    # Retrieve dimensions from da's and x1's shapes (≈2 lines)\n",
        "    n_a, m, T_x = None\n",
        "    n_x, m = None\n",
        "    \n",
        "    # initialize the gradients with the right sizes (≈6 lines)\n",
        "    dx = None\n",
        "    dWax = None\n",
        "    dWaa = None\n",
        "    dba = None\n",
        "    da0 = None\n",
        "    da_prevt = None\n",
        "    \n",
        "    # Loop through all the time steps\n",
        "    for t in reversed(range(None)):\n",
        "        # Compute gradients at time step t. \n",
        "        # Remember to sum gradients from the output path (da) and the previous timesteps (da_prevt) (≈1 line)\n",
        "        gradients = None\n",
        "        # Retrieve derivatives from gradients (≈ 1 line)\n",
        "        dxt, da_prevt, dWaxt, dWaat, dbat = gradients[\"dxt\"], gradients[\"da_prev\"], gradients[\"dWax\"], gradients[\"dWaa\"], gradients[\"dba\"]\n",
        "        # Increment global derivatives w.r.t parameters by adding their derivative at time-step t (≈4 lines)\n",
        "        dx[:, :, t] = None\n",
        "        dWax += None\n",
        "        dWaa += None\n",
        "        dba += None\n",
        "        \n",
        "    # Set da0 to the gradient of a which has been backpropagated through all time-steps (≈1 line) \n",
        "    da0 = None\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Store the gradients in a python dictionary\n",
        "    gradients = {\"dx\": dx, \"da0\": da0, \"dWax\": dWax, \"dWaa\": dWaa,\"dba\": dba}\n",
        "    \n",
        "    return gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0PYumYvHcmB"
      },
      "source": [
        "np.random.seed(1)\n",
        "x_tmp = np.random.randn(3,10,4)\n",
        "a0_tmp = np.random.randn(5,10)\n",
        "parameters_tmp = {}\n",
        "parameters_tmp['Wax'] = np.random.randn(5,3)\n",
        "parameters_tmp['Waa'] = np.random.randn(5,5)\n",
        "parameters_tmp['Wya'] = np.random.randn(2,5)\n",
        "parameters_tmp['ba'] = np.random.randn(5,1)\n",
        "parameters_tmp['by'] = np.random.randn(2,1)\n",
        "\n",
        "a_tmp, y_tmp, caches_tmp = rnn_forward(x_tmp, a0_tmp, parameters_tmp)\n",
        "da_tmp = np.random.randn(5, 10, 4)\n",
        "gradients_tmp = rnn_backward(da_tmp, caches_tmp)\n",
        "\n",
        "print(\"gradients[\\\"dx\\\"][1][2] =\", gradients_tmp[\"dx\"][1][2])\n",
        "print(\"gradients[\\\"dx\\\"].shape =\", gradients_tmp[\"dx\"].shape)\n",
        "print(\"gradients[\\\"da0\\\"][2][3] =\", gradients_tmp[\"da0\"][2][3])\n",
        "print(\"gradients[\\\"da0\\\"].shape =\", gradients_tmp[\"da0\"].shape)\n",
        "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients_tmp[\"dWax\"][3][1])\n",
        "print(\"gradients[\\\"dWax\\\"].shape =\", gradients_tmp[\"dWax\"].shape)\n",
        "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients_tmp[\"dWaa\"][1][2])\n",
        "print(\"gradients[\\\"dWaa\\\"].shape =\", gradients_tmp[\"dWaa\"].shape)\n",
        "print(\"gradients[\\\"dba\\\"][4] =\", gradients_tmp[\"dba\"][4])\n",
        "print(\"gradients[\\\"dba\\\"].shape =\", gradients_tmp[\"dba\"].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPJSIAV9HcbA"
      },
      "source": [
        "**모범 답안**:\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **gradients[\"dx\"][1][2]** =\n",
        "        </td>\n",
        "        <td>\n",
        "           [-2.07101689 -0.59255627  0.02466855  0.01483317]\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dx\"].shape** =\n",
        "        </td>\n",
        "        <td>\n",
        "           (3, 10, 4)\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"da0\"][2][3]** =\n",
        "        </td>\n",
        "        <td>\n",
        "           -0.314942375127\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"da0\"].shape** =\n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 10)\n",
        "        </td>\n",
        "    </tr>\n",
        "         <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWax\"][3][1]** =\n",
        "        </td>\n",
        "        <td>\n",
        "           11.2641044965\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWax\"].shape** =\n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 3)\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWaa\"][1][2]** = \n",
        "        </td>\n",
        "        <td>\n",
        "           2.30333312658\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWaa\"].shape** =\n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 5)\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dba\"][4]** = \n",
        "        </td>\n",
        "        <td>\n",
        "           [-0.74747722]\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dba\"].shape** = \n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 1)\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bHAOo3nHkgu"
      },
      "source": [
        "## 3.2 - LSTM backward pass\n",
        "\n",
        "### 3.2.1 One Step backward\n",
        "\n",
        "LSTM backward pass는 forward pass보다 약간 더 복잡합니다.\n",
        "\n",
        "<img src=\"arts/LSTM_cell_backward_rev3a_c2.png\" style=\"width:500;height:400px;\">\n",
        "<center>그림 8 : lstm_cell_backward. lstm_cell_forward의 일부이지만 출력 함수는 lstm_cell_backward에 포함되지 않습니다.</center>\n",
        "\n",
        "LSTM backward pass에 대한 공식은 다음과 같습니다. (미적분 연습에 익숙한 경우 처음부터 직접 유도 해보십시오.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xv6ro0FeIAlC"
      },
      "source": [
        "#### 3.2.2 gate derivatives\n",
        "dense layer와 활성화 함수 사이의 게이트 미분 ($ \\gamma $ ..)의 위치를 확인합니다 (위의 그래픽 참조). 이것은 다음 단계에서 파라미터의 미분을 계산하는 데 편리합니다.\n",
        "\n",
        "\\begin{align}\n",
        "d\\gamma_o^{\\langle t \\rangle} &= da_{next}*\\tanh(c_{next}) * \\Gamma_o^{\\langle t \\rangle}*\\left(1-\\Gamma_o^{\\langle t \\rangle}\\right)\\tag{7} \\\\[8pt]\n",
        "dp\\widetilde{c}^{\\langle t \\rangle} &= \\left(dc_{next}*\\Gamma_u^{\\langle t \\rangle}+ \\Gamma_o^{\\langle t \\rangle}* (1-\\tanh^2(c_{next})) * \\Gamma_u^{\\langle t \\rangle} * da_{next} \\right) * \\left(1-\\left(\\widetilde c^{\\langle t \\rangle}\\right)^2\\right) \\tag{8} \\\\[8pt]\n",
        "d\\gamma_u^{\\langle t \\rangle} &= \\left(dc_{next}*\\widetilde{c}^{\\langle t \\rangle} + \\Gamma_o^{\\langle t \\rangle}* (1-\\tanh^2(c_{next})) * \\widetilde{c}^{\\langle t \\rangle} * da_{next}\\right)*\\Gamma_u^{\\langle t \\rangle}*\\left(1-\\Gamma_u^{\\langle t \\rangle}\\right)\\tag{9} \\\\[8pt]\n",
        "d\\gamma_f^{\\langle t \\rangle} &= \\left(dc_{next}* c_{prev} + \\Gamma_o^{\\langle t \\rangle} * (1-\\tanh^2(c_{next})) * c_{prev} * da_{next}\\right)*\\Gamma_f^{\\langle t \\rangle}*\\left(1-\\Gamma_f^{\\langle t \\rangle}\\right)\\tag{10}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zr1UFUDIOzk"
      },
      "source": [
        "### 3.2.3 parameter derivatives \n",
        "\n",
        "$ dW_f = d\\gamma_f^{\\langle t \\rangle} \\begin{bmatrix} a_{prev} \\\\ x_t\\end{bmatrix}^T \\tag{11} $\n",
        "$ dW_u = d\\gamma_u^{\\langle t \\rangle} \\begin{bmatrix} a_{prev} \\\\ x_t\\end{bmatrix}^T \\tag{12} $\n",
        "$ dW_c = dp\\widetilde c^{\\langle t \\rangle} \\begin{bmatrix} a_{prev} \\\\ x_t\\end{bmatrix}^T \\tag{13} $\n",
        "$ dW_o = d\\gamma_o^{\\langle t \\rangle} \\begin{bmatrix} a_{prev} \\\\ x_t\\end{bmatrix}^T \\tag{14}$\n",
        "\n",
        "$ db_f, db_u, db_c, db_o $를 계산하려면 $d\\gamma_f^{\\langle t \\rangle}, d\\gamma_u^{\\langle t \\rangle}, dp\\widetilde c^{\\langle t \\rangle}, d\\gamma_o^{\\langle t \\rangle}$ 의 모든 'm'예제 (축 = 1)를 합산하면 됩니다. `keepdims = True` 옵션이 있어야합니다.\n",
        "\n",
        "$\\displaystyle db_f = \\sum_{batch}d\\gamma_f^{\\langle t \\rangle}\\tag{15}$\n",
        "$\\displaystyle db_u = \\sum_{batch}d\\gamma_u^{\\langle t \\rangle}\\tag{16}$\n",
        "$\\displaystyle db_c = \\sum_{batch}d\\gamma_c^{\\langle t \\rangle}\\tag{17}$\n",
        "$\\displaystyle db_o = \\sum_{batch}d\\gamma_o^{\\langle t \\rangle}\\tag{18}$\n",
        "\n",
        "마지막으로, 이전 hidden state, 이전 메모리 상태 및 입력에 대한 미분을 계산합니다.\n",
        "\n",
        "$ da_{prev} = W_f^T d\\gamma_f^{\\langle t \\rangle} + W_u^T   d\\gamma_u^{\\langle t \\rangle}+ W_c^T dp\\widetilde c^{\\langle t \\rangle} + W_o^T d\\gamma_o^{\\langle t \\rangle} \\tag{19}$\n",
        "\n",
        "여기서 연결부를 설명하기 위해 19번 공식의 가중치는 첫 번째 n_a입니다 (예 : $ W_f = W_f [:, : n_a] $ 등 ...).\n",
        "\n",
        "$ dc_{prev} = dc_{next}*\\Gamma_f^{\\langle t \\rangle} + \\Gamma_o^{\\langle t \\rangle} * (1- \\tanh^2(c_{next}))*\\Gamma_f^{\\langle t \\rangle}*da_{next} \\tag{20}$\n",
        "\n",
        "$ dx^{\\langle t \\rangle} = W_f^T d\\gamma_f^{\\langle t \\rangle} + W_u^T  d\\gamma_u^{\\langle t \\rangle}+ W_c^T dp\\widetilde c^{\\langle t \\rangle} + W_o^T d\\gamma_o^{\\langle t \\rangle}\\tag{21} $\n",
        "\n",
        "공식 21의 가중치는 n_a에서 끝까지 입니다 (예 : $ W_f = W_f [:, n_a :] $ 등 ...)\n",
        "\n",
        "<br>\n",
        "\n",
        "**연습 문제 :** 위의 공식 $ 7-21 $을 사용하여 `lstm_cell_backward` 를 구현합니다.\n",
        "\n",
        "    \n",
        "참고 : 코드에서 :\n",
        "\n",
        "$d\\gamma_o^{\\langle t \\rangle}$ is represented by  `dot`,    \n",
        "$dp\\widetilde{c}^{\\langle t \\rangle}$ is represented by  `dcct`,  \n",
        "$d\\gamma_u^{\\langle t \\rangle}$ is represented by  `dit`,  \n",
        "$d\\gamma_f^{\\langle t \\rangle}$ is represented by  `dft`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXQn4qzGHjWA"
      },
      "source": [
        "def lstm_cell_backward(da_next, dc_next, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward pass for the LSTM-cell (single time-step).\n",
        "\n",
        "    Arguments:\n",
        "    da_next -- Gradients of next hidden state, of shape (n_a, m)\n",
        "    dc_next -- Gradients of next cell state, of shape (n_a, m)\n",
        "    cache -- cache storing information from the forward pass\n",
        "\n",
        "    Returns:\n",
        "    gradients -- python dictionary containing:\n",
        "                        dxt -- Gradient of input data at time-step t, of shape (n_x, m)\n",
        "                        da_prev -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)\n",
        "                        dc_prev -- Gradient w.r.t. the previous memory state, of shape (n_a, m, T_x)\n",
        "                        dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        dWo -- Gradient w.r.t. the weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)\n",
        "                        dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)\n",
        "                        dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)\n",
        "                        dbo -- Gradient w.r.t. biases of the output gate, of shape (n_a, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve information from \"cache\"\n",
        "    (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Retrieve dimensions from xt's and a_next's shape (≈2 lines)\n",
        "    n_x, m = None\n",
        "    n_a, m = None\n",
        "    \n",
        "    # Compute gates related derivatives, you can find their values can be found by looking carefully at equations (7) to (10) (≈4 lines)\n",
        "    dot = None\n",
        "    dcct = None\n",
        "    dit = None\n",
        "    dft = None\n",
        "    \n",
        "    # Compute parameters related derivatives. Use equations (11)-(18) (≈8 lines)\n",
        "    dWf = None\n",
        "    dWi = None\n",
        "    dWc = None\n",
        "    dWo = None\n",
        "    dbf = None\n",
        "    dbi = None\n",
        "    dbc = None\n",
        "    dbo = None\n",
        "\n",
        "    # Compute derivatives w.r.t previous hidden state, previous memory state and input. Use equations (19)-(21). (≈3 lines)\n",
        "    da_prev = None\n",
        "    dc_prev = None\n",
        "    dxt = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Save gradients in dictionary\n",
        "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dc_prev\": dc_prev, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi,\n",
        "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
        "\n",
        "    return gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Og6taBDsJdF4"
      },
      "source": [
        "np.random.seed(1)\n",
        "xt_tmp = np.random.randn(3,10)\n",
        "a_prev_tmp = np.random.randn(5,10)\n",
        "c_prev_tmp = np.random.randn(5,10)\n",
        "parameters_tmp = {}\n",
        "parameters_tmp['Wf'] = np.random.randn(5, 5+3)\n",
        "parameters_tmp['bf'] = np.random.randn(5,1)\n",
        "parameters_tmp['Wi'] = np.random.randn(5, 5+3)\n",
        "parameters_tmp['bi'] = np.random.randn(5,1)\n",
        "parameters_tmp['Wo'] = np.random.randn(5, 5+3)\n",
        "parameters_tmp['bo'] = np.random.randn(5,1)\n",
        "parameters_tmp['Wc'] = np.random.randn(5, 5+3)\n",
        "parameters_tmp['bc'] = np.random.randn(5,1)\n",
        "parameters_tmp['Wy'] = np.random.randn(2,5)\n",
        "parameters_tmp['by'] = np.random.randn(2,1)\n",
        "\n",
        "a_next_tmp, c_next_tmp, yt_tmp, cache_tmp = lstm_cell_forward(xt_tmp, a_prev_tmp, c_prev_tmp, parameters_tmp)\n",
        "\n",
        "da_next_tmp = np.random.randn(5,10)\n",
        "dc_next_tmp = np.random.randn(5,10)\n",
        "gradients_tmp = lstm_cell_backward(da_next_tmp, dc_next_tmp, cache_tmp)\n",
        "print(\"gradients[\\\"dxt\\\"][1][2] =\", gradients_tmp[\"dxt\"][1][2])\n",
        "print(\"gradients[\\\"dxt\\\"].shape =\", gradients_tmp[\"dxt\"].shape)\n",
        "print(\"gradients[\\\"da_prev\\\"][2][3] =\", gradients_tmp[\"da_prev\"][2][3])\n",
        "print(\"gradients[\\\"da_prev\\\"].shape =\", gradients_tmp[\"da_prev\"].shape)\n",
        "print(\"gradients[\\\"dc_prev\\\"][2][3] =\", gradients_tmp[\"dc_prev\"][2][3])\n",
        "print(\"gradients[\\\"dc_prev\\\"].shape =\", gradients_tmp[\"dc_prev\"].shape)\n",
        "print(\"gradients[\\\"dWf\\\"][3][1] =\", gradients_tmp[\"dWf\"][3][1])\n",
        "print(\"gradients[\\\"dWf\\\"].shape =\", gradients_tmp[\"dWf\"].shape)\n",
        "print(\"gradients[\\\"dWi\\\"][1][2] =\", gradients_tmp[\"dWi\"][1][2])\n",
        "print(\"gradients[\\\"dWi\\\"].shape =\", gradients_tmp[\"dWi\"].shape)\n",
        "print(\"gradients[\\\"dWc\\\"][3][1] =\", gradients_tmp[\"dWc\"][3][1])\n",
        "print(\"gradients[\\\"dWc\\\"].shape =\", gradients_tmp[\"dWc\"].shape)\n",
        "print(\"gradients[\\\"dWo\\\"][1][2] =\", gradients_tmp[\"dWo\"][1][2])\n",
        "print(\"gradients[\\\"dWo\\\"].shape =\", gradients_tmp[\"dWo\"].shape)\n",
        "print(\"gradients[\\\"dbf\\\"][4] =\", gradients_tmp[\"dbf\"][4])\n",
        "print(\"gradients[\\\"dbf\\\"].shape =\", gradients_tmp[\"dbf\"].shape)\n",
        "print(\"gradients[\\\"dbi\\\"][4] =\", gradients_tmp[\"dbi\"][4])\n",
        "print(\"gradients[\\\"dbi\\\"].shape =\", gradients_tmp[\"dbi\"].shape)\n",
        "print(\"gradients[\\\"dbc\\\"][4] =\", gradients_tmp[\"dbc\"][4])\n",
        "print(\"gradients[\\\"dbc\\\"].shape =\", gradients_tmp[\"dbc\"].shape)\n",
        "print(\"gradients[\\\"dbo\\\"][4] =\", gradients_tmp[\"dbo\"][4])\n",
        "print(\"gradients[\\\"dbo\\\"].shape =\", gradients_tmp[\"dbo\"].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf8_PAUMJhtw"
      },
      "source": [
        "**모범 답안**:\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **gradients[\"dxt\"][1][2]** =\n",
        "        </td>\n",
        "        <td>\n",
        "           3.23055911511\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dxt\"].shape** =\n",
        "        </td>\n",
        "        <td>\n",
        "           (3, 10)\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"da_prev\"][2][3]** =\n",
        "        </td>\n",
        "        <td>\n",
        "           -0.0639621419711\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"da_prev\"].shape** =\n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 10)\n",
        "        </td>\n",
        "    </tr>\n",
        "         <tr>\n",
        "        <td>\n",
        "            **gradients[\"dc_prev\"][2][3]** =\n",
        "        </td>\n",
        "        <td>\n",
        "           0.797522038797\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dc_prev\"].shape** =\n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 10)\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWf\"][3][1]** = \n",
        "        </td>\n",
        "        <td>\n",
        "           -0.147954838164\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWf\"].shape** =\n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 8)\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWi\"][1][2]** = \n",
        "        </td>\n",
        "        <td>\n",
        "           1.05749805523\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWi\"].shape** = \n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 8)\n",
        "        </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWc\"][3][1]** = \n",
        "        </td>\n",
        "        <td>\n",
        "           2.30456216369\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWc\"].shape** = \n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 8)\n",
        "        </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWo\"][1][2]** = \n",
        "        </td>\n",
        "        <td>\n",
        "           0.331311595289\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWo\"].shape** = \n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 8)\n",
        "        </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **gradients[\"dbf\"][4]** = \n",
        "        </td>\n",
        "        <td>\n",
        "           [ 0.18864637]\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dbf\"].shape** = \n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 1)\n",
        "        </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **gradients[\"dbi\"][4]** = \n",
        "        </td>\n",
        "        <td>\n",
        "           [-0.40142491]\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dbi\"].shape** = \n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 1)\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dbc\"][4]** = \n",
        "        </td>\n",
        "        <td>\n",
        "           [ 0.25587763]\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dbc\"].shape** = \n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 1)\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dbo\"][4]** = \n",
        "        </td>\n",
        "        <td>\n",
        "           [ 0.13893342]\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dbo\"].shape** = \n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 1)\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3MOo_8FJnsT"
      },
      "source": [
        "### 3.3 Backward pass through the LSTM RNN\n",
        "\n",
        "이 부분은 위에서 구현 한 `rnn_backward` 함수와 매우 유사합니다. 먼저 리턴 값과 동일한 차원의 변수를 만듭니다. 그런 다음 끝부터 시작하여 모든 time step을 반복하고 각 반복에서 LSTM에 대해 구현 한 1 단계 함수를 호출합니다. 그런 다음 파라미터를 개별적으로 합산하여 업데이트합니다. 마지막으로 업데이트된 gradient가 있는 딕셔너리를 반환합니다.\n",
        "\n",
        "**지시 사항** :`lstm_backward` 함수를 구현합니다. $ T_x $에서 시작하여 뒤로 돌아가는 for 루프를 만듭니다. 각 단계에 대해 `lstm_cell_backward`를 호출하고 새 그라디언트를 추가하여 이전 그라디언트를 업데이트하십시오. `dxt`는 업데이트되지 않고 저장됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jX8wIJ4tJjtG"
      },
      "source": [
        "def lstm_backward(da, caches):\n",
        "    \n",
        "    \"\"\"\n",
        "    Implement the backward pass for the RNN with LSTM-cell (over a whole sequence).\n",
        "\n",
        "    Arguments:\n",
        "    da -- Gradients w.r.t the hidden states, numpy-array of shape (n_a, m, T_x)\n",
        "    caches -- cache storing information from the forward pass (lstm_forward)\n",
        "\n",
        "    Returns:\n",
        "    gradients -- python dictionary containing:\n",
        "                        dx -- Gradient of inputs, of shape (n_x, m, T_x)\n",
        "                        da0 -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)\n",
        "                        dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        dWo -- Gradient w.r.t. the weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)\n",
        "                        dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)\n",
        "                        dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)\n",
        "                        dbo -- Gradient w.r.t. biases of the save gate, of shape (n_a, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve values from the first cache (t=1) of caches.\n",
        "    (caches, x) = caches\n",
        "    (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches[0]\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Retrieve dimensions from da's and x1's shapes (≈2 lines)\n",
        "    n_a, m, T_x = None\n",
        "    n_x, m = None\n",
        "    \n",
        "    # initialize the gradients with the right sizes (≈12 lines)\n",
        "    dx = None\n",
        "    da0 = None\n",
        "    da_prevt = None\n",
        "    dc_prevt = None\n",
        "    dWf = None\n",
        "    dWi = None\n",
        "    dWc = None\n",
        "    dWo = None\n",
        "    dbf = None\n",
        "    dbi = None\n",
        "    dbc = None\n",
        "    dbo = None\n",
        "    \n",
        "    # loop back over the whole sequence\n",
        "    for t in reversed(range(None)):\n",
        "        # Compute all gradients using lstm_cell_backward\n",
        "        gradients = None\n",
        "        # Store or add the gradient to the parameters' previous step's gradient\n",
        "        da_prevt = None\n",
        "        dc_prevt = None\n",
        "        dx[:,:,t] = None\n",
        "        dWf += None\n",
        "        dWi += None\n",
        "        dWc += None\n",
        "        dWo += None\n",
        "        dbf += None\n",
        "        dbi += None\n",
        "        dbc += None\n",
        "        dbo += None\n",
        "    # Set the first activation's gradient to the backpropagated gradient da_prev.\n",
        "    da0 = None\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Store the gradients in a python dictionary\n",
        "    gradients = {\"dx\": dx, \"da0\": da0, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi,\n",
        "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
        "    \n",
        "    return gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtroeOaBJ_Ec"
      },
      "source": [
        "np.random.seed(1)\n",
        "x_tmp = np.random.randn(3,10,7)\n",
        "a0_tmp = np.random.randn(5,10)\n",
        "\n",
        "parameters_tmp = {}\n",
        "parameters_tmp['Wf'] = np.random.randn(5, 5+3)\n",
        "parameters_tmp['bf'] = np.random.randn(5,1)\n",
        "parameters_tmp['Wi'] = np.random.randn(5, 5+3)\n",
        "parameters_tmp['bi'] = np.random.randn(5,1)\n",
        "parameters_tmp['Wo'] = np.random.randn(5, 5+3)\n",
        "parameters_tmp['bo'] = np.random.randn(5,1)\n",
        "parameters_tmp['Wc'] = np.random.randn(5, 5+3)\n",
        "parameters_tmp['bc'] = np.random.randn(5,1)\n",
        "parameters_tmp['Wy'] = np.zeros((2,5))       # unused, but needed for lstm_forward\n",
        "parameters_tmp['by'] = np.zeros((2,1))       # unused, but needed for lstm_forward\n",
        "\n",
        "a_tmp, y_tmp, c_tmp, caches_tmp = lstm_forward(x_tmp, a0_tmp, parameters_tmp)\n",
        "\n",
        "da_tmp = np.random.randn(5, 10, 4)\n",
        "gradients_tmp = lstm_backward(da_tmp, caches_tmp)\n",
        "\n",
        "print(\"gradients[\\\"dx\\\"][1][2] =\", gradients_tmp[\"dx\"][1][2])\n",
        "print(\"gradients[\\\"dx\\\"].shape =\", gradients_tmp[\"dx\"].shape)\n",
        "print(\"gradients[\\\"da0\\\"][2][3] =\", gradients_tmp[\"da0\"][2][3])\n",
        "print(\"gradients[\\\"da0\\\"].shape =\", gradients_tmp[\"da0\"].shape)\n",
        "print(\"gradients[\\\"dWf\\\"][3][1] =\", gradients_tmp[\"dWf\"][3][1])\n",
        "print(\"gradients[\\\"dWf\\\"].shape =\", gradients_tmp[\"dWf\"].shape)\n",
        "print(\"gradients[\\\"dWi\\\"][1][2] =\", gradients_tmp[\"dWi\"][1][2])\n",
        "print(\"gradients[\\\"dWi\\\"].shape =\", gradients_tmp[\"dWi\"].shape)\n",
        "print(\"gradients[\\\"dWc\\\"][3][1] =\", gradients_tmp[\"dWc\"][3][1])\n",
        "print(\"gradients[\\\"dWc\\\"].shape =\", gradients_tmp[\"dWc\"].shape)\n",
        "print(\"gradients[\\\"dWo\\\"][1][2] =\", gradients_tmp[\"dWo\"][1][2])\n",
        "print(\"gradients[\\\"dWo\\\"].shape =\", gradients_tmp[\"dWo\"].shape)\n",
        "print(\"gradients[\\\"dbf\\\"][4] =\", gradients_tmp[\"dbf\"][4])\n",
        "print(\"gradients[\\\"dbf\\\"].shape =\", gradients_tmp[\"dbf\"].shape)\n",
        "print(\"gradients[\\\"dbi\\\"][4] =\", gradients_tmp[\"dbi\"][4])\n",
        "print(\"gradients[\\\"dbi\\\"].shape =\", gradients_tmp[\"dbi\"].shape)\n",
        "print(\"gradients[\\\"dbc\\\"][4] =\", gradients_tmp[\"dbc\"][4])\n",
        "print(\"gradients[\\\"dbc\\\"].shape =\", gradients_tmp[\"dbc\"].shape)\n",
        "print(\"gradients[\\\"dbo\\\"][4] =\", gradients_tmp[\"dbo\"][4])\n",
        "print(\"gradients[\\\"dbo\\\"].shape =\", gradients_tmp[\"dbo\"].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfSg-E93J_yi"
      },
      "source": [
        "**모범 답안**:\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **gradients[\"dx\"][1][2]** =\n",
        "        </td>\n",
        "        <td>\n",
        "           [0.00218254  0.28205375 -0.48292508 -0.43281115]\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dx\"].shape** =\n",
        "        </td>\n",
        "        <td>\n",
        "           (3, 10, 4)\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"da0\"][2][3]** =\n",
        "        </td>\n",
        "        <td>\n",
        "           0.312770310257\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"da0\"].shape** =\n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 10)\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWf\"][3][1]** = \n",
        "        </td>\n",
        "        <td>\n",
        "           -0.0809802310938\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWf\"].shape** =\n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 8)\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWi\"][1][2]** = \n",
        "        </td>\n",
        "        <td>\n",
        "           0.40512433093\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWi\"].shape** = \n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 8)\n",
        "        </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWc\"][3][1]** = \n",
        "        </td>\n",
        "        <td>\n",
        "           -0.0793746735512\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWc\"].shape** = \n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 8)\n",
        "        </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWo\"][1][2]** = \n",
        "        </td>\n",
        "        <td>\n",
        "           0.038948775763\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dWo\"].shape** = \n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 8)\n",
        "        </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **gradients[\"dbf\"][4]** = \n",
        "        </td>\n",
        "        <td>\n",
        "           [-0.15745657]\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dbf\"].shape** = \n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 1)\n",
        "        </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **gradients[\"dbi\"][4]** = \n",
        "        </td>\n",
        "        <td>\n",
        "           [-0.50848333]\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dbi\"].shape** = \n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 1)\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dbc\"][4]** = \n",
        "        </td>\n",
        "        <td>\n",
        "           [-0.42510818]\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dbc\"].shape** = \n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 1)\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dbo\"][4]** = \n",
        "        </td>\n",
        "        <td>\n",
        "           [ -0.17958196]\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **gradients[\"dbo\"].shape** = \n",
        "        </td>\n",
        "        <td>\n",
        "           (5, 1)\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxDX7ifOKKJu"
      },
      "source": [
        "### Congratulations\n",
        "\n",
        "이 과제를 완료 한 것을 축하합니다. 이제 순환 신경망이 어떻게 작동하는지 이해했습니다!\n",
        "\n",
        "RNN을 사용하여 문자 수준 언어 모델을 빌드하는 다음 과제로 넘어가겠습니다."
      ]
    }
  ]
}