{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dinosaurus_Island_Character_level_language_model_final_v3b.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMzyi2Y8AlHXyaPLtijFh4b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skfo763/Google-ML-Bootcamp-phase1/blob/main/course5/week1/Dinosaurus_Island_Character_level_language_model_final_v3b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtYm17-yMiae"
      },
      "source": [
        "# Character level language model - Dinosaurus Island\n",
        "\n",
        "Dinosaurus Island에 오신 것을 환영합니다! 현실 세계에선 공룡은 6천 5백만년 전에 멸종했지만, 이번 과제에서는 그 공룡이 다시 돌아왔다고 합니다. 능력있는 생물학자들이 새로운 종류의 공룡을 만들어냈고, 당신의 임무는 이 공룡들에게 이름을 부여하는 것입니다. 공룡이 이름이 마음에 들지 않으면 미쳐 날뛸 수 있으니 현명하게 선택하세요!\n",
        "\n",
        "<table>\n",
        "<td>\n",
        "<img src=\"arts/dino.jpg\" style=\"width:250;height:300px;\">\n",
        "</td>\n",
        "</table>\n",
        "\n",
        "\n",
        "다행히도 여러분은 지금까지 딥 러닝을 배웠으며, 이를 사용하여 시간을 절약 할 수 있습니다. 여러분의 연구 보조원이 현존하는 모든 공룡 이름 목록을 수집하여 [dataset](dinos.txt)로 정리했습니다. (해당 링크를 클릭하여 자유롭게 살펴보십시오.) 새 공룡 이름을 만들려면 문자 수준의 언어 모델을 만들어 새 이름을 생성해야 합니다. 알고리즘은 위 공룡 이름 데이터를 바탕으로 패턴을 학습하고 무작위로 새 이름을 생성합니다. 여러분이 만들 알고리즘이 공룡의 분노로부터 여러분과 여러분의 팀을 안전하게 지켜 주길 바랍니다!\n",
        "\n",
        "이 과제를 완료하면 다음을 배울 수 있습니다.\n",
        "\n",
        "- RNN을 사용하여 처리 할 텍스트 데이터를 저장하는 방법\n",
        "- 각 시간 단계에서 예측을 샘플링하여 다음 RNN-cell 단위로 전달하여 데이터를 합성하는 방법\n",
        "- 문자 수준 텍스트 생성 순환 신경망 구축 방법\n",
        "- 그래디언트 클리핑이 중요한 이유\n",
        "\n",
        "`rnn_utils` 에서 제공 한 일부 함수를 로드하는 것으로 시작합니다. 특히, 이전 과제에서 구현 한 것과 동일한 `rnn_forward` 및 `rnn_backward`와 같은 함수에 액세스 할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYY6raU0NmF2"
      },
      "source": [
        "import numpy as np\n",
        "from utils import *\n",
        "import random\n",
        "import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCKWSlTQNoyq"
      },
      "source": [
        "## 1 - Problem Statement\n",
        "\n",
        "### 1.1 - Dataset and Preprocessing\n",
        "\n",
        "다음 셀을 실행하여 공룡 이름 데이터 세트를 읽고 고유 문자 (예 : a-z) 목록을 만들고 데이터 세트와 어휘 크기를 계산합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0XR4U2wNtrV"
      },
      "source": [
        "data = open('dinos.txt', 'r').read()\n",
        "data= data.lower()\n",
        "chars = list(set(data))\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikBsR7eCNzoW"
      },
      "source": [
        "- 데이터 세트에 나타나는 문자는 알파벳 a-z(26 자)와 개행문자 \"\\n\"입니다.\n",
        "- 이번 과제에서 개행문자 \"\\n\"은 우리가 강의에서 논의한 `<EOS>`(또는 \"문의 끝\") 토큰과 유사한 역할을 합니다.\n",
        "  - 여기서 \"\\n\"은 문장의 끝이 아닌 공룡 이름의 끝을 나타냅니다.\n",
        "- `char_to_ix` : 아래 셀에서 각 문자를 0-26의 인덱스에 매핑하는 파이썬 딕셔너리 (즉, 해시 테이블)을 만듭니다.\n",
        "- `ix_to_char` : 각 인덱스를 해당 문자에 다시 매핑하는 두 번째 파이썬 딕셔너리도 만듭니다.\n",
        "  - 이것은 softmax 레이어의 확률 분포 출력에서 어떤 문자에 해당하는 인덱스를 파악하는 데 도움이됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XA2fGrlOIxo"
      },
      "source": [
        "chars = sorted(chars)\n",
        "print(chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XajLSLKgOK1p"
      },
      "source": [
        "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
        "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "pp.pprint(ix_to_char)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBtZLjwqOLOI"
      },
      "source": [
        "### 1.2 - Overview of the model\n",
        "\n",
        "모델의 구조는 다음과 같습니다.\n",
        "\n",
        "- 파라미터 초기화\n",
        "- 최적화 루프 실행\n",
        "  - 손실 함수를 계산하기위한 forward propagation\n",
        "  - 손실 함수에 대한 기울기를 계산하기 위한 back propagation\n",
        "  - Gradient exploding이 일어나지 않도록 그라디언트를 자릅니다.\n",
        "  - 경사하강법으로 파라미터를 업데이트합니다.\n",
        "- 최종 학습 된 파라미터 반환\n",
        "\n",
        "<img src=\"arts/rnn.png\" style=\"width:450;height:300px;\">\n",
        "<center>그림 1 : 이전 과제 \"Building a Recurrent Neural Network-Step by Step\"에서 구현 한 것과 유사한 Recurrent Neural Network</center>\n",
        "\n",
        "* 각 time step에서 RNN은 이전 문자가 주어지면 다음 문자가 무엇인지 예측하려고합니다.\n",
        "* 데이터 세트 $ \\mathbf {X} = (x ^ {\\langle 1 \\rangle}, x ^ {\\langle 2 \\rangle}, ..., x ^{\\langle T_x \\rangle}) $은 훈련 세트의 문자.\n",
        "* $ \\mathbf {Y} = (y ^ {\\langle 1 \\rangle}, y ^ {\\langle 2 \\rangle}, ..., y ^ {\\langle T_x \\rangle}) $은 동일한 문자 목록입니다. 하지만 한 문자 앞으로 이동했습니다.\n",
        "* 모든 시간 단계에서 $ t $, $ y ^ {\\langle t \\rangle} = x ^ {\\langle t + 1 \\rangle} $. 시간 $ t $에서의 예측은 시간 $ t + 1 $에서의 입력과 동일합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdJ4KuK0PjyD"
      },
      "source": [
        "## 2 - Building blocks of the model\n",
        "\n",
        "이 부분에서는 전체 모델의 두 가지 중요한 블록을 작성합니다.\n",
        "- Gradient Cliping : Exploding gradient 방지\n",
        "- Sampling : 캐릭터 생성에 사용되는 기술\n",
        "\n",
        "그런 다음 이 두 기능을 적용하여 모델을 빌드합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN5h7vRbPv_l"
      },
      "source": [
        "### 2.1 - Clipping the gradients in the optimization loop\n",
        "\n",
        "#### Exploding gradients\n",
        "\n",
        "* 계산되는 gradient가 매우 크면 \"그라디언트 폭발\"이라고합니다.\n",
        "* 폭발 기울기는 업데이트되는 파라미터가 너무 커서 역전파 동안 최적의 값을 넘어갈 수 있기 때문에 모델 훈련을 더 어렵게 만듭니다.\n",
        "\n",
        "전체 루프 구조는 일반적으로 다음으로 구성됩니다.\n",
        "* Forward propagation\n",
        "* Compute cost\n",
        "* Backward propagation\n",
        "* Parameter update\n",
        "\n",
        "파라미터를 업데이트하기 전에 Gradient clipping을 수행하여 그라디언트가 \"폭발\"하지 않는지 확인합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXH8HjbjQY0A"
      },
      "source": [
        "#### gradient clipping\n",
        "\n",
        "아래 연습 문제에서는 그라디언트 딕셔너리를 가져와 클리핑 된 새로운 그라디언트를 반환하는 함수 `clip`을 구현합니다.\n",
        "\n",
        "* 그라디언트를 자르는 방법에는 여러 가지가 있습니다.\n",
        "* 간단히 행렬의 요소 별로 클리핑 절차를 사용합니다. 여기서 그라디언트 벡터의 모든 요소는 [-N, N] 범위 사이에 놓 이도록 클리핑됩니다.\n",
        "* 예를 들어 N = 10 인 경우\n",
        "  - 범위는 [-10, 10]입니다.\n",
        "  - 그래디언트 벡터의 구성 요소가 10보다 크면 10으로 설정됩니다.\n",
        "  - 그래디언트 벡터의 구성 요소가 -10보다 작 으면 -10으로 설정됩니다.\n",
        "  - 구성 요소가 -10에서 10 사이이면 원래 값을 유지합니다.\n",
        "\n",
        "<img src=\"arts/clip.png\" style=\"width:400;height:150px;\">\n",
        "번역 결과\n",
        "<center>그림 2 : 네트워크가 \"Gradient Exploding\" 문제에 직면 한 경우 Gradient clipping이 있거나 없는 경우의 경사 하강법 시각화.</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNxcveyWRDN_"
      },
      "source": [
        "**연습 문제**:\n",
        "딕셔너리 '그라디언트'의 clipping된 그라디언트를 반환하려면 아래 함수를 구현하세요.\n",
        "\n",
        "* 함수는 최대 임계 값을 인자로 받고 그라디언트의 클리핑 된 버전을 반환합니다.\n",
        "* [numpy.clip](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.clip.html)을 확인하세요.\n",
        "  - \"`out = ...`\" 파라미터를 사용해야합니다.\n",
        "  - \"`out`\" 파라미터를 사용하면 \"in-place\" 변수를 업데이트 할 수 있습니다.\n",
        "  - \"`out`\" 파라미터를 사용하지 않으면 잘린 변수가 \"gradient\"변수에 저장되지만 그래디언트 변수 `dWax`,`dWaa`,`dWya`,`db`,`dby`는 업데이트되지 않습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyG9qSS3Pvwq"
      },
      "source": [
        "### GRADED FUNCTION: clip\n",
        "\n",
        "def clip(gradients, maxValue):\n",
        "    '''\n",
        "    Clips the gradients' values between minimum and maximum.\n",
        "    \n",
        "    Arguments:\n",
        "    gradients -- a dictionary containing the gradients \"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n",
        "    maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue\n",
        "    \n",
        "    Returns: \n",
        "    gradients -- a dictionary with the clipped gradients.\n",
        "    '''\n",
        "    \n",
        "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
        "   \n",
        "    ### START CODE HERE ###\n",
        "    # clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]. (≈2 lines)\n",
        "    for gradient in [None]:\n",
        "        None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
        "    \n",
        "    return gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hChyvW2ARZ1F"
      },
      "source": [
        "# Test with a maxvalue of 10\n",
        "mValue = 10\n",
        "np.random.seed(3)\n",
        "dWax = np.random.randn(5,3)*10\n",
        "dWaa = np.random.randn(5,5)*10\n",
        "dWya = np.random.randn(2,5)*10\n",
        "db = np.random.randn(5,1)*10\n",
        "dby = np.random.randn(2,1)*10\n",
        "gradients = {\"dWax\": dWax, \"dWaa\": dWaa, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
        "gradients = clip(gradients, mValue)\n",
        "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
        "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
        "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
        "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
        "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzQkHktaQYPx"
      },
      "source": [
        "**모범 답안**\n",
        "\n",
        "```Python\n",
        "gradients[\"dWaa\"][1][2] = 10.0\n",
        "gradients[\"dWax\"][3][1] = -10.0\n",
        "gradients[\"dWya\"][1][2] = 0.29713815361\n",
        "gradients[\"db\"][4] = [ 10.]\n",
        "gradients[\"dby\"][1] = [ 8.45833407]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIwXNgt1RciK"
      },
      "source": [
        "# Test with a maxValue of 5\n",
        "mValue = 5\n",
        "np.random.seed(3)\n",
        "dWax = np.random.randn(5,3)*10\n",
        "dWaa = np.random.randn(5,5)*10\n",
        "dWya = np.random.randn(2,5)*10\n",
        "db = np.random.randn(5,1)*10\n",
        "dby = np.random.randn(2,1)*10\n",
        "gradients = {\"dWax\": dWax, \"dWaa\": dWaa, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
        "gradients = clip(gradients, mValue)\n",
        "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
        "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
        "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
        "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
        "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])\n",
        "del mValue # avoid common issue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f81ALhfDRfGP"
      },
      "source": [
        "**모범 답안 :**\n",
        "```Python\n",
        "gradients[\"dWaa\"][1][2] = 5.0\n",
        "gradients[\"dWax\"][3][1] = -5.0\n",
        "gradients[\"dWya\"][1][2] = 0.29713815361\n",
        "gradients[\"db\"][4] = [ 5.]\n",
        "gradients[\"dby\"][1] = [ 5.]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTaMJJpenXyg"
      },
      "source": [
        "### 2.2 - Sampling\n",
        "\n",
        "이제 모델이 학습되었다고 가정해봅시다. 새 텍스트(문자열)를 생성하려고 합니다. 생성 과정은 아래 그림에 설명되어 있습니다.\n",
        "\n",
        "<img src=\"arts/dinos3.png\" style=\"width:500;height:300px;\">\n",
        "<center>그림 3 : 이 그림에서는 모델이 이미 훈련되었다고 가정합니다. 첫 번째 단계에서 $ x ^ {\\langle 1 \\rangle} = \\vec{0} $를 전달하고 신경망에서 한 번에 한 문자 씩 샘플링합니다.</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdlPsl9Cokjz"
      },
      "source": [
        "**연습 문제**: 문자열을 샘플링하는 `sample` 함수를 구현해보세요. 4가지 단계를 따라 구현합니다.\n",
        "\n",
        "\n",
        "- **1단계** : 0으로 구성된 \"더미\" 벡터 $ x ^ {\\langle 1 \\rangle} = \\vec{0} $를 입력합니다.\n",
        "  - 이것은 문자를 생성하기 전의 기본 입력입니다. 마찬가지로 $ a^{\\langle 0 \\rangle} = \\vec {0} $ 도 설정했습니다.\n",
        "\n",
        "- **2단계** : \n",
        "$ a ^ {\\langle 1 \\rangle} $ 및 $ \\hat {y} ^ {\\langle 1 \\rangle} $를 얻기 위해 한 단계의 forward propagation을 실행합니다. 공식은 다음과 같습니다.\n",
        "\n",
        "  - hidden state:  \n",
        "$$ a^{\\langle t+1 \\rangle} = \\tanh(W_{ax}  x^{\\langle t+1 \\rangle } + W_{aa} a^{\\langle t \\rangle } + b)\\tag{1}$$\n",
        "\n",
        "  - activation:\n",
        "$$ z^{\\langle t + 1 \\rangle } = W_{ya}  a^{\\langle t + 1 \\rangle } + b_y \\tag{2}$$\n",
        "\n",
        "  - prediction:\n",
        "$$ \\hat{y}^{\\langle t+1 \\rangle } = softmax(z^{\\langle t + 1 \\rangle })\\tag{3}$$\n",
        "\n",
        "\n",
        "- $ \\hat {y} ^ {\\langle t + 1 \\rangle} $에 대한 세부 정보 :\n",
        "  - $ \\hat {y} ^ {\\langle t + 1 \\rangle} $은 (소프트 맥스) 확률 벡터입니다 (항목은 0과 1 사이이고 합은 1입니다).\n",
        "  - $ \\hat {y} ^ {\\langle t + 1 \\rangle} _i $는 \"i\"로 인덱싱 된 문자가 다음 문자일 확률을 나타냅니다.\n",
        "  - 사전 제공되는 `softmax ()`함수를 사용하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6vGkaKEpnJg"
      },
      "source": [
        "#### 추가 힌트\n",
        "\n",
        "- $ x ^ {\\langle 1 \\rangle} $은 코드에서 `x`입니다. one-hot 인코딩 벡터를 만들 때 행 수는 고유 문자 수와 같고 열 수는 1과 같은 0으로 구성된 numpy 배열을 만듭니다. 1D 배열이 아니라 2D입니다.\n",
        "- $ a ^ {\\langle 0 \\rangle} $은 코드에서 `a_prev`입니다. 행 수는 $ n_{a} $이고 열 수는 1 인 0으로 구성된 numpy 배열입니다. 이는 2D 배열이기도합니다. $ n_{a} $는 $ W_{aa} $의 열 수를 가져 와서 구할 수 있습니다 (행렬 곱셈 $ W_{aa} a ^ {\\langle t \\rangle} $이 작동하려면 숫자가 일치해야합니다. .\n",
        "-[numpy.dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html)\n",
        "-[numpy.tanh](https://docs.scipy.org/doc/numpy/reference/generated/numpy.tanh.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-RFpXSaqzOK"
      },
      "source": [
        "#### Using 2D arrays instead of 1D arrays\n",
        "\n",
        "- $ x ^ {\\langle 1 \\rangle} $ 및 $ a ^ {\\langle 0 \\rangle} $이 1D 벡터가 아니라 2D 배열임을 강조하는 이유가 궁금 할 것입니다.\n",
        "- numpy의 행렬 곱셈의 경우 2D 행렬에 1D 벡터를 곱하면 결국 1D 배열이됩니다.\n",
        "- 이것은 우리가 동일한 shape를 가질 것으로 예상했던 두 개의 배열을 추가 할 때 문제가 됩니다.\n",
        "- 차원 수가 다른 두 배열이 함께 추가되면 Python에선 broadcast 기능이 동작합니다.\n",
        "- 다음은 1D 배열과 2D 배열 사용의 차이점을 보여주는 몇 가지 샘플 코드입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01v3MlB6okCD"
      },
      "source": [
        "matrix1 = np.array([[1,1],[2,2],[3,3]]) # (3,2)\n",
        "matrix2 = np.array([[0],[0],[0]]) # (3,1) \n",
        "vector1D = np.array([1,1]) # (2,) \n",
        "vector2D = np.array([[1],[1]]) # (2,1)\n",
        "print(\"matrix1 \\n\", matrix1,\"\\n\")\n",
        "print(\"matrix2 \\n\", matrix2,\"\\n\")\n",
        "print(\"vector1D \\n\", vector1D,\"\\n\")\n",
        "print(\"vector2D \\n\", vector2D)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtpKm_wArHL9"
      },
      "source": [
        "print(\"Multiply 2D and 1D arrays: result is a 1D array\\n\", \n",
        "      np.dot(matrix1,vector1D))\n",
        "print(\"Multiply 2D and 2D arrays: result is a 2D array\\n\", \n",
        "      np.dot(matrix1,vector2D))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM0Cv4-OrH2S"
      },
      "source": [
        "print(\"Adding (3 x 1) vector to a (3 x 1) vector is a (3 x 1) vector\\n\",\n",
        "      \"This is what we want here!\\n\", \n",
        "      np.dot(matrix1,vector2D) + matrix2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXYkzkgrrIJN"
      },
      "source": [
        "print(\"Adding a (3,) vector to a (3 x 1) vector\\n\",\n",
        "      \"broadcasts the 1D array across the second dimension\\n\",\n",
        "      \"Not what we want here!\\n\",\n",
        "      np.dot(matrix1,vector1D) + matrix2\n",
        "     )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-MggOQbrVhf"
      },
      "source": [
        "- **3단계**: 샘플링:\n",
        "  - 이제 $ y ^ {\\langle t + 1 \\rangle} $가 있으므로 공룡 이름의 다음 문자를 선택하려고 합니다. 가장 가능성이 높은 것을 선택하면 모델은 항상 시작 문자가 주어지면 동일한 결과를 생성합니다. 결과를 더 흥미롭게 만들기 위해 np.random.choice를 사용하여 *가능성 이 높지만* 항상 같지는 않은 다음 문자를 선택합니다.\n",
        "  - $ \\hat {y} ^ {\\langle t + 1 \\rangle} $로 지정된 확률 분포에 따라 다음 캐릭터의 **인덱스**를 선택합니다.\n",
        "  - 이는 $ \\hat {y} ^ {\\langle t + 1 \\rangle} _i = 0.16 $이면 16 % 확률로 인덱스 \"i\"를 선택한다는 의미입니다.\n",
        "  - [np.random.choice] (https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.random.choice.html)를 사용합니다.\n",
        "  `np.random.choice()` 사용 방법의 예 :\n",
        "  ```python\n",
        "    np.random.seed (0)\n",
        "    probs = np.array ([0.1, 0.0, 0.7, 0.2])\n",
        "    idx = np.random.choice (range (len ((probs)), p = probs)\n",
        "  ```\n",
        "  - 이는 분포에 따라 index (`idx`)을 선택한다는 의미입니다.\n",
        "  $P(index = 0) = 0.1, P(index = 1) = 0.0, P(index = 2) = 0.7, P(index = 3) = 0.2$.\n",
        "  - 'p'로 설정된 값은 1D 벡터로 설정되어야합니다.\n",
        "  - 또한 코드에서`y` 인 $ \\ hat {y} ^ {\\langle t + 1 \\rangle} $는 2D 배열입니다.\n",
        "  - 또한 구현에서 `np.random.choice`에 대한 첫 번째 인자는 정렬 된 목록 [0,1, .., vocab_len-1] 이지만 `char_to_ix.values​​()`를 사용하는 것은 *적절하지 않습니다*. 파이썬 딕셔너리의 `values​​()` 함수 호출에 의해 반환 된 값의 *순서*는 딕셔너리에 추가되는 순서와 동일한 순서입니다. 채점자는 루틴을 실행할 때 노트북에서 실행할 때와 다른 순서를 가질 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpXoHnDhufWV"
      },
      "source": [
        "##### 추가 힌트\n",
        "- [range](https://docs.python.org/3/library/functions.html#func-range)\n",
        "- [numpy.ravel](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ravel.html)은 다차원 배열을 가져 와서 1D 벡터 내부에 그 내용을 반환합니다.\n",
        "```python\n",
        "arr = np.array ([[1,2], [3,4]])\n",
        "print ( \"arr\")\n",
        "print (arr)\n",
        "print ( \"arr.ravel ()\")\n",
        "print (arr.ravel ())\n",
        "```\n",
        "Output:\n",
        "```python\n",
        "arr\n",
        "[[1 2]\n",
        " [3 4]]\n",
        "arr.ravel ()\n",
        "[1 2 3 4]\n",
        "```\n",
        "\n",
        "- `append`는 \"in-place\"작업입니다. 즉, 다음과 같이 코드를 작성하지 마십시오.\n",
        "```python\n",
        "fun_hobbies = fun_hobbies.append ( 'learning') ## Doesn't give you what you want\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2LeIGoOu6Qm"
      },
      "source": [
        "\n",
        "- **4단계** : $ x ^ {\\langle t \\rangle} $로 업데이트\n",
        "  - `sample()`에서 구현하는 마지막 단계는 현재 $ x ^ {\\langle t \\rangle} $을 저장하고 있는 변수 `x`를 $ x ^ {\\langle t + 1 \\rangle}$ 값으로 업데이트하는 것입니다.\n",
        "  - 예측 결과로 선택한 캐릭터에 상응하는 one-hot 벡터를 생성하여 $ x ^ {\\langle t + 1 \\rangle} $을 나타냅니다.\n",
        "  - 그런 다음 1단계에서 $ x ^ {\\langle t + 1 \\rangle}$ 을 전달하고 생성한 공룡 이름의 끝에 도달했음을 나타내는 \"\\n\"문자를 얻을 때까지 프로세스를 계속 반복합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8FIgOWBvTVJ"
      },
      "source": [
        "##### 추가 힌트\n",
        "- 새로운 one-hot 벡터로 설정하기 전에 `x` 를 재설정하려면 모든 값을 0으로 설정해야합니다.\n",
        "  - 새 numpy 배열을 만들 수 있습니다. [numpy.zeros](https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html)\n",
        "  - 또는 하나의 숫자로 모든 값 채우기 : [numpy.ndarray.fill](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.fill.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s4WkcoUrura"
      },
      "source": [
        "# GRADED FUNCTION: sample\n",
        "\n",
        "def sample(parameters, char_to_ix, seed):\n",
        "    \"\"\"\n",
        "    Sample a sequence of characters according to a sequence of probability distributions output of the RNN\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b. \n",
        "    char_to_ix -- python dictionary mapping each character to an index.\n",
        "    seed -- used for grading purposes. Do not worry about it.\n",
        "\n",
        "    Returns:\n",
        "    indices -- a list of length n containing the indices of the sampled characters.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve parameters and relevant shapes from \"parameters\" dictionary\n",
        "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
        "    vocab_size = by.shape[0]\n",
        "    n_a = Waa.shape[1]\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Step 1: Create the a zero vector x that can be used as the one-hot vector \n",
        "    # representing the first character (initializing the sequence generation). (≈1 line)\n",
        "    x = None\n",
        "    # Step 1': Initialize a_prev as zeros (≈1 line)\n",
        "    a_prev = None\n",
        "    \n",
        "    # Create an empty list of indices, this is the list which will contain the list of indices of the characters to generate (≈1 line)\n",
        "    indices = []\n",
        "    \n",
        "    # idx is the index of the one-hot vector x that is set to 1\n",
        "    # All other positions in x are zero.\n",
        "    # We will initialize idx to -1\n",
        "    idx = -1 \n",
        "    \n",
        "    # Loop over time-steps t. At each time-step:\n",
        "    # sample a character from a probability distribution \n",
        "    # and append its index (`idx`) to the list \"indices\". \n",
        "    # We'll stop if we reach 50 characters \n",
        "    # (which should be very unlikely with a well trained model).\n",
        "    # Setting the maximum number of characters helps with debugging and prevents infinite loops. \n",
        "    counter = 0\n",
        "    newline_character = char_to_ix['\\n']\n",
        "    \n",
        "    while (idx != newline_character and counter != 50):\n",
        "        \n",
        "        # Step 2: Forward propagate x using the equations (1), (2) and (3)\n",
        "        a = None\n",
        "        z = None\n",
        "        y = None\n",
        "        \n",
        "        # for grading purposes\n",
        "        np.random.seed(counter+seed) \n",
        "        \n",
        "        # Step 3: Sample the index of a character within the vocabulary from the probability distribution y\n",
        "        # (see additional hints above)\n",
        "        idx = None\n",
        "\n",
        "        # Append the index to \"indices\"\n",
        "        None\n",
        "        \n",
        "        # Step 4: Overwrite the input x with one that corresponds to the sampled index `idx`.\n",
        "        # (see additional hints above)\n",
        "        x = None\n",
        "        x[None] = None\n",
        "        \n",
        "        # Update \"a_prev\" to be \"a\"\n",
        "        a_prev = None\n",
        "        \n",
        "        # for grading purposes\n",
        "        seed += 1\n",
        "        counter +=1\n",
        "        \n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    if (counter == 50):\n",
        "        indices.append(char_to_ix['\\n'])\n",
        "    \n",
        "    return indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnmK_hb_vdsS"
      },
      "source": [
        "np.random.seed(2)\n",
        "_, n_a = 20, 100\n",
        "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
        "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
        "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
        "\n",
        "\n",
        "indices = sample(parameters, char_to_ix, 0)\n",
        "print(\"Sampling:\")\n",
        "print(\"list of sampled indices:\\n\", indices)\n",
        "print(\"list of sampled characters:\\n\", [ix_to_char[i] for i in indices])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHXUFf1Xvfre"
      },
      "source": [
        "**모범 답안:**\n",
        "\n",
        "```Python\n",
        "Sampling:\n",
        "list of sampled indices:\n",
        " [12, 17, 24, 14, 13, 9, 10, 22, 24, 6, 13, 11, 12, 6, 21, 15, 21, 14, 3, 2, 1, 21, 18, 24, 7, 25, 6, 25, 18, 10, 16, 2, 3, 8, 15, 12, 11, 7, 1, 12, 10, 2, 7, 7, 11, 17, 24, 12, 13, 24, 0]\n",
        "list of sampled characters:\n",
        " ['l', 'q', 'x', 'n', 'm', 'i', 'j', 'v', 'x', 'f', 'm', 'k', 'l', 'f', 'u', 'o', 'u', 'n', 'c', 'b', 'a', 'u', 'r', 'x', 'g', 'y', 'f', 'y', 'r', 'j', 'p', 'b', 'c', 'h', 'o', 'l', 'k', 'g', 'a', 'l', 'j', 'b', 'g', 'g', 'k', 'q', 'x', 'l', 'm', 'x', '\\n']\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwzgcQAivmRE"
      },
      "source": [
        "## 3 - Building the language model \n",
        "\n",
        "텍스트 생성을 위한 문자 수준 언어 모델을 구축해 봅니다.\n",
        "\n",
        "### 3.1 - Gradient descent \n",
        "\n",
        "* 이 섹션에서는 확률 적 경사 하강법(clipped gradient 포함)의 한 단계를 수행하는 함수를 구현합니다.\n",
        "* 학습 예제를 한 번에 하나씩 진행하므로 최적화 알고리즘은 확률 적 경사 하강 법이됩니다.\n",
        "\n",
        "다음은 RNN에 대한 일반적인 최적화 루프의 단계입니다.\n",
        "\n",
        "- 손실을 계산하기 위해 RNN을 통해 forward propagation\n",
        "- backward propagation을 수행하여 파라미터에 대한 손실의 기울기를 계산합니다.\n",
        "- Gradient clipping\n",
        "- 경사 하강 법을 사용하여 파라미터 업데이트\n",
        "\n",
        "\n",
        "**연습 문제** : 최적화 프로세스를 구현합니다 (확률 적 경사 하강 법의 한 단계).\n",
        "\n",
        "다음과 같은 함수가 제공됩니다.\n",
        "\n",
        "```python\n",
        "def rnn_forward(X, Y, a_prev, parameters):\n",
        "    \"\"\" Performs the forward propagation through the RNN and computes the cross-entropy loss.\n",
        "    It returns the loss' value as well as a \"cache\" storing values to be used in backpropagation.\"\"\"\n",
        "    ....\n",
        "    return loss, cache\n",
        "    \n",
        "def rnn_backward(X, Y, parameters, cache):\n",
        "    \"\"\" Performs the backward propagation through time to compute the gradients of the loss with respect\n",
        "    to the parameters. It returns also all the hidden states.\"\"\"\n",
        "    ...\n",
        "    return gradients, a\n",
        "\n",
        "def update_parameters(parameters, gradients, learning_rate):\n",
        "    \"\"\" Updates parameters using the Gradient Descent Update Rule.\"\"\"\n",
        "    ...\n",
        "    return parameters\n",
        "```\n",
        "\n",
        "또한 여러분이 사전에 `clip` 함수를 구현했다는 것을 기억하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhFtQOS8wTdt"
      },
      "source": [
        "#### Parameters\n",
        "\n",
        "* `parameters`가 `optimize` 함수의 반환 된 값 중 하나가 아니더라도`parameters` 사전 내부의 가중치 및 bias는 최적화에 의해 업데이트됩니다. `parameters` 사전은 참조를 통해 함수에 전달되므로이 사전을 변경하면 함수 외부에서 액세스하더라도 `parameters` 사전이 변경됩니다.\n",
        "* 파이썬 사전과 목록은 \"참조에 의한 전달\"입니다. 즉, 딕셔너리 변수를 함수에 전달하고 함수 내에서 사전을 수정하면 동일한 딕셔너리가 변경됩니다 (해당 딕셔너리의 복사본이 아님)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_DkhbIqvhFC"
      },
      "source": [
        "# GRADED FUNCTION: optimize\n",
        "\n",
        "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
        "    \"\"\"\n",
        "    Execute one step of the optimization to train the model.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.\n",
        "    Y -- list of integers, exactly the same as X but shifted one index to the left.\n",
        "    a_prev -- previous hidden state.\n",
        "    parameters -- python dictionary containing:\n",
        "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
        "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
        "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
        "                        b --  Bias, numpy array of shape (n_a, 1)\n",
        "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
        "    learning_rate -- learning rate for the model.\n",
        "    \n",
        "    Returns:\n",
        "    loss -- value of the loss function (cross-entropy)\n",
        "    gradients -- python dictionary containing:\n",
        "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
        "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
        "                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)\n",
        "                        db -- Gradients of bias vector, of shape (n_a, 1)\n",
        "                        dby -- Gradients of output bias vector, of shape (n_y, 1)\n",
        "    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    \n",
        "    # Forward propagate through time (≈1 line)\n",
        "    loss, cache = None\n",
        "    \n",
        "    # Backpropagate through time (≈1 line)\n",
        "    gradients, a = None\n",
        "    \n",
        "    # Clip your gradients between -5 (min) and 5 (max) (≈1 line)\n",
        "    gradients = None\n",
        "    \n",
        "    # Update parameters (≈1 line)\n",
        "    parameters = None\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return loss, gradients, a[len(X)-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D06B8ADhwlDo"
      },
      "source": [
        "np.random.seed(1)\n",
        "vocab_size, n_a = 27, 100\n",
        "a_prev = np.random.randn(n_a, 1)\n",
        "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
        "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
        "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
        "X = [12,3,5,11,22,3]\n",
        "Y = [4,14,11,22,25, 26]\n",
        "\n",
        "loss, gradients, a_last = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
        "print(\"Loss =\", loss)\n",
        "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
        "print(\"np.argmax(gradients[\\\"dWax\\\"]) =\", np.argmax(gradients[\"dWax\"]))\n",
        "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
        "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
        "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])\n",
        "print(\"a_last[4] =\", a_last[4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfqTNS6gwm9B"
      },
      "source": [
        "**모범 답안:**\n",
        "\n",
        "```Python\n",
        "Loss = 126.503975722\n",
        "gradients[\"dWaa\"][1][2] = 0.194709315347\n",
        "np.argmax(gradients[\"dWax\"]) = 93\n",
        "gradients[\"dWya\"][1][2] = -0.007773876032\n",
        "gradients[\"db\"][4] = [-0.06809825]\n",
        "gradients[\"dby\"][1] = [ 0.01538192]\n",
        "a_last[4] = [-1.]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJlVaRpcwq_l"
      },
      "source": [
        "### 3.2 - Training the model \n",
        "\n",
        "* 공룡 이름의 데이터 세트가 주어지면 데이터 세트의 각 line (이름 하나)을 하나의 훈련 데이터로 사용합니다.\n",
        "* 확률 적 경사 하강 법의 2000 단계마다 무작위로 선택한 여러 이름을 샘플링하여 알고리즘이 어떻게 작동하는지 확인합니다.\n",
        "\n",
        "\n",
        "**연습 문제** : 안내에 따라 `model()`을 구현하세요. `examples[index]`에 공룡 이름 (문자열)이 하나 포함 된 경우 예제 (X, Y)를 만들려면 다음을 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee7j_z-Cw51z"
      },
      "source": [
        "##### Set the index `idx` into the list of examples\n",
        "\n",
        "\n",
        "* 반복문을 사용하여 \"examples\" 리스트에서 공룡 이름을 섞은 목록을 살펴 봅니다.\n",
        "* 예를 들어, n_e 예제가 있고 for 루프가 인덱스를 n_e 이후로 증가시키는 경우, j가 n_e 일 때 모델에 예제를 계속 공급할 수 있도록 인덱스주기를 0으로 되 돌리는 방법을 생각해보십시오. , n_e + 1 등\n",
        "* 힌트 : n_e + 1을 n_e로 나눈 값은 0이고 나머지는 1입니다.\n",
        "* `%` 는 파이썬의 나머지 연산자입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZltqhiZw6Hw"
      },
      "source": [
        "##### Extract a single example from the list of examples\n",
        "\n",
        "* `single_example` : 이전에 설정한 `idx` 인덱스를 사용하여 예제 목록에서 한 단어를 가져옵니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNlDB3giw7BE"
      },
      "source": [
        "##### Convert a string into a list of characters: `single_example_chars`\n",
        "\n",
        "\n",
        "* `single_example_chars` : 문자열은 문자 목록입니다.\n",
        "* list conprehension(for 루프보다 권장 됨)을 사용하여 문자 목록을 생성 할 수 있습니다.\n",
        "```Python\n",
        "str = 'I love learning'\n",
        "list_of_chars = [c for c in str]\n",
        "print(list_of_chars)\n",
        "```\n",
        "\n",
        "```\n",
        "[ 'I', '', 'l', 'o', 'v', 'e', ​​'', 'l', 'e', ​​'a', 'r', 'n', 'i' , 'n', 'g']\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIihCnIOxf-_"
      },
      "source": [
        "##### Convert list of characters to a list of integers: `single_example_ix`\n",
        "\n",
        "* 각 문자와 관련된 인덱스가 포함 된 목록을 만듭니다.\n",
        "* 딕셔너리 `char_to_ix`를 사용합니다.\n",
        "* 문자열에서 문자 목록을 가져 오는 데 사용되는 list comprehension과 결합 할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBhz1NFExiff"
      },
      "source": [
        "##### Create the list of input characters: `X`\n",
        "\n",
        "* `rnn_forward`는 **`None`** 값을 플래그로 사용하여 입력 벡터를 제로 벡터로 설정합니다.\n",
        "* 입력 문자 목록 앞에 리스트 [**`None`**]을 추가합니다.\n",
        "* 리스트에 값을 추가하는 방법은 여러 가지가 있습니다. 한 가지 방법은 두 개의 목록을 함께 추가하는 것입니다. `[ 'a'] + [ 'b']`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnbyXdojxi-t"
      },
      "source": [
        "##### Get the integer representation of the newline character `ix_newline`\n",
        "\n",
        "* `ix_newline` : 개행 문자는 공룡 이름의 끝을 나타냅니다.\n",
        "  - 개행 문자 `'\\n'`의 정수 표현을 얻습니다.\n",
        "  - `char_to_ix` 사용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y8MEmVxxjRc"
      },
      "source": [
        "##### Set the list of labels (integer representation of the characters): `Y`\n",
        "\n",
        "* 목표는 공룡 이름의 다음 문자를 예측하도록 RNN을 훈련시키는 것이므로 레이블은 입력 'X'의 문자보다 한 단계 앞선 문자 목록입니다.\n",
        "  - 예를 들어`Y [0]`은`X [1]`과 동일한 값을 포함합니다.\n",
        "* RNN은 마지막 문자에서 줄 바꿈을 예측해야하므로 레이블 끝에 ix_newline을 추가합니다.\n",
        "  - 줄 바꿈 문자의 정수 표현을`Y` 끝에 추가합니다.\n",
        "  - '추가'는 내부 작업입니다.\n",
        "  - 두 개의 목록을 함께 추가하는 것이 더 쉬울 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZPRQxYwxiNe"
      },
      "source": [
        "# GRADED FUNCTION: model\n",
        "\n",
        "def model(data, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27, verbose = False):\n",
        "    \"\"\"\n",
        "    Trains the model and generates dinosaur names. \n",
        "    \n",
        "    Arguments:\n",
        "    data -- text corpus\n",
        "    ix_to_char -- dictionary that maps the index to a character\n",
        "    char_to_ix -- dictionary that maps a character to an index\n",
        "    num_iterations -- number of iterations to train the model for\n",
        "    n_a -- number of units of the RNN cell\n",
        "    dino_names -- number of dinosaur names you want to sample at each iteration. \n",
        "    vocab_size -- number of unique characters found in the text (size of the vocabulary)\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- learned parameters\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve n_x and n_y from vocab_size\n",
        "    n_x, n_y = vocab_size, vocab_size\n",
        "    \n",
        "    # Initialize parameters\n",
        "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
        "    \n",
        "    # Initialize loss (this is required because we want to smooth our loss)\n",
        "    loss = get_initial_loss(vocab_size, dino_names)\n",
        "    \n",
        "    # Build list of all dinosaur names (training examples).\n",
        "    with open(\"dinos.txt\") as f:\n",
        "        examples = f.readlines()\n",
        "    examples = [x.lower().strip() for x in examples]\n",
        "    \n",
        "    # Shuffle list of all dinosaur names\n",
        "    np.random.seed(0)\n",
        "    np.random.shuffle(examples)\n",
        "    \n",
        "    # Initialize the hidden state of your LSTM\n",
        "    a_prev = np.zeros((n_a, 1))\n",
        "    \n",
        "    # Optimization loop\n",
        "    for j in range(num_iterations):\n",
        "        \n",
        "        ### START CODE HERE ###\n",
        "        \n",
        "        # Set the index `idx` (see instructions above)\n",
        "        idx = None\n",
        "        \n",
        "        # Set the input X (see instructions above)\n",
        "        single_example = None\n",
        "        single_example_chars = None\n",
        "        single_example_ix = None\n",
        "        X = None\n",
        "        \n",
        "        # Set the labels Y (see instructions above)\n",
        "        ix_newline = None\n",
        "        Y = None\n",
        "\n",
        "        # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n",
        "        # Choose a learning rate of 0.01\n",
        "        curr_loss, gradients, a_prev = None\n",
        "        \n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # debug statements to aid in correctly forming X, Y\n",
        "        if verbose and j in [0, len(examples) -1, len(examples)]:\n",
        "            print(\"j = \" , j, \"idx = \", idx,) \n",
        "        if verbose and j in [0]:\n",
        "            print(\"single_example =\", single_example)\n",
        "            print(\"single_example_chars\", single_example_chars)\n",
        "            print(\"single_example_ix\", single_example_ix)\n",
        "            print(\" X = \", X, \"\\n\", \"Y =       \", Y, \"\\n\")\n",
        "        \n",
        "        # Use a latency trick to keep the loss smooth. It happens here to accelerate the training.\n",
        "        loss = smooth(loss, curr_loss)\n",
        "\n",
        "        # Every 2000 Iteration, generate \"n\" characters thanks to sample() to check if the model is learning properly\n",
        "        if j % 2000 == 0:\n",
        "            \n",
        "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
        "            \n",
        "            # The number of dinosaur names to print\n",
        "            seed = 0\n",
        "            for name in range(dino_names):\n",
        "                \n",
        "                # Sample indices and print them\n",
        "                sampled_indices = sample(parameters, char_to_ix, seed)\n",
        "                print_sample(sampled_indices, ix_to_char)\n",
        "                \n",
        "                seed += 1  # To get the same result (for grading purposes), increment the seed by one. \n",
        "      \n",
        "            print('\\n')\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PBR76hCxuAe"
      },
      "source": [
        "다음 셀을 실행하면 첫 번째 반복에서 모델이 임의의 문자를 출력하는 것을 관찰해야합니다. 수천 번의 반복 후에 모델은 합리적으로 보이는 이름을 생성하는 방법을 배워야합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGP3flvTwoGf"
      },
      "source": [
        "parameters = model(data, ix_to_char, char_to_ix, verbose = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtZZPcxjxwTZ"
      },
      "source": [
        "**모범 답안**\n",
        "\n",
        "```Python\n",
        "j =  0 idx =  0\n",
        "single_example = turiasaurus\n",
        "single_example_chars ['t', 'u', 'r', 'i', 'a', 's', 'a', 'u', 'r', 'u', 's']\n",
        "single_example_ix [20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19]\n",
        " X =  [None, 20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19] \n",
        " Y =        [20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19, 0] \n",
        "\n",
        "Iteration: 0, Loss: 23.087336\n",
        "\n",
        "Nkzxwtdmfqoeyhsqwasjkjvu\n",
        "Kneb\n",
        "Kzxwtdmfqoeyhsqwasjkjvu\n",
        "Neb\n",
        "Zxwtdmfqoeyhsqwasjkjvu\n",
        "Eb\n",
        "Xwtdmfqoeyhsqwasjkjvu\n",
        "\n",
        "\n",
        "j =  1535 idx =  1535\n",
        "j =  1536 idx =  0\n",
        "Iteration: 2000, Loss: 27.884160\n",
        "\n",
        "...\n",
        "\n",
        "Iteration: 34000, Loss: 22.447230\n",
        "\n",
        "Onyxipaledisons\n",
        "Kiabaeropa\n",
        "Lussiamang\n",
        "Pacaeptabalsaurus\n",
        "Xosalong\n",
        "Eiacoteg\n",
        "Troia\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPktveN2x6Ou"
      },
      "source": [
        "\n",
        "## 결론\n",
        "\n",
        "알고리즘이 훈련이 끝날 무렵 그럴듯한 공룡 이름을 생성하기 시작했음을 알 수 있습니다. 처음에는 임의의 캐릭터를 생성했지만 마지막에는 멋진 엔딩으로 공룡 이름을 볼 수있었습니다. 알고리즘을 더 오래 실행하고 하이퍼 파라미터를 사용하여 더 나은 결과를 얻을 수 있는지 확인하십시오. 우리의 구현은 `maconucon`, `marloralus` 및 `macingsersaurus`와 같은 정말 멋진 이름을 생성했습니다. 여러분의 모델은 공룡 이름이 'saurus', 'don', 'aura', 'tor'등으로 끝나는 경향이 있다는 것을 알게되기를 바랍니다.\n",
        "\n",
        "모델이 멋지지 않은 이름을 생성하는 경우 그것이 완전히 모델의 탓만은 아닙니다. 실제 공룡 이름이 모두 멋지게 들리는 것은 아닙니다. (예를 들어, `dromaeosauroides`는 실제 공룡 이름이며 훈련 세트에 있습니다.)하지만이 모델은 가장 멋진 것을 선택할 수있는 후보 세트를 제공해야합니다!\n",
        "\n",
        "이 할당은 비교적 작은 데이터 세트를 사용했기 때문에 CPU에서 RNN을 빠르게 훈련 할 수있었습니다. 영어 모델을 학습하려면 훨씬 더 큰 데이터 세트가 필요하며 일반적으로 훨씬 더 많은 계산이 필요하며 GPU에서 여러 시간 동안 실행할 수 있습니다. 우리는 공룡 이름을 꽤 오랫동안 사용했으며 지금까지 우리가 가장 좋아하는 이름은 위대하고 무패하며 치열한 망고 사우루스입니다!\n",
        "\n",
        "<img src=\"images/mangosaurus.jpeg\" style=\"width:250;height:300px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRbR6LokyOQf"
      },
      "source": [
        "## 4 - Writing like Shakespeare\n",
        "\n",
        "이 노트의 나머지 부분은 선택 사항이며 채점되지 않지만 재미 있고 유익한 내용이므로 어쨌든 해주시기를 바랍니다.\n",
        "\n",
        "비슷하지만 더 복잡한 작업은 셰익스피어의 시를 만드는 것입니다. 공룡 이름 데이터 세트에서 배우는 대신 셰익스피어의 시 모음을 사용할 수 있습니다. LSTM 셀을 사용하면 텍스트의 여러 문자에 걸친 장기 종속성을 학습 할 수 있습니다. 예를 들어, 시퀀스 어딘가에 나타나는 문자가 시퀀스에서 훨씬 나중에 다른 문자가되어야하는 것에 영향을 줄 수 있습니다. 이름이 매우 짧기 때문에 이러한 장기적 의존성은 공룡 이름에 덜 중요했습니다.\n",
        "\n",
        "<img src=\"arts/shakespeare.jpg\" style=\"width:500;height:400px;\">\n",
        "<center> Let's become poets!</center>\n",
        "\n",
        "\n",
        "Keras로 셰익스피어시 생성 모델을 구현했습니다. 다음 셀을 실행하여 필요한 패키지 및 모델을 로드하십시오. 이 작업은 몇 분 정도 걸릴 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AcxDRKmxz_D"
      },
      "source": [
        "from __future__ import print_function\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Model, load_model, Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Input, Masking\n",
        "from keras.layers import LSTM\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from shakespeare_utils import *\n",
        "import sys\n",
        "import io"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEWK3RTgyil0"
      },
      "source": [
        "시간을 절약하기 위해 우리는 이미 [* \"The Sonnets\"*] (shakespeare.txt)라는 셰익스피어시 모음에 대해 ~ 1000 epochs에 대한 모델을 훈련 시켰습니다.\n",
        "\n",
        "한 개의 epoch에 대해 추가로 모델을 훈련시켜 봅시다. one epoch에 대한 학습이 완료되려면 몇 분 정도 걸립니다. 입력을 요청하는 `generate_output`을 실행할 수 있습니다 (`<`40 자). 시는 문장으로 시작하고 RNN- 셰익스피어가 나머지시를 완성합니다! 예를 들어, \"Forsooth this maketh no sense\"(따옴표를 입력하지 마십시오)를 시도하십시오. 끝에 공백을 포함하는지 여부에 따라 결과가 다를 수도 있습니다. 두 가지 방법을 모두 시도하고 다른 입력도 시도해보세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtdYQ5tyysOC"
      },
      "source": [
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "model.fit(x, y, batch_size=128, epochs=1, callbacks=[print_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z56HeJOyvOP"
      },
      "source": [
        "# Run this cell to try with different inputs without having to re-train the model \n",
        "generate_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaRyBQXpyy0-"
      },
      "source": [
        "RNN-Shakespeare 모델은 공룡 이름으로 만든 모델과 매우 유사합니다. 유일한 주요 차이점은 다음과 같습니다.\n",
        "- 기본 RNN 대신 LSTM을 사용하여 장거리 종속성 캡처\n",
        "- 모델이 더 깊고 적층 된 LSTM 모델 (2 층)\n",
        "- Python 대신 Keras를 사용하여 코드 단순화\n",
        "\n",
        "자세한 내용은 [GitHub](https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py)에서 Keras Team의 텍스트 생성 구현을 확인할 수도 있습니다.\n",
        "\n",
        "이 과제를 끝마친 것을 축하합니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwPhl1KJy4zD"
      },
      "source": [
        "**References**:\n",
        "- This exercise took inspiration from Andrej Karpathy's implementation: https://gist.github.com/karpathy/d4dee566867f8291f086. To learn more about text generation, also check out Karpathy's [blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).\n",
        "- For the Shakespearian poem generator, our implementation was based on the implementation of an LSTM text generator by the Keras team: https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzAY8f18y8EP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}