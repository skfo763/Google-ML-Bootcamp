{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Operations_on_word_vectors_v2a",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skfo763/Google-ML-Bootcamp-phase1/blob/main/course5/week2/Operations_on_word_vectors_v2a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qEmOzB58wTL"
      },
      "source": [
        "# Operations on word vectors\n",
        "\n",
        "이번 주차 첫 번째 과제에 오신것을 환영합니다!\n",
        "\n",
        "Word embedding은 훈련하는데 상당히 많은 연산을 필요로 하기 때문에, 대부분의 ML 실무자들은 사전에 훈련된 embedding 세트를 가지고 작업합니다.\n",
        "\n",
        "**이번 과제를 통해 여러분은 다음 내용을 배우게 됩니다**\n",
        "- 사전에 훈련된 단어 벡터를 가지고 와서, 코사인 유사성 개념을 활용해 단어들 사이의 유사성을 측정합니다.\n",
        "- Word embedding을 사용해 Man - Woman, King-??? 과 같은 단어 유추 문제를 해결합니다.\n",
        "- Word embedding에서 성별에 관한 편향을 수정합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnzAco3t9ebv"
      },
      "source": [
        "이제 시작해봅시다. 아래 코드 블록을 실행시켜 필요한 패키지를 불러오세요,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeBiMovb9g8e"
      },
      "source": [
        "import numpy as np\n",
        "from w2v_utils import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNK_azmT9iSJ"
      },
      "source": [
        "#### Load the word vector\n",
        "\n",
        "- 이번 과제에서, 우리는 단어를 표현하기 위해 50 차원의 GloVe 벡터를 사용합니다.\n",
        "- `word_to_vec_map` 변수를 불러오기 위해 아래 코드 블록을 실행하세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGFMkUv29vqW"
      },
      "source": [
        "words, word_to_vec_map = read_glove_vecs('../../readonly/glove.6B.50d.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNcgqLsu9y2o"
      },
      "source": [
        "위에서 여러분이 불러온 데이터는,\n",
        "- `words` : 단어 목록이 들어 있는 집합입니다.\n",
        "- `word_to_vec_map` 각 단어와 GloVe 벡터를 맵핑하는 딕셔너리입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq3fwXJ49_oT"
      },
      "source": [
        "#### Embedding vectors versus ont-hot vectors\n",
        "- 지난 동영상 강의에서, one-hot 벡터 표현은 두 단어의 유사성을 계산하기에 적절하지 않다는 것을 설명했습니다. 이는 모든 one-hot 벡터는 다른 one-hot 벡터와 동일한 유클리드 거리를 가지기 때문입니다.\n",
        "- GloVe 벡터와 같은 Embedding 벡터 는 각 단어에 의미에 대해 보다 더 유용한 정보를 제공합니다.\n",
        "- 이제 GloVe 벡터를 통해 두 단어 사이의 유사성을 어떻게 측정하는지 알아봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-7SyOOR-dGc"
      },
      "source": [
        "## 1 - Cosine similarity\n",
        "\n",
        "두 단어 사이의 유사성을 축정하기 위해서는, 두 단어에 대한 embedding 벡터 사이의 유사성을 측정하는 방법이 필요합니다. 두 벡터 $u$, $v$가 주어지면, 코사인 유사성은 다음과 같은 공식으로 정의할 수 있습니다.\n",
        "\n",
        "$$\\text{CosineSimilarity(u, v)} = \\frac {u \\cdot v} {||u||_2 ||v||_2} = cos(\\theta) \\tag{1}$$\n",
        "\n",
        "- $u \\cdot v$는 두 벡터의 내적입니다.\n",
        "- $||u||_2$는 벡터 $u$에 대한 정규화를 의미합니다.\n",
        "- $\\theta$는 $u$와 $v$ 사잇각입니다.\n",
        "- 코사인 유사성은 $u$와 $v$ 사이의 각도에 따라 달라집니다.\n",
        "  - 만약 $u$와 $v$가 매우 유사하다면, 둘 사이의 코사인 유사성은 1에 가까워집니다.\n",
        "  - 만약 $u$와 $v$가 서로 다르다면, 코사인 유사성은 더 작은 값을 가집니다.\n",
        "\n",
        "<img src=\"arts/cosine_sim.png\" style=\"width:800px;height:250px;\">\n",
        "<center>그림 1 : 두 벡터 사이의 각에 대한 코사인을 통해 두 벡터 사이의 유사성을 측정하는 모습</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0wUngqI_ZUv"
      },
      "source": [
        "**연습 문제**: 두 단어 벡터 사이의 유사성을 측정하기 위한 `cosine_similarity()` 함수를 구현하세요.\n",
        "\n",
        "**Reminder**: $u$에 대한 정규화된 값 $||u||_2$를 계산하기 위한 공식은 다음과 같습니다.\n",
        "$\\sqrt{\\sum_{i=1}^{n} u_i^2}$\n",
        "\n",
        "\n",
        "#### 추가 힌트\n",
        "- `np.dot`, `np.sum` 또는 `np.sqrt` 함수를 사용할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khNUSrP39_UE"
      },
      "source": [
        "# GRADED FUNCTION: cosine_similarity\n",
        "\n",
        "def cosine_similarity(u, v):\n",
        "    \"\"\"\n",
        "    Cosine similarity reflects the degree of similarity between u and v\n",
        "        \n",
        "    Arguments:\n",
        "        u -- a word vector of shape (n,)          \n",
        "        v -- a word vector of shape (n,)\n",
        "\n",
        "    Returns:\n",
        "        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n",
        "    \"\"\"\n",
        "    \n",
        "    distance = 0.0\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Compute the dot product between u and v (≈1 line)\n",
        "    dot = None\n",
        "    # Compute the L2 norm of u (≈1 line)\n",
        "    norm_u = None\n",
        "    \n",
        "    # Compute the L2 norm of v (≈1 line)\n",
        "    norm_v = None\n",
        "    # Compute the cosine similarity defined by formula (1) (≈1 line)\n",
        "    cosine_similarity = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtYu3W8A_2Ki"
      },
      "source": [
        "father = word_to_vec_map[\"father\"]\n",
        "mother = word_to_vec_map[\"mother\"]\n",
        "ball = word_to_vec_map[\"ball\"]\n",
        "crocodile = word_to_vec_map[\"crocodile\"]\n",
        "france = word_to_vec_map[\"france\"]\n",
        "italy = word_to_vec_map[\"italy\"]\n",
        "paris = word_to_vec_map[\"paris\"]\n",
        "rome = word_to_vec_map[\"rome\"]\n",
        "\n",
        "print(\"cosine_similarity(father, mother) = \", cosine_similarity(father, mother))\n",
        "print(\"cosine_similarity(ball, crocodile) = \",cosine_similarity(ball, crocodile))\n",
        "print(\"cosine_similarity(france - paris, rome - italy) = \",cosine_similarity(france - paris, rome - italy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5SySUMP_2ls"
      },
      "source": [
        "**모범 답안**:\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            <b>cosine_similarity(father, mother)</b> =\n",
        "        </td>\n",
        "        <td>\n",
        "         0.890903844289\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            <b>cosine_similarity(ball, crocodile)</b> =\n",
        "        </td>\n",
        "        <td>\n",
        "         0.274392462614\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            <b>cosine_similarity(france - paris, rome - italy)</b> =\n",
        "        </td>\n",
        "        <td>\n",
        "         -0.675147930817\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkCCFPS2AAG-"
      },
      "source": [
        "#### Try different words!\n",
        "- 위의 모범 답안과 같은 값을 얻었다면, 자유롭게 입력값을 수정하고 다른 단어 쌍 간의 코사인 유사성을 측정해보세요.\n",
        "- 다른 입력의 코사인 유사성을 가지고 위 코드를 여러번 수행하면, 단어 벡터가 어떻게 작동하는지 더 잘 이해할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkfa2XWOAhBG"
      },
      "source": [
        "## 2 - Word analogy task\n",
        "\n",
        "- 단어 유추 작업에서, 우리는 다음과 같은 문장을 완성해야 합니다.\n",
        "  \n",
        "  <font color='brown'>\"*a* is to *b* as *c* is to **____**\"</font>.\n",
        "\n",
        "- 예를 들어, 아래 문장과 같습니다.\n",
        "\n",
        "  <font color='brown'> '*man* is to *woman* as *king* is to *queen*' </font>. \n",
        "\n",
        "- 우리는 단어 단어 벡터 $e_a, e_b, e_c, e_d$에 대해 $e_b - e_a \\approx e_d - e_c$를 만족하는 단어 *d*를 찾아야 합니다.\n",
        "- $e_b - e_a$와 $e_d - e_c$ 사이의 유사성을 측정하기 위해서, 앞서 구현했던 코사인 유사성 개념을 사용합니다.\n",
        "\n",
        "\n",
        "**연습 문제** : 단어 유추 작업을 수행하는 아래 코드를 완성하세요.\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhU39Ww3_-nm"
      },
      "source": [
        "# GRADED FUNCTION: complete_analogy\n",
        "\n",
        "def complete_analogy(word_a, word_b, word_c, word_to_vec_map):\n",
        "    \"\"\"\n",
        "    Performs the word analogy task as explained above: a is to b as c is to ____. \n",
        "    \n",
        "    Arguments:\n",
        "    word_a -- a word, string\n",
        "    word_b -- a word, string\n",
        "    word_c -- a word, string\n",
        "    word_to_vec_map -- dictionary that maps words to their corresponding vectors. \n",
        "    \n",
        "    Returns:\n",
        "    best_word --  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity\n",
        "    \"\"\"\n",
        "    \n",
        "    # convert words to lowercase\n",
        "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Get the word embeddings e_a, e_b and e_c (≈1-3 lines)\n",
        "    e_a, e_b, e_c = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    words = word_to_vec_map.keys()\n",
        "    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n",
        "    best_word = None                   # Initialize best_word with None, it will help keep track of the word to output\n",
        "\n",
        "    # to avoid best_word being one of the input words, skip the input words\n",
        "    # place the input words in a set for faster searching than a list\n",
        "    # We will re-use this set of input words inside the for-loop\n",
        "    input_words_set = set([word_a, word_b, word_c])\n",
        "    \n",
        "    # loop over the whole word vector set\n",
        "    for w in words:        \n",
        "        # to avoid best_word being one of the input words, skip the input words\n",
        "        if w in input_words_set:\n",
        "            continue\n",
        "        \n",
        "        ### START CODE HERE ###\n",
        "        # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  (≈1 line)\n",
        "        cosine_sim = None\n",
        "        \n",
        "        # If the cosine_sim is more than the max_cosine_sim seen so far,\n",
        "            # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (≈3 lines)\n",
        "        if None > None:\n",
        "            max_cosine_sim = None\n",
        "            best_word = None\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "    return best_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9KQ3sgBBeva"
      },
      "source": [
        "아래 코드 블록를 실행시켜 여러분의 위 코드를 테스트해보세요. 1-2분정도 걸릴 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnJMEqFhBmRc"
      },
      "source": [
        "triads_to_try = [('italy', 'italian', 'spain'), ('india', 'delhi', 'japan'), ('man', 'woman', 'boy'), ('small', 'smaller', 'large')]\n",
        "for triad in triads_to_try:\n",
        "    print ('{} -> {} :: {} -> {}'.format( *triad, complete_analogy(*triad,word_to_vec_map)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqLyQboWBpOq"
      },
      "source": [
        "**모범 답안**:\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            <b>italy -> italian</b> ::\n",
        "        </td>\n",
        "        <td>\n",
        "         spain -> spanish\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            <b>india -> delhi</b> ::\n",
        "        </td>\n",
        "        <td>\n",
        "         japan -> tokyo\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            <b>man -> woman</b> ::\n",
        "        </td>\n",
        "        <td>\n",
        "         boy -> girl\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            <b>small -> smaller</b> ::\n",
        "        </td>\n",
        "        <td>\n",
        "         large -> larger\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8Jc1AJWB0Fx"
      },
      "source": [
        "* 모범 답안과 같은 결과를 얻었다면, 위의 입력 셀을 자유롭게 수정하여 유추 알고리즘을 테스트하십시오.\n",
        "* 작동하는 다른 비유 쌍을 찾으십시오. 그러나 알고리즘이 정답을 제공하지 않는 부분도 찾으십시오.\n",
        "  * 예를 들어 `small-> small as big-> ?.`과 같은 문제가 그렇습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfqneOP_C4qE"
      },
      "source": [
        "### Congratulations!\n",
        "\n",
        "이번 과제의 채점되는 영역이 끝났습니다. 기억해야 할 주요 사항은 다음과 같습니다.\n",
        "\n",
        "- 코사인 유사성은 단어 벡터 쌍 간의 유사성을 비교하는 좋은 방법입니다.\n",
        "  - 물론 L2(Euclidean) 거리로도 작동합니다.\n",
        "- NLP 애플리케이션의 경우 사전 훈련 된 단어 벡터 세트를 사용하는 것이 시작하기에 좋은 방법입니다.\n",
        "- 채점되는 부분을 완료했어도, 이 과제의 나머지 부분을 살펴보고 단어 벡터의 편향성 제거에 대해 알아 보는 것이 좋습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDPjrIMfDMVH"
      },
      "source": [
        "## 3 - Debiasing word vector (OPTIONAL/UNGRADED)\n",
        "\n",
        "\n",
        "다음 연습에서는 word embedding에 반영 될 수있는 성별 편향을 조사하고 편향을 줄이기위한 알고리즘을 탐색합니다. 편향성에 대한 주제에 대해 배우는 것 외에도 이 연습은 단어 벡터가하는 일에 대한 직관을 공부하는데도 도움이 됩니다. 이 섹션에는 약간의 선형 대수가 포함되지만 선형 대수에 대한 전문가가 아니어도 완료 할 수 있으므로 한 번 시도해 보는 것이 좋습니다. 이번 과제의 해다되는 부분은 선택 사항이며 채점되지 않습니다.\n",
        "\n",
        "\n",
        "먼저 GloVe word embedding이 성별과 어떤 관련이 있는지 살펴 보겠습니다. 먼저 $ g = e_{woman} - e_{man} $ 벡터를 계산합니다. 여기서 $ e_{woman} $는 단어 *woman*에 해당하는 단어 벡터를 나타내고 $ e_ {man} $는 단어에 해당합니다. *man* 이라는 단어에 해당하는 벡터입니다. 결과 벡터 $g$는 \"성별\" 개념을 대략적으로 인코딩합니다. ($ g_1 = e_{mother} - e_{father} $, $ g_2 = e_{girl} - e_{boy} $ 등을 계산하고 그에 대한 평균을 계산하면 더 정확한 표현을 얻을 수 있습니다.하지만  $ e_ {woman} -e_ {man} $만을 사용해도 당분간 충분한 결과를 제공합니다.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PF26KtdTD4Np"
      },
      "source": [
        "g = word_to_vec_map['woman'] - word_to_vec_map['man']\n",
        "print(g)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMMbjLMKD7ri"
      },
      "source": [
        "이제 $g$를 사용하는 다른 단어의 코사인 유사성을 고려할 것입니다. 유사성의 양의 값과 음의 코사인 유사성이 무엇을 의미하는지 고려하십시오."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcXg6aSrD-UG"
      },
      "source": [
        "print ('List of names and their similarities with constructed vector:')\n",
        "\n",
        "# girls and boys name\n",
        "name_list = ['john', 'marie', 'sophie', 'ronaldo', 'priya', 'rahul', 'danielle', 'reza', 'katy', 'yasmin']\n",
        "\n",
        "for w in name_list:\n",
        "    print (w, cosine_similarity(word_to_vec_map[w], g))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXYeu3FjEEao"
      },
      "source": [
        "보시다시피, 여성 이름은 우리가 생성 한 벡터 $ g $와 양의 코사인 유사성을 갖는 경향이있는 반면, 남성 이름은 음의 코사인 유사성을 갖는 경향이 있습니다. 이것은 놀라운 일이 아니며 충분히 그 결과를 받아 들일 수 있습니다.\n",
        "\n",
        "그러나 다른 단어로 시도해 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2QH0IMLEJ5g"
      },
      "source": [
        "print('Other words and their similarities:')\n",
        "word_list = ['lipstick', 'guns', 'science', 'arts', 'literature', 'warrior','doctor', 'tree', 'receptionist', \n",
        "             'technology',  'fashion', 'teacher', 'engineer', 'pilot', 'computer', 'singer']\n",
        "for w in word_list:\n",
        "    print (w, cosine_similarity(word_to_vec_map[w], g))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvCnF7pkEMUL"
      },
      "source": [
        "위 코드 블록을 실행시켰더니, 예상치 못한 결과를 확인할 수 있습니다. 이러한 결과는 올바르지 않은 특정 성별에 대한 고정관념을 반영하고 있습니다. 예를 들어 \"컴퓨터\"라는 단어는 남자에 더 가깝고, \"문학\"이라는 단어는 여자에 더 가깝습니다.\n",
        "\n",
        "\n",
        "[Boliukbasi et al., 2016](https://arxiv.org/abs/1607.06520)의 알고리즘을 사용하여, 이러한 벡터의 편향을 줄이는 방법을 아래에서 살펴보겠습니다. \"actor/actress\" 또는 \"grandmother/grandfather\"과 같이 본질적으로 성별의 의미를 내포하고 있는 단어는 유지되어야 하며, \"receptionist\" 혹은 \"technology\" 와 같은 단어는 성별과 관련이 없기에 중립화되어야 합니다. debiasing 단계에서 이 두 가지 유형의 단어들을 서로 다르게 취급해야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWDVTrPlE54e"
      },
      "source": [
        "### 3.1 - Neturialize bias for non-gender specfic words\n",
        "\n",
        "\n",
        "아래 그림은 중립화 작업을 시각화하는 데 도움이됩니다. 50 차원 단어 임베딩을 사용하는 경우 50 차원 공간은 편향을 담고 있는 방향 $g$와 나머지 49 차원의 두 부분으로 나눌 수 있습니다. $ g _ {\\perp} $라고합니다. 선형 대수학에서 우리는 49 차원 $ g _ {\\perp} $가 $ g $에 수직 (또는 \"직교\")이라고 말하며, 이는 $ g $에 90도에 있음을 의미합니다. 중립화 단계는 $ e_{receptionist} $와 같은 벡터를 취하고 $g$ 방향으로 성분을 0으로 만들어 $ e_{receptionist}^{debiased} $를 제공합니다.\n",
        "\n",
        "$ g _ {\\perp} $는 49 차원이지만 2D 화면에 그릴 수있는 한계를 고려할 때 아래의 1 차원 축을 사용하여 설명합니다.\n",
        "\n",
        "<img src=\"arts/neutral.png\" style=\"width:800px;height:300px;\">\n",
        "<center>그림 2 : 중립화 작업을 적용하기 전후에 표현 된 \"receptionist\"에 대한 단어 벡터.</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHjD1wtUFiXM"
      },
      "source": [
        "**연습 문제** : \"receptionist\"와 \"scientist\"와 같은 단어들의 편향을 제거하는 `neturialize()` 함수를 구현하세요. 주어진 embedding $e$에 대해서, 아래 공식을 사용해 $e^{debiased}$를 계산할 수 있습니다.\n",
        "\n",
        "$$e^{bias\\_component} = \\frac{e \\cdot g}{||g||_2^2} * g\\tag{2}$$\n",
        "$$e^{debiased} = e - e^{bias\\_component}\\tag{3}$$\n",
        "\n",
        "만약 여러분이 이미 선형 대수학에 익숙하다면, $ e^{bias\\_component}$를 $g$ direction에 대한 $e$ 투영으로 생각할 수 있습니다. 다만 선형 대수학에 익숙하지 않더라도 위 개념을 잘 모른다고 크게 걱정하지 않아도 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCz3beC8EgOi"
      },
      "source": [
        "def neutralize(word, g, word_to_vec_map):\n",
        "    \"\"\"\n",
        "    Removes the bias of \"word\" by projecting it on the space orthogonal to the bias axis. \n",
        "    This function ensures that gender neutral words are zero in the gender subspace.\n",
        "    \n",
        "    Arguments:\n",
        "        word -- string indicating the word to debias\n",
        "        g -- numpy-array of shape (50,), corresponding to the bias axis (such as gender)\n",
        "        word_to_vec_map -- dictionary mapping words to their corresponding vectors.\n",
        "    \n",
        "    Returns:\n",
        "        e_debiased -- neutralized word vector representation of the input \"word\"\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Select word vector representation of \"word\". Use word_to_vec_map. (≈ 1 line)\n",
        "    e = None\n",
        "    \n",
        "    # Compute e_biascomponent using the formula given above. (≈ 1 line)\n",
        "    e_biascomponent = None\n",
        " \n",
        "    # Neutralize e by subtracting e_biascomponent from it \n",
        "    # e_debiased should be equal to its orthogonal projection. (≈ 1 line)\n",
        "    e_debiased = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return e_debiased"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7C8YwBvGPzK"
      },
      "source": [
        "e = \"receptionist\"\n",
        "print(\"cosine similarity between \" + e + \" and g, before neutralizing: \", cosine_similarity(word_to_vec_map[\"receptionist\"], g))\n",
        "\n",
        "e_debiased = neutralize(\"receptionist\", g, word_to_vec_map)\n",
        "print(\"cosine similarity between \" + e + \" and g, after neutralizing: \", cosine_similarity(e_debiased, g))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUhGPNTCGSgW"
      },
      "source": [
        "**모범 답안**: 두 번째 결과는 본질적으로 0이며 숫자 반올림 ($ 10 ^ {-17} $ 정도)까지입니다.\n",
        "\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            <b>cosine similarity between receptionist and g, before neutralizing:</b> :\n",
        "        </td>\n",
        "        <td>\n",
        "         0.330779417506\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            <b>cosine similarity between receptionist and g, after neutralizing:</b> :\n",
        "        </td>\n",
        "        <td>\n",
        "         -3.26732746085e-17\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUyuU4tbGce8"
      },
      "source": [
        "### 3.2 - Equalization algorithm for gender-specific words\n",
        "\n",
        "\n",
        "다음으로 \"actor\"및 \"actress\"와 같은 단어 쌍에도 편향성 제거를 적용하는 방법을 살펴 보겠습니다. Equalization은 성별 속성을 통해서만 다를 수있는 단어 쌍에 적용됩니다. 구체적인 예로, \"actor\"가 \"actress\"보다 \"babysit\"에 더 가깝다고 가정합니다. \"babysit\"에 중립화를 적용하면 babysitting과 관련된 성별-고정관념을 줄일 수 있습니다. 그러나 이것이 \"actor\"와 \"actress\"가 \"babysit\"와 같은 거리에 있다는 것을 보장하지는 않습니다. Equalizition 알고리즘이 이를 문제를 해결합니다.\n",
        "\n",
        "\n",
        "Equalization의 핵심 아이디어는 특정 단어 쌍이 49 차원 $ g_\\perp $에서 같은 거리에 있는지 확인하는 것입니다. Equalization 단계는 두 Equalization의 단계가 이제 $e_ {receptionist}^{debiased} $ 또는 중립화 된 다른 작업과 동일한 거리에 있는지 확인합니다. 그림에서 Equalization이 작동하는 방식은 다음과 같습니다.\n",
        "\n",
        "<img src=\"arts/equalize10.png\" style=\"width:800px;height:400px;\">\n",
        "\n",
        "이를 수행하기 위한 선형 대수의 미분 계산은 좀 더 복잡합니다. (자세한 내용은 Bolukbasi et al., 2016을 참조하십시오.) 그러나 주요 공식은 다음과 같습니다.\n",
        "\n",
        "$$ \\mu = \\frac{e_{w1} + e_{w2}}{2}\\tag{4}$$ \n",
        "\n",
        "$$ \\mu_{B} = \\frac {\\mu \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}\n",
        "\\tag{5}$$ \n",
        "\n",
        "$$\\mu_{\\perp} = \\mu - \\mu_{B} \\tag{6}$$\n",
        "\n",
        "$$ e_{w1B} = \\frac {e_{w1} \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}\n",
        "\\tag{7}$$ \n",
        "$$ e_{w2B} = \\frac {e_{w2} \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}\n",
        "\\tag{8}$$\n",
        "\n",
        "\n",
        "$$e_{w1B}^{corrected} = \\sqrt{ |{1 - ||\\mu_{\\perp} ||^2_2} |} * \\frac{e_{\\text{w1B}} - \\mu_B} {||(e_{w1} - \\mu_{\\perp}) - \\mu_B||} \\tag{9}$$\n",
        "\n",
        "\n",
        "$$e_{w2B}^{corrected} = \\sqrt{ |{1 - ||\\mu_{\\perp} ||^2_2} |} * \\frac{e_{\\text{w2B}} - \\mu_B} {||(e_{w2} - \\mu_{\\perp}) - \\mu_B||} \\tag{10}$$\n",
        "\n",
        "$$e_1 = e_{w1B}^{corrected} + \\mu_{\\perp} \\tag{11}$$\n",
        "$$e_2 = e_{w2B}^{corrected} + \\mu_{\\perp} \\tag{12}$$\n",
        "\n",
        "<br>\n",
        "\n",
        "**연습 문제**: 아래 기능을 구현하십시오. 위의 공식을 사용하여 단어 쌍의 최종 Equalization 버전을 얻으십시오. 행운을 빕니다!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM7mkM4iGWPw"
      },
      "source": [
        "def equalize(pair, bias_axis, word_to_vec_map):\n",
        "    \"\"\"\n",
        "    Debias gender specific words by following the equalize method described in the figure above.\n",
        "    \n",
        "    Arguments:\n",
        "    pair -- pair of strings of gender specific words to debias, e.g. (\"actress\", \"actor\") \n",
        "    bias_axis -- numpy-array of shape (50,), vector corresponding to the bias axis, e.g. gender\n",
        "    word_to_vec_map -- dictionary mapping words to their corresponding vectors\n",
        "    \n",
        "    Returns\n",
        "    e_1 -- word vector corresponding to the first word\n",
        "    e_2 -- word vector corresponding to the second word\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Step 1: Select word vector representation of \"word\". Use word_to_vec_map. (≈ 2 lines)\n",
        "    w1, w2 = None\n",
        "    e_w1, e_w2 = None\n",
        "    \n",
        "    # Step 2: Compute the mean of e_w1 and e_w2 (≈ 1 line)\n",
        "    mu = None\n",
        "\n",
        "    # Step 3: Compute the projections of mu over the bias axis and the orthogonal axis (≈ 2 lines)\n",
        "    mu_B = None\n",
        "    mu_orth = None\n",
        "\n",
        "    # Step 4: Use equations (7) and (8) to compute e_w1B and e_w2B (≈2 lines)\n",
        "    e_w1B = None\n",
        "    e_w2B = None\n",
        "        \n",
        "    # Step 5: Adjust the Bias part of e_w1B and e_w2B using the formulas (9) and (10) given above (≈2 lines)\n",
        "    corrected_e_w1B = None\n",
        "    corrected_e_w2B = None\n",
        "\n",
        "    # Step 6: Debias by equalizing e1 and e2 to the sum of their corrected projections (≈2 lines)\n",
        "    e1 = None\n",
        "    e2 = None\n",
        "                                                                \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return e1, e2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhoWBliQHaf-"
      },
      "source": [
        "print(\"cosine similarities before equalizing:\")\n",
        "print(\"cosine_similarity(word_to_vec_map[\\\"man\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"man\"], g))\n",
        "print(\"cosine_similarity(word_to_vec_map[\\\"woman\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"woman\"], g))\n",
        "print()\n",
        "e1, e2 = equalize((\"man\", \"woman\"), g, word_to_vec_map)\n",
        "print(\"cosine similarities after equalizing:\")\n",
        "print(\"cosine_similarity(e1, gender) = \", cosine_similarity(e1, g))\n",
        "print(\"cosine_similarity(e2, gender) = \", cosine_similarity(e2, g))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYTKFQ8AHb9G"
      },
      "source": [
        "**모범 답안**:\n",
        "\n",
        "cosine similarities before equalizing:\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            <b>cosine_similarity(word_to_vec_map[\"man\"], gender)</b> =\n",
        "        </td>\n",
        "        <td>\n",
        "         -0.117110957653\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            <b>cosine_similarity(word_to_vec_map[\"woman\"], gender)</b> =\n",
        "        </td>\n",
        "        <td>\n",
        "         0.356666188463\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>\n",
        "\n",
        "cosine similarities after equalizing:\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            <b>cosine_similarity(u1, gender)</b> =\n",
        "        </td>\n",
        "        <td>\n",
        "         -0.700436428931\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            <b>cosine_similarity(u2, gender)</b> =\n",
        "        </td>\n",
        "        <td>\n",
        "         0.700436428931\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zANZ7aVoHmVs"
      },
      "source": [
        "다른 단어 쌍에 위 `equalize` 함수를 적용하려면 위의 셀에 있는 입력 단어를 자유롭게 사용하십시오.\n",
        "\n",
        "이러한 편향성 제거 알고리즘은 편향을 줄이는 데 매우 유용하지만 완벽하지는 않으며 편향의 모든 흔적을 제거하지는 않습니다. 예를 들어 이 구현의 한가지 약점은, 편향 direction $g$가 _woman_ 및 _man_ 단어 쌍만 사용하여 정의되었다는 것입니다. 앞에서 설명한 것처럼, $g$가 $g_1 = e_{woman} - e_{man}$; $g_2 = e_{mother} - e_{father}$; $g_3 = e_{girl} - e_{boy}$의 값들을 평균낸 값으로 정의되었다면, 50차원의 word embedding 공간에서 \"성별\" 차원에 대한 더 나은 추정치를 얻을 수 있습니다. 이러한 종류의 바리에이션도 한번 구현해보세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn4EazeAIQCY"
      },
      "source": [
        "### Congratulations\n",
        "\n",
        "\n",
        "이 과제의 마지막까지 수행했으며, 단어 벡터를 사용하고 수정할 수있는 많은 방법을 보았습니다.\n",
        "\n",
        "이 과제를 마친 것을 축하합니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dlkml0FtIWod"
      },
      "source": [
        "**References**:\n",
        "- The debiasing algorithm is from Bolukbasi et al., 2016, [Man is to Computer Programmer as Woman is to\n",
        "Homemaker? Debiasing Word Embeddings](https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf)\n",
        "- The GloVe word embeddings were due to Jeffrey Pennington, Richard Socher, and Christopher D. Manning. (https://nlp.stanford.edu/projects/glove/)\n"
      ]
    }
  ]
}