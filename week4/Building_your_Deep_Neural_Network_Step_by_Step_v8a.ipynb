{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Building_your_Deep_Neural_Network_Step_by_Step_v8a",
      "provenance": [],
      "collapsed_sections": [
        "7yq8NYyzU3EA",
        "iMb6uQ-dbBT6",
        "skSmSRlbiUi7",
        "vr1h0eHJ49Zs",
        "W7evfywJlpO-"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skfo763/Google-ML-Bootcamp-phase1/blob/main/week4/Building_your_Deep_Neural_Network_Step_by_Step_v8a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjRdV4yrOwR7"
      },
      "source": [
        "# 단계별로 심층 신경망을 만들어봅시다! #\n",
        "\n",
        "4주차 첫 번째 과제에 오신것을 환영합니다! 이전 주차 과제에서 여러분들은 2개의 레이어, 특히 1개의 은닉층을 가지는 신경망을 만들어 보았습니다. 이번주에는, 레이어가 꽤 많은 심층 신경망을 만들어 볼 것입니다.\n",
        "\n",
        "- 이 주피터 노트북에서, 여러분들은 심층 신경망을 만들기 위해 필요한 여러가지 함수들을 구현할 것입니다.\n",
        "- 이후 4주차 두 번째 과제에서는, 이 함수들을 사용헤서 이미지 분류를 할 수 있는 심층 신경망을 만들 것입니다.\n",
        "\n",
        "**이 과제가 끝나고 나면:**\n",
        "- 비선형 ReLU 함수를 사용해 모델 성능을 향상시킬 수 있습니다.\n",
        "- 1개 이상의 은닉층을 사용하는 심층 신경망을 만들 수 있습니다.\n",
        "- 쉽게 재사용할 수 있는 신경망 클래스를 개발할 수 있습니다.\n",
        "\n",
        "**표기법(Notation):**\n",
        "- 위첨자 $[l]$ 은 해당 변수가 $l$번째 층과 연관된 변수라는 것을 의미합니다.\n",
        "  - $a^{[L]}$ 은 $L$ 번째 층의 활성화 값을 의미합니다.\n",
        "  - $W^{[L]}$ 과 $b^{[L]}$ 변수는 $L$ 번째 층의 파라미터를 의미합니다.\n",
        "- 위첨자 $(i)$ 는 해당 변수가 데이터 셋의 $i$ 번째 데이터와 연관이 있다는 것을 의미합니다.\n",
        "  - $x^{(i)}$ 는 $i$ 번째 훈련 데이터를 의미합니다\n",
        "- 아랫첨자 $i$ 는 벡터의 $i$ 번째 성분을 의미합니다.\n",
        "  -  $a^{[l]}_i$ 는 $l$ 번째 층의 활성화 값들 중 $i$ 번째 값을 의미합니다.).\n",
        "\n",
        "이제 시작해 봅시다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yq8NYyzU3EA"
      },
      "source": [
        "## 1. 패키지 다운로드하기\n",
        "\n",
        "늘 그랬듯이, 가장 먼저 이 과제를 수행하는 동안 필요한 다양한 패키지들을 다운로드해봅시다.\n",
        "\n",
        "- [numpy](www.numpy.org)는 파이썬의 가장 기본적인 과학/수학 연산 관련 패키지입니다.\n",
        "- [matplotlib](http://matplotlib.org)은 파이썬 환경에서 그래프를 그릴 수 있도록 해주는 유명한 라이브러리입니다.\n",
        "- `dnn_utils` 는 과제를 수행하기 위해 필요한 다양한 빌트인 함수들을 제공합니다.\n",
        "- `testCases` 는 과제에서 구현한 함수가 잘 작동하는지 테스트할 수 있는 예시들을 제공합니다.\n",
        "- `np.random.seed(1)` 은 랜덤 함수가 일관성을 유지하도록 시드를 설정해줍니다. 과제를 채점하기 위해 필요한 사항이므로, 강제로 시드를 변경하지 마세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfdo7YY4Vhk8"
      },
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "from testCases_v4a import *\n",
        "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "np.random.seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCTCDaC0Vtoy"
      },
      "source": [
        "## 2. 전반적인 과제의 개요 ##\n",
        "\n",
        "심층 신경망을 만들기 위해, 지금부터 몇가지 \"helper\" 함수들을 만들도록 하겠습니다. 이 helper 함수들은 다음 과제에서 2-layer와 L-layer인 심층 신경망을 만드는데 사용될 것입니다. 직접 helper 함수들을 구현할 때 각 단계를 안내하는 자세한 지시사항이 있습니다.\n",
        "\n",
        "결론적으로, 이 과제의 개요는 다음과 같습니다:\n",
        "- 2-layer 신경망과 $L$-layer 신경망의 파라미터를 초기화합니다.\n",
        "- `forward propagation`를 담당하는 모듈을 구현합니다(아래 그림에서 보라색에 해당합니다)\n",
        "  - forward propagation 단계 중 선형 부분 (($Z^{[l]}$ 을 계산하는 부분) 을 완성합니다.\n",
        "  - 활성화 함수 (ReLU, sigmoid)를 구현합니다.\n",
        "  - 두 단계를 합쳐 [선형 부분 -> 활성화 함수] 로 이어지는 forward 함수를 구현합니다.\n",
        "  - [LINEAR->RELU] `forward` 함수를 $L-1$ 만큼 쌓고, (1번째 층부터 L-1번째 층 까지) 이후 [LINEAR->SIGMOID] `forward` 함수를 마지막 층 $L$에 추가하여 L_model `forward` 함수를 완성합니다.\n",
        "- `loss`를 계산합니다.\n",
        "- `backward propagation` 을 구현합니다(아래 그림에서 빨간 색에 해당합니다)\n",
        "  - 오차 역전파 과정 중 선형 부분을 구현합니다.\n",
        "  - 활성화 함수의 `gradient`를 계산합니다. (`relu_backward` / `sigmoid_backward`)\n",
        "  - 두 단계를 합쳐 새 [LINEAR->ACTIVATION] `backward` 함수를 만듭니다.\n",
        "  - [LINEAR->RELU] `backward` 함수를 $L-1$ 번 만큼 쌓고, [LINEAR->SIGMOID] `backward` 함수를 끝에 추가하여 L_model `backward` 함수를 완성합니다.\n",
        "- 최종적으로 파라미터를 업데이트합니다.\n",
        "\n",
        "<img src=\"arts/final outline.png\" style=\"width:800px;height:500px;\">\n",
        "<caption><center>그림 1</center></caption></br>\n",
        "\n",
        "**매 forward 함수에 대해 상응하는 backward 함수가 있다는 것을 기억**하세요. 이 때문이 우리는 forward 모듈을 동작시킬 때마다 몇 가지 변수들을 캐시에 저장해야 합니다. 이렇게 저장된 캐시 변수는 이후 gradient를 계산할 때 사용될 수 있습니다. 다시 말해 backpropagation 모듈이 동작할 때 이 캐시 데이터를 활용해서 gradient를 계산합니다. 이 과제에선 일련의 단계를 어떻게 수행하는지 보여줄 예정입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMb6uQ-dbBT6"
      },
      "source": [
        "## 3. 초기화 ##\n",
        "\n",
        "이제 인공 신경망의 파라미터를 초기화하는 두 가지 helper 함수를 만들어봅시다. 첫 번째 함수는 2개의 층을 가지는 모델의 파라미터를 초기화하는데 사용될 것이고, 두 번째는 L개의 층을 가지는 모델을 위해 사용됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyfGmc4D5GzT"
      },
      "source": [
        "### 3-1. 2-layer 신경망 ###\n",
        "\n",
        "**연습 문제** : 2개 층을 가지는 인공 신경망의 파라미터를 초기화 해봅시다.\n",
        "\n",
        "**지시 사항** :\n",
        "- 모델의 구조는 선형 함수 -> ReLU -> 선형 함수 -> sigmoid 입니다.\n",
        "- 가중치 행렬을 랜덤하게 초기화하세요. 올바른 shape에 대해 `np.random.randn(shape) * 0.01` 코드를 사용해 보세요.\n",
        "- bias는 0으로 초기화합니다. `np.zeros(shape)` 를 사용해보세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkAA19VeV08g"
      },
      "source": [
        "# GRADED FUNCTION: initialize_parameters\n",
        "\n",
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    n_x -- size of the input layer\n",
        "    n_h -- size of the hidden layer\n",
        "    n_y -- size of the output layer\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your parameters:\n",
        "                    W1 -- weight matrix of shape (n_h, n_x)\n",
        "                    b1 -- bias vector of shape (n_h, 1)\n",
        "                    W2 -- weight matrix of shape (n_y, n_h)\n",
        "                    b2 -- bias vector of shape (n_y, 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    \n",
        "    ### START CODE HERE ### (≈ 4 lines of code)\n",
        "    W1 = None\n",
        "    b1 = None\n",
        "    W2 = None\n",
        "    b2 = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    assert(W1.shape == (n_h, n_x))\n",
        "    assert(b1.shape == (n_h, 1))\n",
        "    assert(W2.shape == (n_y, n_h))\n",
        "    assert(b2.shape == (n_y, 1))\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiWrVfSmceOH"
      },
      "source": [
        "parameters = initialize_parameters(3,2,1)\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXTs_mGccezL"
      },
      "source": [
        "**Expected output**:\n",
        "<table style=\"width:80%\">\n",
        "  <tr>\n",
        "    <td>W1</td>\n",
        "    <td> [[ 0.01624345 -0.00611756 -0.00528172]\n",
        " [-0.01072969  0.00865408 -0.02301539]] </td> \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> b1</td>\n",
        "    <td>[[ 0.]\n",
        " [ 0.]]</td> \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>W2</td>\n",
        "    <td> [[ 0.01744812 -0.00761207]]</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> b2 </td>\n",
        "    <td> [[ 0.]] </td> \n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAjq-tFJdFun"
      },
      "source": [
        "### 3-2. L-layer 신경망 ###\n",
        "\n",
        "L개의 층을 가지는 신경망은 많은 가중치 행렬과 bias 벡터를 초기화해야 하기 때문에 초기화 과정이 꽤 어렵습니다. 아래에서 `initialize_parameters_deep()` 함수를 구현하면서, 각 층의 차원이 정확한지 확인해야 합니다. $n^{[l]}$ 값이 각 층에 있는 유닛의 개수라는 것을 기억하세요. 따라서 만약 샘플 데이터인 입력 행렬 X가 $(12288, 209)$, (개수 $m=209$) 라고 한다면, 각각 층의 가중치는 다음과 같을 것입니다.\n",
        "\n",
        "</br>\n",
        "\n",
        "<table style=\"width:100%\">\n",
        "    <tr>\n",
        "        <td></td> \n",
        "        <td> Shape of W </td> \n",
        "        <td> Shape of b  </td> \n",
        "        <td> Activation </td>\n",
        "        <td> Shape of Activation </td> \n",
        "    <tr>\n",
        "    <tr>\n",
        "        <td> Layer 1 </td> \n",
        "        <td> $(n^{[1]},12288)$ </td> \n",
        "        <td> $(n^{[1]},1)$ </td> \n",
        "        <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td>    \n",
        "        <td> $(n^{[1]},209)$ </td> \n",
        "    <tr>\n",
        "    <tr>\n",
        "        <td> Layer 2 </td> \n",
        "        <td> $(n^{[2]}, n^{[1]})$  </td> \n",
        "        <td> $(n^{[2]},1)$ </td> \n",
        "        <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td> \n",
        "        <td> $(n^{[2]}, 209)$ </td> \n",
        "    <tr>\n",
        "       <tr>\n",
        "        <td> $\\vdots$ </td> \n",
        "        <td> $\\vdots$  </td> \n",
        "        <td> $\\vdots$  </td> \n",
        "        <td> $\\vdots$</td> \n",
        "        <td> $\\vdots$  </td> \n",
        "    <tr>\n",
        "   <tr>\n",
        "        <td> Layer L-1 </td> \n",
        "        <td> $(n^{[L-1]}, n^{[L-2]})$ </td> \n",
        "        <td> $(n^{[L-1]}, 1)$  </td> \n",
        "        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td> \n",
        "        <td> $(n^{[L-1]}, 209)$ </td> \n",
        "    <tr>\n",
        "   <tr>\n",
        "        <td> Layer L </td> \n",
        "        <td> $(n^{[L]}, n^{[L-1]})$ </td> \n",
        "        <td> $(n^{[L]}, 1)$ </td>\n",
        "        <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n",
        "        <td> $(n^{[L]}, 209)$  </td> \n",
        "    <tr>\n",
        "</table>\n",
        "\n",
        "</br>\n",
        "\n",
        "$WX + b$ 공식을 계산할 때 파이썬에서는 broadcasting 이 적용된다는 사실을 잊지 마세요.\n",
        "\n",
        "$$ W = \\begin{bmatrix}\n",
        "    j  & k  & l\\\\\n",
        "    m  & n & o \\\\\n",
        "    p  & q & r \n",
        "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
        "    a  & b  & c\\\\\n",
        "    d  & e & f \\\\\n",
        "    g  & h & i \n",
        "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
        "    s  \\\\\n",
        "    t  \\\\\n",
        "    u\n",
        "\\end{bmatrix}\\tag{2}$$\n",
        "\n",
        "위와 같은 상황에서 $WX + b$ 는 아래와 같이 계산될 것입니다.\n",
        "\n",
        "$$ WX + b = \\begin{bmatrix}\n",
        "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
        "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
        "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
        "\\end{bmatrix}\\tag{3}  $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZPjGFYlfXFX"
      },
      "source": [
        "**연습 문제**: L-layer 신경망을 초기화하는 함수를 구현하세요.\n",
        "\n",
        "**지시 사항**:\n",
        "- 모델 구조: `[선형 함수 -> ReLU]` L-1회 반복 -> 선형 함수 -> `sigmoid`\n",
        "- 가중치 행렬을 랜덤하게 초기화하세요. 올바른 shape에 대해 `np.random.randn(shape) * 0.01` 코드를 사용해 보세요.\n",
        "- bias는 0으로 초기화합니다. `np.zeros(shape)` 를 사용해보세요.\n",
        "- 각 층의 unit들의 개수 $n^{[l]}$ 가 `layer_dims` 변수에 저장됩니다. \n",
        "  - 예를 들어, 지난 주의 \"평면 데이터 분류 모델\" 에 대한 `layer_dims`는 [2, 4, 1] 이 됩니다. 해석하자면 입력 층에는 두 개의 unit, 첫 번째 은닉 층은 4개의 unit, 출력 유닛 1개로 구성되어 있다는 것을 의미합니다.\n",
        "  - 이는 W1의 `shape`는 (4,1), b1이 (4,1) W2가 (1,4), b2가 (1,1)임을 의미합니다. 이제 이 값들을 L개의 층을 가진 모델로 일반화합니다.\n",
        "- 다음은 $L = 1$(단일층 신경망) 에 대한 구현입니다. 이 예시는 L 개의 층을 가진 신경망을 구현하는데 도움을 줄 것입니다.\n",
        "  ```python\n",
        "      if L == 1:\n",
        "          parameters[\"W\" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01\n",
        "          parameters[\"b\" + str(L)] = np.zeros((layer_dims[1], 1))\n",
        "  ```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUHe3LaKh-QG"
      },
      "source": [
        "# GRADED FUNCTION: initialize_parameters_deep\n",
        "\n",
        "def initialize_parameters_deep(layer_dims):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
        "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(3)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        ### START CODE HERE ### (≈ 2 lines of code)\n",
        "        parameters['W' + str(l)] = None\n",
        "        parameters['b' + str(l)] = None\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2SGz5SYh8mq"
      },
      "source": [
        "parameters = initialize_parameters_deep([5,4,3])\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJN9aCWRiEc5"
      },
      "source": [
        "**Expected output**:\n",
        "       \n",
        "<table style=\"width:80%\">\n",
        "  <tr>\n",
        "    <td> W1 </td>\n",
        "    <td>[[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
        " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
        " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
        " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]</td> \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> b1 </td>\n",
        "    <td>[[ 0.]\n",
        " [ 0.]\n",
        " [ 0.]\n",
        " [ 0.]]</td> \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> W2 </td>\n",
        "    <td>[[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
        " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
        " [-0.00768836 -0.00230031  0.00745056  0.01976111]]</td> \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> b2 </td>\n",
        "    <td>[[ 0.]\n",
        " [ 0.]\n",
        " [ 0.]]</td> \n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skSmSRlbiUi7"
      },
      "source": [
        "## 4. Forward Propagation 모듈 구현하기 ##\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr1h0eHJ49Zs"
      },
      "source": [
        "### 4-1. Linear Forward 계산하기 ##\n",
        "\n",
        "이제 초기화된 파라미터를 가지고, forward propagation 모듈을 만들어봅시다. 모델을 구현하기 위한 기본적인 기능을 담당하는 함수를 만드는 것으로 시작해봅시다. 세 가지 함수를 아래와 같은 순서로 완성하면 됩니다.\n",
        "\n",
        "- 선형 함수 (LINEAR)\n",
        "- ReLU / sigmoid 의 활성화 함수 (LINEAR -> ACTIVATION)\n",
        "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID (전체 모델)\n",
        "\n",
        "모든 테스트 데이터에 대하여 벡터화된 선형 forward 모듈은 아래 공식을 따라 계산합니다.\n",
        "\n",
        "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
        "\n",
        "추가로, $A^{[0]} = X$ 입니다. \n",
        "\n",
        "**연습 문제**: forward propagation 함수의 선형 부분을 구현해보세요.\n",
        "\n",
        "**복습** : 이 부분의 수학적 표현은 $Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$ 입니다. 이전과 마찬가지로 `np.dot()` 함수를 쓰는 것이 유용할 것입니다. 만약 행렬의 차원을 맞추지 못하겠다면, `W.shape`를 사용해보세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWCGyNrgkwlX"
      },
      "source": [
        "# GRADED FUNCTION: linear_forward\n",
        "\n",
        "def linear_forward(A, W, b):\n",
        "    \"\"\"\n",
        "    Implement the linear part of a layer's forward propagation.\n",
        "\n",
        "    Arguments:\n",
        "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "\n",
        "    Returns:\n",
        "    Z -- the input of the activation function, also called pre-activation parameter \n",
        "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    Z = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "    \n",
        "    return Z, cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_w96126lnM3"
      },
      "source": [
        "A, W, b = linear_forward_test_case()\n",
        "\n",
        "Z, linear_cache = linear_forward(A, W, b)\n",
        "print(\"Z = \" + str(Z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhayH2Qslogr"
      },
      "source": [
        "**모범 답안**:\n",
        "\n",
        "<table style=\"width:35%\">\n",
        "  \n",
        "  <tr>\n",
        "    <td> Z </td>\n",
        "    <td> [[ 3.26295337 -1.23429987]] </td> \n",
        "  </tr>\n",
        "  \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7evfywJlpO-"
      },
      "source": [
        "### 4-2. 선형 활성화 함수 구현하기 ###\n",
        "\n",
        "이번 과제에서 여러분은 두 가지 활성화 함수를 사용할 수 있습니다.\n",
        "\n",
        "- **Sigmoid** :  $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$. 이번 과제에서는 사전에 구현된 `sigmoid()` 함수를 제공합니다. 이 함수는 **두 개의** 변수를 리턴합니다.\n",
        "  - 활성화 값 `A`\n",
        "  - 이 정방향 함수와 상응하는 역방향 함수에서 사용할 \"Z\" 값을 담고 있는 `cache` 변수\n",
        "  - sigmoid 함수는 아래 코드처럼 사용하세요.\n",
        "    ```python\n",
        "    A, activation_cache = sigmoid(Z)\n",
        "    ```\n",
        "\n",
        "- **ReLU**: ReLU 함수의 수학적인 공식은 $A = RELU(Z) = max(0, Z)$ 입니다. 이번 과제에서는 사전에 구현되어 있는 `relu()` 함수를 사용합니다. 이 함수 역시 **두 개의** 변수를 리턴합니다.\n",
        "  - 활성화 값 `A`\n",
        "  - 역방향 전파에 사용되는 Z값을 담은 `cache` 변수\n",
        "  - ReLU는 아래 코드처럼 사용하세요\n",
        "    ```python\n",
        "    A, activation_cache = relu(Z)\n",
        "    ```\n",
        "\n",
        "</br>\n",
        "\n",
        "편의를 위해 Linear와 Activation 두 가지 기능을 하나로 합칩니다. 따라서 Linear을 먼저 수행한 다음, Activation을 이어서 수행하는 함수를 구현합니다.\n",
        "\n",
        "**연습 문제** : Linear -> Activation 을 수행하는 forward propagation를 구현하세요. \n",
        "- 활성화 함수 `g`는 `sigmoid()` 또는 `relu()`가 될 수 있습니다. \n",
        "- 공식은 $A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$ 입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s31_qG_cm8nh"
      },
      "source": [
        "# GRADED FUNCTION: linear_activation_forward\n",
        "\n",
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    \"\"\"\n",
        "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
        "\n",
        "    Arguments:\n",
        "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "\n",
        "    Returns:\n",
        "    A -- the output of the activation function, also called the post-activation value \n",
        "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
        "             stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    \n",
        "    if activation == \"sigmoid\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        ### START CODE HERE ### (≈ 2 lines of code)\n",
        "        Z, linear_cache = None\n",
        "        A, activation_cache = None\n",
        "        ### END CODE HERE ###\n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        ### START CODE HERE ### (≈ 2 lines of code)\n",
        "        Z, linear_cache = None\n",
        "        A, activation_cache = None\n",
        "        ### END CODE HERE ###\n",
        "    \n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEqYW_Nzp1Lf"
      },
      "source": [
        "A_prev, W, b = linear_activation_forward_test_case()\n",
        "\n",
        "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
        "print(\"With sigmoid: A = \" + str(A))\n",
        "\n",
        "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
        "print(\"With ReLU: A = \" + str(A))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVViRcKQp15i"
      },
      "source": [
        "**Expected output**:\n",
        "<table style=\"width:35%\">\n",
        "  <tr>\n",
        "    <td> With sigmoid: A </td>\n",
        "    <td > [[ 0.96890023  0.11013289]]</td> \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> With ReLU: A </td>\n",
        "    <td > [[ 3.43896131  0.        ]]</td> \n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "</br>\n",
        "\n",
        "**메모** : 딥 러닝에서 \"[LINEAR -> ACTIVATION]\"의 일련의 계산은 하나의 층으로 간주합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze6xHuuRq7W7"
      },
      "source": [
        "#### d) L-layer 모델 ####\n",
        "\n",
        "L-layer 신경망을 구현할 때 더 많은 편의를 위해서 이전에 구현했던 `relu()` 함수를 활성화 함수로 사용하는 `linear_activation_foward()` 를 $L-1$ 회 복사한 함수가 필요합니다. 그리고 나서 `sigmoid()` 함수를 활성화 함수로 사용하는 `linear_activation_forward()` 함수를 추가합니다.\n",
        "\n",
        "<img src=\"arts/model_architecture_kiank.png\" style=\"width:600px;height:300px;\">\n",
        "<caption>Figure 2: [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID model</caption>\n",
        "\n",
        "</br>\n",
        "\n",
        "**연습 문제**: 위 모델의 전체 forward propagation을 구현하세요.\n",
        "\n",
        "**지시 사항**: 아래 코드에서 `AL`은 $A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$ 을 의미합니다. 이 `AL` 값은 `Yhat` 이라고도 불립니다 (i.e. $A^{[L]}$ = $\\hat{Y}$)\n",
        "\n",
        "**팁**:\n",
        "- 앞서 작성했던 함수들을 사용하세요.\n",
        "- [LINEAR -> RELU] 층을 L-1회 반복하기 위해서 반복문을 사용하세요.\n",
        "- 앞서 언급했던 캐싱 처리할 데이터들을 `caches` 변수에 저장하는 것을 잊지 마세요. `c` 변수를 `list`에 추가하기 위해서는 `list.append(c)`를 사용하면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ri76I0uuqGQ2"
      },
      "source": [
        "# GRADED FUNCTION: L_model_forward\n",
        "\n",
        "def L_model_forward(X, parameters):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (input size, number of examples)\n",
        "    parameters -- output of initialize_parameters_deep()\n",
        "    \n",
        "    Returns:\n",
        "    AL -- last post-activation value\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
        "    \"\"\"\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2                  # number of layers in the neural network\n",
        "    \n",
        "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        ### START CODE HERE ### (≈ 2 lines of code)\n",
        "        A, cache = None\n",
        "        None\n",
        "        ### END CODE HERE ###\n",
        "    \n",
        "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
        "    ### START CODE HERE ### (≈ 2 lines of code)\n",
        "    AL, cache = None\n",
        "    None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    assert(AL.shape == (1,X.shape[1]))\n",
        "            \n",
        "    return AL, caches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTkSBm8Qwp2V"
      },
      "source": [
        "X, parameters = L_model_forward_test_case_2hidden()\n",
        "AL, caches = L_model_forward(X, parameters)\n",
        "print(\"AL = \" + str(AL))\n",
        "print(\"Length of caches list = \" + str(len(caches)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NltRpZiwqix"
      },
      "source": [
        "**모범 답안**:\n",
        "<table style=\"width:50%\">\n",
        "  <tr>\n",
        "    <td> **AL** </td>\n",
        "    <td > [[ 0.03921668  0.70498921  0.19734387  0.04728177]]</td> \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> **Length of caches list ** </td>\n",
        "    <td > 3 </td> \n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "</br>\n",
        "\n",
        "축하드립니다! 이제 훈련 데이터X를 입력 값으로, 예측값을 담고 있는 $A^{[L]}$ 열 벡터를 출력으로 하는 전체 forward propagation 모듈을 구현했습니다. 이 모듈은 이후 역방향 전파에 사용되는 중간값인 `caches` 값을 기록합니다. $A^{[L]}$ 값을 활용해서 예측 결과의 비용(cost)을 계산할 수 있습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iIk82vRxndK"
      },
      "source": [
        "## 5. 비용 함수 계산하기 ##\n",
        "\n",
        "이제 순방향 propagation을 구현해봅시다. 먼저 모델이 실제로 학습 중인지 확인하기 위해 cost를 계산해야 합니다. \n",
        "\n",
        "**연습 문제**: 비용 $J$를 계산하기 위해서 크로스 엔트로피 함수를 계산하세요. 다음 공식을 활용하면 됩니다 : $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8psymwXpxlv-"
      },
      "source": [
        "# GRADED FUNCTION: compute_cost\n",
        "\n",
        "def compute_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function defined by equation (7).\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
        "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- cross-entropy cost\n",
        "    \"\"\"\n",
        "    \n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Compute loss from aL and y.\n",
        "    ### START CODE HERE ### (≈ 1 lines of code)\n",
        "    cost = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP9aT6iwyy1A"
      },
      "source": [
        "Y, AL = compute_cost_test_case()\n",
        "\n",
        "print(\"cost = \" + str(compute_cost(AL, Y)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-n65SjxuyzM8"
      },
      "source": [
        "**모범 답안**:\n",
        "<table>\n",
        "    <tr>\n",
        "    <td> cost </td>\n",
        "    <td> 0.2797765635793422</td> \n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AV1HVPHzy0Z"
      },
      "source": [
        "## 6. Backward propagation 모듈 구현하기 ##\n",
        "\n",
        "위 forward propagation과 마찬가지로 backward propagation을 위한 helper 함수들을 구현합시다. backward propagation은 각 파라미터에 대한 손실 함수의 gradient를 계산하기 위해 사용된다는 점을 기억하세요.\n",
        "\n",
        "**리마인더**:\n",
        "\n",
        "<img src=\"arts/backprop_kiank.png\" style=\"width:650px;height:250px;\">\n",
        "<caption>Figure 3 : Forward and Backward propagation for LINEAR->RELU->LINEAR->SIGMOID</br>The purple blocks represent the forward propagation, and the red blocks represent the backward propagation.</caption>\n",
        "\n",
        "이제, forward propagation과 마찬가지로 3단계에 걸쳐 backward propagation을 구현해봅시다.\n",
        "- 선형(LINEAR) 역전파 구현\n",
        "- ReLU 혹은 sigmoid 활성화 함수의 미분계수를 계산하는 LINEAR -> ACTIVATION 역방향 활성화 함수 구현\n",
        "- $L-1$ 회의 LINEAR -> RELU -> LINEAR -> SIGMOID 역전파"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAP0yinc2dhd"
      },
      "source": [
        "### 6-1. Linear 역전파 구현하기 ###\n",
        "\n",
        "$l$ 번째 층에 대하여, linear part는 : $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ 입니다.\n",
        "\n",
        "이미 미분계수 $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$ 를 계산했다고 가정하면, 이어서 $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ 를 구할 수 있습니다.\n",
        "\n",
        "<img src=\"arts/linearback_kiank.png\" style=\"width:250px;height:300px;\">\n",
        "<caption>Figure 4</caption>\n",
        "\n",
        "세 출력 값 $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ 은 입력값 $dZ^{[l]}$ 을 사용해서 계산할 수 있습니다. 공식은 아래와 같습니다.\n",
        "\n",
        "$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
        "$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
        "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n",
        "\n",
        "</br>\n",
        "\n",
        "**연습 문제**: 위의 세 가지 공식을 사용해 `linear_backward()` 함수를 구현하세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvjXJkEV1GEN"
      },
      "source": [
        "# GRADED FUNCTION: linear_backward\n",
        "\n",
        "def linear_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "    Arguments:\n",
        "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    ### START CODE HERE ### (≈ 3 lines of code)\n",
        "    dW = None\n",
        "    db = None\n",
        "    dA_prev = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp3S2Ars37d9"
      },
      "source": [
        "# Set up some test inputs\n",
        "dZ, linear_cache = linear_backward_test_case()\n",
        "\n",
        "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "print (\"dA_prev = \"+ str(dA_prev))\n",
        "print (\"dW = \" + str(dW))\n",
        "print (\"db = \" + str(db))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd_KoTNd37Og"
      },
      "source": [
        "**모범 답안**:\n",
        "    \n",
        "```\n",
        "dA_prev = \n",
        " [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n",
        " [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n",
        " [-0.4319552  -1.30987417  1.72354705  0.05070578]\n",
        " [-0.38981415  0.60811244 -1.25938424  1.47191593]\n",
        " [-2.52214926  2.67882552 -0.67947465  1.48119548]]\n",
        "dW = \n",
        " [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n",
        " [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n",
        " [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]\n",
        "db = \n",
        " [[-0.14713786]\n",
        " [-0.11313155]\n",
        " [-0.13209101]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg1A16_B4Bsu"
      },
      "source": [
        "### 6-2. 선형 활성화 backward 구현하기 ###\n",
        "\n",
        "다음으로 두 번째 helper 함수 `linear_backward()` 함수를 구현해 봅시다. \n",
        "\n",
        "`linear_activation_backward()` 함수를 구현하기 위해서 다음의 두 가지 backwrad 함수를 제공합니다.\n",
        "\n",
        "- **sigmoid_backward**: `sigmoid()` 활성화 함수의 역방향 도함수입니다. 아래와 같은 방법으로 호출할 수 있습니다.\n",
        "  ```python\n",
        "  dZ = sigmoid_backward(dA, activation_cache)\n",
        "  ```\n",
        "\n",
        "- **relu_backward**: `relu()` 활성화 함수의 역방향 도함수입니다. 아래와 같은 방법으로 호출할 수 있습니다.\n",
        "  ```python\n",
        "  dZ = relu_backward(dA, activation_cache)\n",
        "  ```\n",
        "\n",
        "만약 $g(.)$ 이 활성화 함수라면,\n",
        "`sigmoid_backward`와 `relu_backward`는 $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$\n",
        "를 계산합니다.\n",
        "\n",
        "**연습 문제** : *LINEAR -> ACTIVATION 층을 담당하는 역전파 함수를 구현하세요.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lpQKt9-6jbd"
      },
      "source": [
        "# GRADED FUNCTION: linear_activation_backward\n",
        "\n",
        "def linear_activation_backward(dA, cache, activation):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
        "    \n",
        "    Arguments:\n",
        "    dA -- post-activation gradient for current layer l \n",
        "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "    \n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        ### START CODE HERE ### (≈ 2 lines of code)\n",
        "        dZ = None\n",
        "        dA_prev, dW, db = None\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "    elif activation == \"sigmoid\":\n",
        "        ### START CODE HERE ### (≈ 2 lines of code)\n",
        "        dZ = None\n",
        "        dA_prev, dW, db = None\n",
        "        ### END CODE HERE ###\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVmdXgH-7LC-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnfwkCRE7LUJ"
      },
      "source": [
        "**sigmoid 함수의 모범 답안:**\n",
        "\n",
        "<table style=\"width:100%\">\n",
        "  <tr>\n",
        "    <td > dA_prev </td> \n",
        "           <td >[[ 0.11017994  0.01105339]\n",
        " [ 0.09466817  0.00949723]\n",
        " [-0.05743092 -0.00576154]] </td> \n",
        "  </tr> \n",
        "    <tr>\n",
        "    <td > dW </td> \n",
        "           <td > [[ 0.10266786  0.09778551 -0.01968084]] </td> \n",
        "  </tr> \n",
        "    <tr>\n",
        "    <td > db </td> \n",
        "           <td > [[-0.05729622]] </td> \n",
        "  </tr> \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQIxOTKi7SGJ"
      },
      "source": [
        "**ReLU 함수의 모범 답안:**\n",
        "\n",
        "<table style=\"width:100%\">\n",
        "  <tr>\n",
        "    <td > dA_prev </td> \n",
        "           <td > [[ 0.44090989  0.        ]\n",
        " [ 0.37883606  0.        ]\n",
        " [-0.2298228   0.        ]] </td> \n",
        "  </tr> \n",
        "    <tr>\n",
        "    <td > dW </td> \n",
        "           <td > [[ 0.44513824  0.37371418 -0.10478989]] </td> \n",
        "  </tr> \n",
        "    <tr>\n",
        "    <td > db </td> \n",
        "           <td > [[-0.20837892]] </td> \n",
        "  </tr> \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loaBpqlA7bWe"
      },
      "source": [
        "### 6-3. L-Model Backward 구현 ###\n",
        "\n",
        "이제 전체 신경망에 대한 역전파 기능을 구현합니다. `L_model_forward()` 함수를 구현할 때 각 반복마다 (X, W, b, z)를 담고 있는 `cache`를 저장했습니다. 역전파 모듈은 이 변수를 사용하여 gradient를 계산합니다. 따라서 `L_model_backward()` 함수는 $L$에서 시작하여 뒤 방향으로 모든 은닉층에 대해 반복적으로 gradient를 계산합니다. 각 단계마다, $l$번 층에서 캐시된 값을 사용하여 역전파를 계산합니다. 아래 그림 5번은 역전파가 어떤 방식으로 이루어지는지 나타내줍니다.\n",
        "\n",
        "<img src=\"arts/mn_backward.png\" style=\"width:450px;height:300px;\">\n",
        "<caption>Figure 5 : Backward pass</caption>\n",
        "\n",
        "</br>\n",
        "\n",
        "**backpropagation 초기화하기**: 우리는 이 신경망의 출력 $A^{[L]}$이  $\\sigma(Z^{[L]})$ 라는 것을 알고 있습니다. 따라서 여러분은 이를 통해서 `dAL` $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$ 을 계산하는 코드를 작성해야 합니다.\n",
        "코드를 작성할 때, 미적분에 대한 깊은 지식이 없이도 아래 공식을 사용하여 쉽게 구현할 수 있습니다.\n",
        "```python\n",
        "dAL = -(np.divide(Y, AL) - np.divide(1-Y, 1-AL))\n",
        "```\n",
        "\n",
        "이렇게 구해진 활성화 값에 대한 gradient `dAL`을 사용해 역전파를 이어서 수행할 수 있습니다. 그림 5번에서 볼 수 있듯이, `dAL` 값을 이전에 구현했던 LINEAR -> SIGMOID backward 함수 (이 함수는 앞서 다루었던 `L_model_forward` 함수에서 캐싱된 데이터를 사용합니다)에 투입할 수 있습니다.\n",
        "\n",
        "그리고 나서, 반복문을 통해서 다른 모든 층에 대해 LINEAR -> RELU backward 함수를 적용할 수 있습니다. 이 때 각 반복마다 ($dA$, $dW$, $db$) 값을 `grads` 딕셔너리에 저장할 필요가 있습니다. 아래 공식을 사용하세요:\n",
        "\n",
        "$$grads[\"dW\" + str(l)] = dW^{[l]}\\tag{15} $$\n",
        "\n",
        "예를 들어, $l=3$일 때 이 함수는 $dW^{[l]}$ 값을 `grads[\"dW3\"]`의 형태로 저장할 것입니다.\n",
        "\n",
        "**연습 문제**: [LINEAR->RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID 모델의 backpropagation을 구현하세요.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK0nrAjz7Owq"
      },
      "source": [
        "# GRADED FUNCTION: L_model_backward\n",
        "\n",
        "def L_model_backward(AL, Y, caches):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
        "    \n",
        "    Arguments:\n",
        "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
        "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
        "    \n",
        "    Returns:\n",
        "    grads -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ... \n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ... \n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # Initializing the backpropagation\n",
        "    ### START CODE HERE ### (1 line of code)\n",
        "    dAL = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
        "    ### START CODE HERE ### (approx. 2 lines)\n",
        "    current_cache = None\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Loop from l=L-2 to l=0\n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
        "        ### START CODE HERE ### (approx. 5 lines)\n",
        "        current_cache = None\n",
        "        dA_prev_temp, dW_temp, db_temp = None\n",
        "        grads[\"dA\" + str(l)] = None\n",
        "        grads[\"dW\" + str(l + 1)] = None\n",
        "        grads[\"db\" + str(l + 1)] = None\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3-_4UacB17q"
      },
      "source": [
        "AL, Y_assess, caches = L_model_backward_test_case()\n",
        "grads = L_model_backward(AL, Y_assess, caches)\n",
        "print_grads(grads)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEUKE2YTB1qw"
      },
      "source": [
        "**모범 답안:**\n",
        "\n",
        "<table style=\"width:60%\">\n",
        "  <tr>\n",
        "    <td > dW1 </td> \n",
        "           <td > [[ 0.41010002  0.07807203  0.13798444  0.10502167]\n",
        " [ 0.          0.          0.          0.        ]\n",
        " [ 0.05283652  0.01005865  0.01777766  0.0135308 ]] </td> \n",
        "  </tr> \n",
        "    <tr>\n",
        "    <td > db1 </td> \n",
        "           <td > [[-0.22007063]\n",
        " [ 0.        ]\n",
        " [-0.02835349]] </td> \n",
        "  </tr> \n",
        "  <tr>\n",
        "  <td > dA1 </td> \n",
        "           <td > [[ 0.12913162 -0.44014127]\n",
        " [-0.14175655  0.48317296]\n",
        " [ 0.01663708 -0.05670698]] </td> \n",
        "\n",
        "  </tr> \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX61VU-lB885"
      },
      "source": [
        "### 6-4. 파라미터 업데이트 하기 ###\n",
        "\n",
        "이번에는 경사 하강법을 사용하여 모델의 파라미터를 업데이트하겠습니다.\n",
        "\n",
        "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
        "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
        "\n",
        "지금까지처럼 $\\alpha$ 변수는 학습률입니다. 업데이트된 파라미터를 계산하고 나서 이 값들을 `parameters` 딕셔너리에 집어넣어봅시다.\n",
        "\n",
        "**연습 문제**: 경사 하강법을 이용하여 파라미터를 업데이트하는 `update_parameters()` 함수를 구현해보세요.\n",
        "\n",
        "**지시 사항**: 모든 입력 층 $l = 1, 2, ..., L$에 대하여 파라미터 $W^{[l]}$ and $b^{[l]}$ 를 업데이트해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "890mocyjF5pm"
      },
      "source": [
        "# GRADED FUNCTION: update_parameters\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using gradient descent\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "                  parameters[\"W\" + str(l)] = ... \n",
        "                  parameters[\"b\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    # Update rule for each parameter. Use a for loop.\n",
        "    ### START CODE HERE ### (≈ 3 lines of code)\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = None\n",
        "        parameters[\"b\" + str(l+1)] = None\n",
        "    ### END CODE HERE ###\n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xp7pL6H9F8AZ"
      },
      "source": [
        "parameters, grads = update_parameters_test_case()\n",
        "parameters = update_parameters(parameters, grads, 0.1)\n",
        "\n",
        "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
        "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
        "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
        "print (\"b2 = \"+ str(parameters[\"b2\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9gz4Q1kF9pJ"
      },
      "source": [
        "**모범 답안**:\n",
        "\n",
        "<table style=\"width:100%\"> \n",
        "    <tr>\n",
        "    <td > W1 </td> \n",
        "           <td > [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
        " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
        " [-1.0535704  -0.86128581  0.68284052  2.20374577]] </td> \n",
        "  </tr> \n",
        "    <tr>\n",
        "    <td > b1 </td> \n",
        "           <td > [[-0.04659241]\n",
        " [-1.28888275]\n",
        " [ 0.53405496]] </td> \n",
        "  </tr> \n",
        "  <tr>\n",
        "    <td > W2 </td> \n",
        "           <td > [[-0.55569196  0.0354055   1.32964895]]</td> \n",
        "  </tr> \n",
        "    <tr>\n",
        "    <td > b2 </td> \n",
        "           <td > [[-0.84610769]] </td> \n",
        "  </tr> \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzpynEzQGEaK"
      },
      "source": [
        "## 7. 결론 ##\n",
        "\n",
        "심층 신경망을 구현하기 위해 필요한 모든 함수들을 성공적으로 구현했습니다. 꽤나 긴 과제였지만, 그래도 한 시름 놓았습니다. 이후 진행할 과제를 더 쉽게 수행할 수 있을 겁니다. 다음 과제에선 이번에 구현한 모든 함수를 합쳐서 두 모델을 만들 것입니다.\n",
        "- 2-layer 신경망\n",
        "- L-layer 신경망\n",
        "\n",
        "좀 더 구체적으로는, 이 신경망들을 이용해 고양이와 고양이가 아닌 이미지를 분류하는 모델을 만듭니다."
      ]
    }
  ]
}